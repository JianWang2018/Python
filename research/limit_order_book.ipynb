{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "load the limit order book data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File b'/media/jianwang/b2ba2597-9566-445c-87e6-e801c3aee85d/jian/research/data/order_book/AAPL_2012-06-21_34200000_57600000_orderbook_10.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b4881b81790f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# header =-1 means that the first line is not the header, otherwise, the first line will be header\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# data_order is for order book and data mess is for message book\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mdata_order_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_load\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mticker_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mname_order\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float64\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mdata_mess_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_load\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mticker_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mname_mess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float64\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Time for importing the \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mticker_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" data is:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[0;32m    496\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 731\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3246)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6111)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: File b'/media/jianwang/b2ba2597-9566-445c-87e6-e801c3aee85d/jian/research/data/order_book/AAPL_2012-06-21_34200000_57600000_orderbook_10.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set default parameters\n",
    "ticker_list=[\"AAPL\",\"AMZN\",\"GOOG\",\"INTC\",\"MSFT\"]\n",
    "start_ind=10*3600\n",
    "end_ind=15.5*3600\n",
    "data_order_list=[]\n",
    "data_mess_list=[]\n",
    "time_index_list=[]\n",
    "path_save='/media/jianwang/New Volume/research/order_book/'\n",
    "path_load=\"/media/jianwang/b2ba2597-9566-445c-87e6-e801c3aee85d/jian/research/data/order_book/\"\n",
    "\n",
    "\n",
    "#read the stock ticker\n",
    "#totally 5 dataset\n",
    "\n",
    "for i in range(len(ticker_list)):\n",
    "    #get the path for the csv files\n",
    "    # name_order is for the order book and name_mess for the message book\n",
    "    name_order='_2012-06-21_34200000_57600000_orderbook_10.csv'\n",
    "    name_mess='_2012-06-21_34200000_57600000_message_10.csv'\n",
    "    # calculate the cputime for reading the data\n",
    "    t=time.time()\n",
    "    # header =-1 means that the first line is not the header, otherwise, the first line will be header\n",
    "    # data_order is for order book and data mess is for message book\n",
    "    data_order_list.append(np.array(pd.read_csv(path_load+ticker_list[i]+name_order,header=-1),dtype=\"float64\"))\n",
    "    data_mess_list.append(np.array(pd.read_csv(path_load+ticker_list[i]+name_mess,header=-1),dtype=\"float64\"))\n",
    "    print(\"Time for importing the \"+ticker_list[i]+\" data is:\",time.time()-t)\n",
    "    print(\"The shape of the order data is: \",data_order_list[i].shape, \" of message data is: \", data_mess_list[i].shape)\n",
    "    # get the time index\n",
    "    time_index_list.append(data_mess＿list[i][:,0])\n",
    "\n",
    "\n",
    "#print the sample of data\n",
    "print(\"Check the original data:\")\n",
    "\n",
    "for i in range(len(ticker_list)):\n",
    "    print()\n",
    "    print(\"The first five sampe of \"+ticker_list[i]+\" is: \",data_order_list[i][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature \n",
    "\n",
    "build model feature according to the paper of Dr. Alec kircheval and yuan zhang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # save the feature array\n",
    "# ##get the original order,message and time index data, header =-1 means that did not\n",
    "# ##read the first column as the name\n",
    "#%%\n",
    "# # use a loop to read data\n",
    "# for ticker_ind in range(len(ticker_list)):\n",
    "#     data_order=data_order_list[ticker_ind]\n",
    "#     data_mess=data_mess_list[ticker_ind]\n",
    "#     time_index=data_mess[:,0]\n",
    "#     # obtain the reduced order message and time_index dataset, half an hour after the\n",
    "#     # 9:30 and half an hour before 16:00\n",
    "#     # data_reduced is used to install the data from 10 to 15:30, take half hour for auction\n",
    "#     data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "#     data_mess_reduced=data_mess[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "#     time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "\n",
    "#     test_lower=0\n",
    "#     # test up is the up index of the original data to construct the test data\n",
    "#     test_upper=len(data_order_reduced)\n",
    "#     # data_test is the subset of data_reduced from the lower index to upper index\n",
    "#     data_order_test=data_order_reduced[test_lower:test_upper,:]\n",
    "#     data_mess_test=data_mess_reduced[test_lower:test_upper,:]\n",
    "#     t=time.time()\n",
    "#     feature_array=get_features (data_order, data_mess,data_order_test,data_mess_test)\n",
    "#     np.savetxt(path_save+ticker_list[ticker_ind]+'_feature_array.txt',feature_array,delimiter=' ')\n",
    "#     print (\"Time for building \"+ticker_list[ticker_ind]+\" is:\",time.time()-t)\n",
    "\n",
    "\n",
    "# load the feature\n",
    "#%%\n",
    "import time\n",
    "t=time.time()\n",
    "feature_array_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    feature_array_list.append(np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_feature_array.txt',\\\n",
    "                                                   sep=' ',header=-1)))\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## response \n",
    "build response, divided into four categories, ask_low, bid_high, no arbitrage and multi-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the total response is:\n",
      "\n",
      "(400236, 1)\n",
      "(269571, 1)\n",
      "(147766, 1)\n",
      "(622641, 1)\n",
      "(667701, 1)\n",
      "The shape of the reduced response is:\n",
      "\n",
      "(309538, 1)\n",
      "(218710, 1)\n",
      "(118877, 1)\n",
      "(458160, 1)\n",
      "(511299, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# this function used to build the y\n",
    "# ask_low as 1 bad high as -1 and no arbitrage as 0\n",
    "# option=1 return ask low, option =2 return bid high, option =3 return no arbi, option =4 return total(ask_low=1,\n",
    "# bid_high =-1 and no arbi =0)\n",
    "#%%\n",
    "def build_y(ask_low,bid_high,no_arbi,option):\n",
    "    if (option==1):\n",
    "        return ask_low\n",
    "    elif option==2:\n",
    "        return bid_high\n",
    "    elif option==3:\n",
    "        return no_arbi\n",
    "    elif option==4:\n",
    "        return ask_low-bid_high\n",
    "    else:\n",
    "        print(\"option should be 1,2,3,4\")\n",
    "\n",
    "## save y data\n",
    "#%%\n",
    "#time_ind=1\n",
    "#option_ind=1\n",
    "#for ticker_ind in range(len(ticker_list)):\n",
    "#    response=build_y(ask_low_time_list[ticker_ind][time_ind],bid_high_time_list[ticker_ind][time_ind],\\\n",
    "#                                 no_arbi_time_list[ticker_ind][time_ind],option=option_ind)\n",
    "#    np.savetxt(path_save+ticker_list[ticker_ind]+'_response.txt',response)\n",
    "\n",
    "\n",
    "\n",
    "## load y data\n",
    "#%%\n",
    "response_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    response_list.append((np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_response.txt',header=-1))))\n",
    "\n",
    "\n",
    "## print the shape of the response\n",
    "## note it is the total response\n",
    "#%%\n",
    "print(\"The shape of the total response is:\\n\")\n",
    "\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_list[ticker_ind].shape)\n",
    "\n",
    "# need to get the response from 10 to 15:30\n",
    "# the shape of the response and the feature array should be equal\n",
    "response_reduced_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    first_ind = np.where(time_index_list[ticker_ind]>=start_ind)[0][0]\n",
    "    last_ind=np.where(time_index_list[ticker_ind]<=end_ind)[0][-1]\n",
    "    response_reduced_list.append(response_list[ticker_ind][first_ind:last_ind+1])\n",
    "\n",
    "print(\"The shape of the reduced response is:\\n\")\n",
    "\n",
    "## print the shape of reduced response\n",
    "## response reduced is used for testing and training the model\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_reduced_list[ticker_ind].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and test data\n",
    "\n",
    "using certain percentage to build train and test model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for importing the AAPL data is: 2.5101521015167236\n",
      "The shape of the order data is:  (400391, 40)  of message data is:  (400391, 6)\n",
      "Time for importing the AMZN data is: 1.8147871494293213\n",
      "The shape of the order data is:  (269748, 40)  of message data is:  (269748, 6)\n",
      "Time for importing the GOOG data is: 0.9467873573303223\n",
      "The shape of the order data is:  (147916, 40)  of message data is:  (147916, 6)\n",
      "Time for importing the INTC data is: 3.160951614379883\n",
      "The shape of the order data is:  (624040, 40)  of message data is:  (624040, 6)\n",
      "Time for importing the MSFT data is: 3.6491353511810303\n",
      "The shape of the order data is:  (668765, 40)  of message data is:  (668765, 6)\n",
      "Check the original data:\n",
      "\n",
      "The first five sampe of AAPL is:  [[  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86100000e+06   2.00000000e+02   5.85100000e+06   5.00000000e+00\n",
      "    5.86890000e+06   3.00000000e+02   5.85010000e+06   8.90000000e+01\n",
      "    5.86950000e+06   5.00000000e+01   5.84970000e+06   5.00000000e+00\n",
      "    5.87000000e+06   1.00000000e+02   5.84930000e+06   3.00000000e+02\n",
      "    5.87100000e+06   1.00000000e+01   5.84650000e+06   3.00000000e+02\n",
      "    5.87390000e+06   1.00000000e+02   5.84530000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84380000e+06   2.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84270000e+06   3.00000000e+02]\n",
      " [  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85320000e+06   1.80000000e+01\n",
      "    5.86100000e+06   2.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86890000e+06   3.00000000e+02   5.85100000e+06   5.00000000e+00\n",
      "    5.86950000e+06   5.00000000e+01   5.85010000e+06   8.90000000e+01\n",
      "    5.87000000e+06   1.00000000e+02   5.84970000e+06   5.00000000e+00\n",
      "    5.87100000e+06   1.00000000e+01   5.84930000e+06   3.00000000e+02\n",
      "    5.87390000e+06   1.00000000e+02   5.84650000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84530000e+06   3.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84380000e+06   2.00000000e+02]\n",
      " [  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85320000e+06   1.80000000e+01\n",
      "    5.86100000e+06   2.00000000e+02   5.85310000e+06   1.80000000e+01\n",
      "    5.86890000e+06   3.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86950000e+06   5.00000000e+01   5.85100000e+06   5.00000000e+00\n",
      "    5.87000000e+06   1.00000000e+02   5.85010000e+06   8.90000000e+01\n",
      "    5.87100000e+06   1.00000000e+01   5.84970000e+06   5.00000000e+00\n",
      "    5.87390000e+06   1.00000000e+02   5.84930000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84650000e+06   3.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84530000e+06   3.00000000e+02]]\n",
      "\n",
      "The first five sampe of AMZN is:  [[  2.23950000e+06   1.00000000e+02   2.23180000e+06   1.00000000e+02\n",
      "    2.23990000e+06   1.00000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24400000e+06   5.47000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24540000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.24890000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.26770000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29430000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02\n",
      "    2.29800000e+06   1.00000000e+02   2.18970000e+06   1.00000000e+02]\n",
      " [  2.23950000e+06   1.00000000e+02   2.23810000e+06   2.10000000e+01\n",
      "    2.23990000e+06   1.00000000e+02   2.23180000e+06   1.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24400000e+06   5.47000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24540000e+06   1.00000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24890000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.26770000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.29430000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29800000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02]\n",
      " [  2.23950000e+06   1.00000000e+02   2.23810000e+06   2.10000000e+01\n",
      "    2.23960000e+06   2.00000000e+01   2.23180000e+06   1.00000000e+02\n",
      "    2.23990000e+06   1.00000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24400000e+06   5.47000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24540000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.24890000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.26770000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29430000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02]]\n",
      "\n",
      "The first five sampe of GOOG is:  [[  5.80230000e+06   1.00000000e+02   5.79400000e+06   4.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]\n",
      " [  5.80230000e+06   1.00000000e+02   5.79400000e+06   1.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]\n",
      " [  5.80230000e+06   1.00000000e+02   5.79400000e+06   1.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]]\n",
      "\n",
      "The first five sampe of INTC is:  [[  2.75200000e+05   6.60000000e+01   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74600000e+05   7.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74500000e+05   9.00000000e+02\n",
      "    2.76100000e+05   2.30000000e+03   2.74400000e+05   2.80000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76300000e+05   2.00000000e+03   2.74200000e+05   4.06300000e+03]\n",
      " [  2.75200000e+05   1.66000000e+02   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74600000e+05   7.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74500000e+05   9.00000000e+02\n",
      "    2.76100000e+05   2.30000000e+03   2.74400000e+05   2.80000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76300000e+05   2.00000000e+03   2.74200000e+05   4.06300000e+03]\n",
      " [  2.75200000e+05   1.66000000e+02   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75500000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74600000e+05   7.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74500000e+05   9.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74400000e+05   2.80000000e+03\n",
      "    2.76100000e+05   2.30000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74200000e+05   4.06300000e+03]]\n",
      "\n",
      "The first five sampe of MSFT is:  [[  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10500000e+05   1.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10600000e+05   1.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08800000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08500000e+05   4.00000000e+02\n",
      "    3.11400000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]\n",
      " [  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10500000e+05   2.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10600000e+05   1.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08800000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08500000e+05   4.00000000e+02\n",
      "    3.11400000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]\n",
      " [  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10400000e+05   1.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10500000e+05   2.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10600000e+05   1.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08800000e+05   2.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08500000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]]\n",
      "79.46510910987854\n",
      "The shape of the total response is:\n",
      "\n",
      "(400236, 1)\n",
      "(269571, 1)\n",
      "(147766, 1)\n",
      "(622641, 1)\n",
      "(667701, 1)\n",
      "The shape of the reduced response is:\n",
      "\n",
      "(309538, 1)\n",
      "(218710, 1)\n",
      "(118877, 1)\n",
      "(458160, 1)\n",
      "(511299, 1)\n",
      "total array shape: (309538, 135)\n",
      "test_y shape: (30954,)\n",
      "train_y shape: (278584,)\n",
      "total array shape: (309538, 135)\n",
      "train_x shape: (247630, 134)\n",
      "test_x shape: (61908, 134)\n",
      "test_y shape: (61908,)\n",
      "train_y shape: (247630,)\n",
      "[  1.39259502e-14   2.34763051e-15  -7.06956468e-15  -4.92044944e-16\n",
      "   1.64523898e-14   2.14204194e-15  -3.82715034e-15   1.62615774e-15\n",
      "   2.04390808e-14  -1.28046063e-15  -4.00391804e-15  -2.65015836e-15\n",
      "   4.23444702e-15   7.09027685e-16  -1.45937079e-14  -1.14776865e-16\n",
      "  -2.00600062e-14  -1.68391709e-15  -1.07979010e-14   2.38889705e-15\n",
      "   4.22791650e-15   8.11755595e-16  -1.86600608e-14   1.57948526e-15\n",
      "   1.01211774e-14   7.63789773e-16  -2.04879812e-14  -5.75666980e-16\n",
      "   2.19418474e-14   2.39610545e-17   5.99685602e-15   1.54857954e-15\n",
      "  -1.20089089e-14  -3.13694114e-15  -9.45855768e-15   3.41373069e-16\n",
      "  -1.95099914e-14  -2.34983985e-15  -5.06540835e-15  -2.45773464e-15\n",
      "  -5.62441414e-16  -5.96343948e-16  -1.25753392e-16   2.70804212e-16\n",
      "  -1.13173435e-16  -1.20964679e-16   1.03360181e-17   7.88575323e-17\n",
      "   3.98774195e-16  -9.94237604e-17   5.50687926e-15   6.35188797e-15\n",
      "   8.37257099e-15   1.67746490e-14  -1.53134129e-14  -6.80332995e-15\n",
      "  -5.09100674e-15   1.37696340e-14  -1.06669231e-14   9.40457940e-15\n",
      "  -4.50264593e-15   3.45955322e-15  -9.06442649e-17  -6.09184839e-16\n",
      "  -3.07231794e-16   1.95331127e-15   3.26529625e-15   4.23939041e-16\n",
      "  -2.12906640e-15  -5.05523374e-16   5.82097065e-16  -1.85615588e-15\n",
      "   1.82553654e-15   2.53685490e-15  -8.40377362e-16   3.40989962e-16\n",
      "   2.95034494e-16  -5.01166232e-15  -1.32957983e-14  -8.83059818e-15\n",
      "   6.09367035e-14  -5.20668525e-14  -1.87361064e-16   5.84217711e-17\n",
      "  -3.88695173e-16  -3.79035335e-16  -3.03793997e-16   2.29724795e-15\n",
      "  -5.07649669e-15   2.09025970e-15  -2.91364298e-15  -1.62267296e-15\n",
      "   1.01304410e-15   3.85854934e-15  -2.26728424e-15  -2.48222687e-15\n",
      "  -5.74272420e-16   1.88874599e-15   1.20056147e-16   1.32321997e-15\n",
      "  -2.18228146e-15   3.23605165e-15  -1.90358293e-15  -2.25685701e-15\n",
      "   7.66147170e-16  -1.83318975e-16  -2.14742197e-15   1.06412263e-15\n",
      "  -1.34353440e-16   5.15470177e-16   1.10274293e-15   2.08232160e-15\n",
      "   1.88262658e-15   2.94087220e-15   3.27842115e-15   4.13972924e-16\n",
      "  -1.05498124e-16   1.81422275e-16   1.27425367e-15  -1.57823786e-15\n",
      "  -5.81267227e-16   9.31438061e-16  -3.82108803e-16  -4.87712724e-16\n",
      "  -1.50226899e-16   2.58520159e-16  -3.46529824e-15   6.95040950e-16\n",
      "  -4.80807988e-16  -3.19604977e-15  -2.23794250e-17   6.40834467e-16\n",
      "   3.63368905e-15   1.51408666e-15]\n",
      "[ 0.00893932  0.0012163   0.00891669  0.00199028  0.0089571   0.00199068\n",
      "  0.00889756  0.00511242  0.00896376  0.00101617  0.0088651  -0.00156945\n",
      "  0.00893288 -0.00297427  0.0088477  -0.00836159  0.00894363  0.00176518\n",
      "  0.00876454 -0.00710706  0.00896774  0.0036686   0.00873523 -0.00104048\n",
      "  0.00893784  0.01505549  0.00874203  0.00186365  0.0089689   0.00223734\n",
      "  0.00878879 -0.00441225  0.00890796  0.0038717   0.0087713   0.00171931\n",
      "  0.00888919  0.00648263  0.00876072 -0.0034556   0.00298348  0.00472627\n",
      "  0.0061101   0.00563692  0.00825771  0.00951717  0.00819441  0.00739076\n",
      "  0.00604075  0.00560558  0.00892894  0.00892837  0.00891563  0.00889157\n",
      "  0.00885575  0.00885347  0.00884202  0.00888119  0.00884218  0.00882791\n",
      "  0.00204579  0.00115609 -0.00294633  0.00168537  0.00335302 -0.00290929\n",
      "  0.00416975 -0.00635456 -0.00140362  0.00192396  0.0031067   0.00202197\n",
      "  0.00890267  0.00352693 -0.00042258 -0.00527003  0.0029405   0.00212222\n",
      "  0.00894154  0.00880971  0.00885482 -0.00437663  0.0073378   0.00808012\n",
      "  0.00173333  0.00128039 -0.00059763  0.00434622  0.00303184  0.00297175\n",
      "  0.00432812  0.00428515  0.00403988  0.00365531 -0.00090237 -0.00298658\n",
      " -0.00244931 -0.00121953 -0.00217522 -0.00183446 -0.00242696 -0.00256073\n",
      " -0.00263417 -0.00340147 -0.00373844  0.00094609  0.00196623 -0.00136297\n",
      " -0.00133103  0.00234631  0.00089228  0.00107855  0.0024298   0.00633219\n",
      " -0.00414566  0.0038272  -0.01041283  0.00463432 -0.00361599  0.00020092\n",
      "  0.01525463 -0.00401765 -0.00268375  0.00472893  0.00187398 -0.00290205\n",
      " -0.00172033 -0.00562311  0.00324177 -0.00381602  0.00214496 -0.00226909\n",
      " -0.00190964 -0.00692654]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# Random split\n",
    "#%%\n",
    "import random\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "ticker_ind=0\n",
    "\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)\n",
    "# choose 10000 random sample\n",
    "total_array=total_array[np.random.randint(len(total_array),size=len(total_array)),:]\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x, test_x, train_y, test_y =train_test_split(\\\n",
    "total_array[:,:134],total_array[:,134], test_size=0.1, random_state=42)\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)\n",
    "\n",
    "#time series split\n",
    "#%%\n",
    "\n",
    "\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)\n",
    "# choose 10000 random sample\n",
    "total_array=total_array[np.random.randint(len(total_array),size=len(total_array)),:]\n",
    "\n",
    "train_num_index=int(len(total_array)*0.8)\n",
    "ticker_ind=0\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x=total_array[:train_num_index,:134]\n",
    "test_x=total_array[train_num_index:,:134]\n",
    "train_y=total_array[:train_num_index,134]\n",
    "test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "print(\"train_x shape:\",train_x.shape)\n",
    "print(\"test_x shape:\",test_x.shape)\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)\n",
    "\n",
    "# scale data\n",
    "#%%\n",
    "\n",
    "# can use the processing.scale function to scale the data\n",
    "from sklearn import preprocessing\n",
    "# note that we need to transfer the data type to float\n",
    "# remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "# use scale for zero mean and one std\n",
    "scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "train_x_scale=scaler.transform(train_x)\n",
    "test_x_scale=scaler.transform(test_x)\n",
    "\n",
    "print(np.mean(train_x_scale,0))\n",
    "print(np.mean(test_x_scale,0))\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model test\n",
    "\n",
    "with the same train and test data use different machine learning models to test the training and testing time, model accuracy and other statistical metrics such as f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259.023060798645\n",
      "train_accuracy is: 0.999874813229\n",
      "precision is: \t 0.998444899979\n",
      "recall is: \t 0.999363237583\n",
      "f1 score is: \t 0.998903857714\n",
      "accuracy is: 0.997770885831\n",
      "precision is: \t 0.969791963522\n",
      "recall is: \t 0.990684133916\n",
      "f1 score is: \t 0.980126728111\n",
      "Confusion matrix, without normalization\n",
      "[[58367    32]\n",
      " [  106  3403]]\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "time_rf=time.time()\n",
    "clf =  RandomForestClassifier(max_depth=None,n_estimators=100,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-time_ada)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y=np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "# test the score for the test data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
