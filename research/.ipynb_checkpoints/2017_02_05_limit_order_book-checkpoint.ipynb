{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit order book\n",
    "\n",
    "author: Jian Wang\n",
    "\n",
    "time: 2016-02-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Model prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for importing the AAPL data is: 2.2476625442504883\n",
      "The shape of the order data is:  (400391, 40)  of message data is:  (400391, 6)\n",
      "Time for importing the AMZN data is: 1.3033385276794434\n",
      "The shape of the order data is:  (269748, 40)  of message data is:  (269748, 6)\n",
      "Time for importing the GOOG data is: 0.8174479007720947\n",
      "The shape of the order data is:  (147916, 40)  of message data is:  (147916, 6)\n",
      "Time for importing the INTC data is: 3.5862882137298584\n",
      "The shape of the order data is:  (624040, 40)  of message data is:  (624040, 6)\n",
      "Time for importing the MSFT data is: 3.8824493885040283\n",
      "The shape of the order data is:  (668765, 40)  of message data is:  (668765, 6)\n",
      "Check the original data:\n",
      "\n",
      "The first five sampe of AAPL is:  [[  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86100000e+06   2.00000000e+02   5.85100000e+06   5.00000000e+00\n",
      "    5.86890000e+06   3.00000000e+02   5.85010000e+06   8.90000000e+01\n",
      "    5.86950000e+06   5.00000000e+01   5.84970000e+06   5.00000000e+00\n",
      "    5.87000000e+06   1.00000000e+02   5.84930000e+06   3.00000000e+02\n",
      "    5.87100000e+06   1.00000000e+01   5.84650000e+06   3.00000000e+02\n",
      "    5.87390000e+06   1.00000000e+02   5.84530000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84380000e+06   2.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84270000e+06   3.00000000e+02]\n",
      " [  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85320000e+06   1.80000000e+01\n",
      "    5.86100000e+06   2.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86890000e+06   3.00000000e+02   5.85100000e+06   5.00000000e+00\n",
      "    5.86950000e+06   5.00000000e+01   5.85010000e+06   8.90000000e+01\n",
      "    5.87000000e+06   1.00000000e+02   5.84970000e+06   5.00000000e+00\n",
      "    5.87100000e+06   1.00000000e+01   5.84930000e+06   3.00000000e+02\n",
      "    5.87390000e+06   1.00000000e+02   5.84650000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84530000e+06   3.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84380000e+06   2.00000000e+02]\n",
      " [  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85320000e+06   1.80000000e+01\n",
      "    5.86100000e+06   2.00000000e+02   5.85310000e+06   1.80000000e+01\n",
      "    5.86890000e+06   3.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86950000e+06   5.00000000e+01   5.85100000e+06   5.00000000e+00\n",
      "    5.87000000e+06   1.00000000e+02   5.85010000e+06   8.90000000e+01\n",
      "    5.87100000e+06   1.00000000e+01   5.84970000e+06   5.00000000e+00\n",
      "    5.87390000e+06   1.00000000e+02   5.84930000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84650000e+06   3.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84530000e+06   3.00000000e+02]]\n",
      "\n",
      "The first five sampe of AMZN is:  [[  2.23950000e+06   1.00000000e+02   2.23180000e+06   1.00000000e+02\n",
      "    2.23990000e+06   1.00000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24400000e+06   5.47000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24540000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.24890000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.26770000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29430000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02\n",
      "    2.29800000e+06   1.00000000e+02   2.18970000e+06   1.00000000e+02]\n",
      " [  2.23950000e+06   1.00000000e+02   2.23810000e+06   2.10000000e+01\n",
      "    2.23990000e+06   1.00000000e+02   2.23180000e+06   1.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24400000e+06   5.47000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24540000e+06   1.00000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24890000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.26770000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.29430000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29800000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02]\n",
      " [  2.23950000e+06   1.00000000e+02   2.23810000e+06   2.10000000e+01\n",
      "    2.23960000e+06   2.00000000e+01   2.23180000e+06   1.00000000e+02\n",
      "    2.23990000e+06   1.00000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24400000e+06   5.47000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24540000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.24890000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.26770000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29430000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02]]\n",
      "\n",
      "The first five sampe of GOOG is:  [[  5.80230000e+06   1.00000000e+02   5.79400000e+06   4.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]\n",
      " [  5.80230000e+06   1.00000000e+02   5.79400000e+06   1.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]\n",
      " [  5.80230000e+06   1.00000000e+02   5.79400000e+06   1.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]]\n",
      "\n",
      "The first five sampe of INTC is:  [[  2.75200000e+05   6.60000000e+01   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74600000e+05   7.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74500000e+05   9.00000000e+02\n",
      "    2.76100000e+05   2.30000000e+03   2.74400000e+05   2.80000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76300000e+05   2.00000000e+03   2.74200000e+05   4.06300000e+03]\n",
      " [  2.75200000e+05   1.66000000e+02   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74600000e+05   7.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74500000e+05   9.00000000e+02\n",
      "    2.76100000e+05   2.30000000e+03   2.74400000e+05   2.80000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76300000e+05   2.00000000e+03   2.74200000e+05   4.06300000e+03]\n",
      " [  2.75200000e+05   1.66000000e+02   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75500000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74600000e+05   7.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74500000e+05   9.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74400000e+05   2.80000000e+03\n",
      "    2.76100000e+05   2.30000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74200000e+05   4.06300000e+03]]\n",
      "\n",
      "The first five sampe of MSFT is:  [[  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10500000e+05   1.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10600000e+05   1.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08800000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08500000e+05   4.00000000e+02\n",
      "    3.11400000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]\n",
      " [  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10500000e+05   2.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10600000e+05   1.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08800000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08500000e+05   4.00000000e+02\n",
      "    3.11400000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]\n",
      " [  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10400000e+05   1.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10500000e+05   2.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10600000e+05   1.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08800000e+05   2.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08500000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]]\n",
      "82.3326678276062\n",
      "The shape of the total response is:\n",
      "\n",
      "(400236, 1)\n",
      "(269571, 1)\n",
      "(147766, 1)\n",
      "(622641, 1)\n",
      "(667701, 1)\n",
      "The shape of the reduced response is:\n",
      "\n",
      "(309538, 1)\n",
      "(218710, 1)\n",
      "(118877, 1)\n",
      "(458160, 1)\n",
      "(511299, 1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Aug 26 00:03:47 2016\n",
    "\n",
    "@author: jianwang\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set default parameters\n",
    "ticker_list=[\"AAPL\",\"AMZN\",\"GOOG\",\"INTC\",\"MSFT\"]\n",
    "start_ind=10*3600\n",
    "end_ind=15.5*3600\n",
    "data_order_list=[]\n",
    "data_mess_list=[]\n",
    "time_index_list=[]\n",
    "path_save='/media/jianwang/Study/Research/order_book/'\n",
    "path_load=\"/media/jianwang/Study/Research/order_book/\"\n",
    "\n",
    "## set random seed to produce the same results\n",
    "\n",
    "np.random.seed(987612345)\n",
    "\n",
    "#read the stock ticker\n",
    "#totally 5 dataset\n",
    "\n",
    "for i in range(len(ticker_list)):\n",
    "    #get the path for the csv files\n",
    "    # name_order is for the order book and name_mess for the message book\n",
    "    name_order='_2012-06-21_34200000_57600000_orderbook_10.csv'\n",
    "    name_mess='_2012-06-21_34200000_57600000_message_10.csv'\n",
    "    # calculate the cputime for reading the data\n",
    "    t=time.time()\n",
    "    # header =-1 means that the first line is not the header, otherwise, the first line will be header\n",
    "    # data_order is for order book and data mess is for message book\n",
    "    data_order_list.append(np.array(pd.read_csv(path_load+ticker_list[i]+name_order,header=-1),dtype=\"float64\"))\n",
    "    data_mess_list.append(np.array(pd.read_csv(path_load+ticker_list[i]+name_mess,header=-1),dtype=\"float64\"))\n",
    "    print(\"Time for importing the \"+ticker_list[i]+\" data is:\",time.time()-t)\n",
    "    print(\"The shape of the order data is: \",data_order_list[i].shape, \" of message data is: \", data_mess_list[i].shape)\n",
    "    # get the time index\n",
    "    time_index_list.append(data_messï¼¿list[i][:,0])\n",
    "\n",
    "\n",
    "#print the sample of data\n",
    "print(\"Check the original data:\")\n",
    "\n",
    "for i in range(len(ticker_list)):\n",
    "    print()\n",
    "    print(\"The first five sampe of \"+ticker_list[i]+\" is: \",data_order_list[i][:3])\n",
    "\n",
    "    # -*- coding: utf-8 -*-\n",
    "\n",
    "# # save the feature array\n",
    "# ##get the original order,message and time index data, header =-1 means that did not\n",
    "# ##read the first column as the name\n",
    "#%%\n",
    "# # use a loop to read data\n",
    "# for ticker_ind in range(len(ticker_list)):\n",
    "#     data_order=data_order_list[ticker_ind]\n",
    "#     data_mess=data_mess_list[ticker_ind]\n",
    "#     time_index=data_mess[:,0]\n",
    "#     # obtain the reduced order message and time_index dataset, half an hour after the\n",
    "#     # 9:30 and half an hour before 16:00\n",
    "#     # data_reduced is used to install the data from 10 to 15:30, take half hour for auction\n",
    "#     data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "#     data_mess_reduced=data_mess[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "#     time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "\n",
    "#     test_lower=0\n",
    "#     # test up is the up index of the original data to construct the test data\n",
    "#     test_upper=len(data_order_reduced)\n",
    "#     # data_test is the subset of data_reduced from the lower index to upper index\n",
    "#     data_order_test=data_order_reduced[test_lower:test_upper,:]\n",
    "#     data_mess_test=data_mess_reduced[test_lower:test_upper,:]\n",
    "#     t=time.time()\n",
    "#     feature_array=get_features (data_order, data_mess,data_order_test,data_mess_test)\n",
    "#     np.savetxt(path_save+ticker_list[ticker_ind]+'_feature_array.txt',feature_array,delimiter=' ')\n",
    "#     print (\"Time for building \"+ticker_list[ticker_ind]+\" is:\",time.time()-t)\n",
    "\n",
    "\n",
    "# load the feature\n",
    "#%%\n",
    "import time\n",
    "t=time.time()\n",
    "feature_array_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    feature_array_list.append(np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_feature_array.txt',\\\n",
    "                                                   sep=' ',header=-1)))\n",
    "print(time.time()-t)\n",
    "\n",
    "# this function used to build the y\n",
    "# ask_low as 1 bad high as -1 and no arbitrage as 0\n",
    "# option=1 return ask low, option =2 return bid high, option =3 return no arbi, option =4 return total(ask_low=1,\n",
    "# bid_high =-1 and no arbi =0)\n",
    "#%%\n",
    "def build_y(ask_low,bid_high,no_arbi,option):\n",
    "    if (option==1):\n",
    "        return ask_low\n",
    "    elif option==2:\n",
    "        return bid_high\n",
    "    elif option==3:\n",
    "        return no_arbi\n",
    "    elif option==4:\n",
    "        return ask_low-bid_high\n",
    "    else:\n",
    "        print(\"option should be 1,2,3,4\")\n",
    "\n",
    "## save y data\n",
    "#%%\n",
    "#time_ind=1\n",
    "#option_ind=1\n",
    "#for ticker_ind in range(len(ticker_list)):\n",
    "#    response=build_y(ask_low_time_list[ticker_ind][time_ind],bid_high_time_list[ticker_ind][time_ind],\\\n",
    "#                                 no_arbi_time_list[ticker_ind][time_ind],option=option_ind)\n",
    "#    np.savetxt(path_save+ticker_list[ticker_ind]+'_response.txt',response)\n",
    "\n",
    "\n",
    "\n",
    "## load y data\n",
    "#%%\n",
    "response_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    response_list.append((np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_response.txt',header=-1))))\n",
    "\n",
    "\n",
    "## print the shape of the response\n",
    "## note it is the total response\n",
    "#%%\n",
    "print(\"The shape of the total response is:\\n\")\n",
    "\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_list[ticker_ind].shape)\n",
    "\n",
    "# need to get the response from 10 to 15:30\n",
    "# the shape of the response and the feature array should be equal\n",
    "response_reduced_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    first_ind = np.where(time_index_list[ticker_ind]>=start_ind)[0][0]\n",
    "    last_ind=np.where(time_index_list[ticker_ind]<=end_ind)[0][-1]\n",
    "    response_reduced_list.append(response_list[ticker_ind][first_ind:last_ind+1])\n",
    "\n",
    "print(\"The shape of the reduced response is:\\n\")\n",
    "\n",
    "## print the shape of reduced response\n",
    "## response reduced is used for testing and training the model\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_reduced_list[ticker_ind].shape)\n",
    "\n",
    "    \n",
    "    # random generate a given \n",
    "def random_choice(num, key):\n",
    "    temp=np.random.choice(num,size=key,replace=False)\n",
    "    temp_sort=sorted(temp)\n",
    "    for i in range(len(temp)):\n",
    "        num[temp_sort[i]]=temp[i]\n",
    "    \n",
    "    return num\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.train and test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# Random split\n",
    "#%%---------------------------------------------------------------------\n",
    "import random\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "ticker_ind=1\n",
    "size=100000\n",
    "\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x, test_x, train_y, test_y =train_test_split(\\\n",
    "total_array[:,:134],total_array[:,134], test_size=0.1, random_state=42)\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.,   100.,   200.,   300.,   400.,   500.,   600.,   700.,\n",
       "         800.,   900.,  1000.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0,1000,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total array shape: (100000, 135)\n",
      "train_x shape: (90000, 134)\n",
      "test_x shape: (10000, 134)\n",
      "test_y shape: (10000,)\n",
      "train_y shape: (90000,)\n",
      "38.220980644226074\n",
      "train_accuracy is: 0.989366666667\n",
      "precision is: \t 0.808894230769\n",
      "recall is: \t 0.999257609503\n",
      "f1 score is: \t 0.894055131186\n",
      "test time is:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.11334586143493652\n",
      "test accuracy is: 0.9883\n",
      "precision is: \t 0.681318681319\n",
      "recall is: \t 0.995983935743\n",
      "f1 score is: \t 0.809135399674\n",
      "total array shape: (120000, 135)\n",
      "train_x shape: (108000, 134)\n",
      "test_x shape: (12000, 134)\n",
      "test_y shape: (12000,)\n",
      "train_y shape: (108000,)\n",
      "48.89140748977661\n",
      "train_accuracy is: 0.985194444444\n",
      "precision is: \t 0.717767573296\n",
      "recall is: \t 0.99975399754\n",
      "f1 score is: \t 0.835612213427\n",
      "test time is: 0.15018796920776367\n",
      "test accuracy is: 0.97975\n",
      "precision is: \t 0.497933884298\n",
      "recall is: \t 1.0\n",
      "f1 score is: \t 0.664827586207\n",
      "total array shape: (140000, 135)\n",
      "train_x shape: (126000, 134)\n",
      "test_x shape: (14000, 134)\n",
      "test_y shape: (14000,)\n",
      "train_y shape: (126000,)\n",
      "57.72841024398804\n",
      "train_accuracy is: 0.984841269841\n",
      "precision is: \t 0.710996667676\n",
      "recall is: \t 0.999574105622\n",
      "f1 score is: \t 0.830943529828\n",
      "test time is:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.1622474193572998\n",
      "test accuracy is: 0.981642857143\n",
      "precision is: \t 0.6825\n",
      "recall is: \t 0.994535519126\n",
      "f1 score is: \t 0.809488510007\n",
      "total array shape: (160000, 135)\n",
      "train_x shape: (144000, 134)\n",
      "test_x shape: (16000, 134)\n",
      "test_y shape: (16000,)\n",
      "train_y shape: (144000,)\n",
      "67.78393459320068\n",
      "train_accuracy is: 0.984229166667\n",
      "precision is: \t 0.693140305777\n",
      "recall is: \t 0.999414748342\n",
      "f1 score is: \t 0.818566749221\n",
      "test time is: 0.1854867935180664\n",
      "test accuracy is: 0.980875\n",
      "precision is: \t 0.564285714286\n",
      "recall is: \t 0.997474747475\n",
      "f1 score is: \t 0.720802919708\n",
      "total array shape: (180000, 135)\n",
      "train_x shape: (162000, 134)\n",
      "test_x shape: (18000, 134)\n",
      "test_y shape: (18000,)\n",
      "train_y shape: (162000,)\n",
      "76.31377649307251\n",
      "train_accuracy is: 0.983469135802\n",
      "precision is: \t 0.664113510799\n",
      "recall is: \t 0.999433106576\n",
      "f1 score is: \t 0.797978273989\n",
      "test time is: 0.20574665069580078\n",
      "test accuracy is: 0.982277777778\n",
      "precision is: \t 0.596153846154"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall is: \t 0.991471215352\n",
      "f1 score is: \t 0.744595676541\n",
      "total array shape: (200000, 135)\n",
      "train_x shape: (180000, 134)\n",
      "test_x shape: (20000, 134)\n",
      "test_y shape: (20000,)\n",
      "train_y shape: (180000,)\n",
      "85.83449625968933\n",
      "train_accuracy is: 0.983405555556\n",
      "precision is: \t 0.653935453912\n",
      "recall is: \t 0.998935981557\n",
      "f1 score is: \t 0.790430084894\n",
      "test time is: 0.22883081436157227\n",
      "test accuracy is: 0.98525\n",
      "precision is: \t 0.658851113716\n",
      "recall is: \t 0.992932862191\n",
      "f1 score is: \t 0.792107117689\n",
      "total array shape: (220000, 135)\n",
      "train_x shape: (198000, 134)\n",
      "test_x shape: (22000, 134)\n",
      "test_y shape: (22000,)\n",
      "train_y shape: (198000,)\n",
      "96.60502362251282\n",
      "train_accuracy is: 0.983373737374\n",
      "precision is: \t 0.65035634507\n",
      "recall is: \t 0.999182873018\n",
      "f1 score is: \t 0.787886597938\n",
      "test time is: 0.24707651138305664\n",
      "test accuracy is: 0.979818181818\n",
      "precision is: \t 0.592558139535"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall is: \t 0.99066874028\n",
      "f1 score is: \t 0.741559953434\n",
      "total array shape: (240000, 135)\n",
      "train_x shape: (216000, 134)\n",
      "test_x shape: (24000, 134)\n",
      "test_y shape: (24000,)\n",
      "train_y shape: (216000,)\n",
      "107.49977731704712\n",
      "train_accuracy is: 0.982657407407\n",
      "precision is: \t 0.643917761279\n",
      "recall is: \t 0.999261447563\n",
      "f1 score is: \t 0.783167399861\n",
      "test time is: 0.2632150650024414\n",
      "test accuracy is: 0.978541666667\n",
      "precision is: \t 0.617339312407\n",
      "recall is: \t 0.996381182147\n",
      "f1 score is: \t 0.76234425473\n",
      "total array shape: (260000, 135)\n",
      "train_x shape: (234000, 134)\n",
      "test_x shape: (26000, 134)\n",
      "test_y shape: (26000,)\n",
      "train_y shape: (234000,)\n",
      "122.68259739875793\n",
      "train_accuracy is: 0.981675213675\n",
      "precision is: \t 0.636579572447\n",
      "recall is: \t 0.999467234949\n",
      "f1 score is: \t 0.777777777778\n",
      "test time is: 0.32149696350097656\n",
      "test accuracy is:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.980038461538\n",
      "precision is: \t 0.66774611399\n",
      "recall is: \t 0.994214079074\n",
      "f1 score is: \t 0.798915149167\n",
      "total array shape: (280000, 135)\n",
      "train_x shape: (252000, 134)\n",
      "test_x shape: (28000, 134)\n",
      "test_y shape: (28000,)\n",
      "train_y shape: (252000,)\n",
      "142.228191614151\n",
      "train_accuracy is: 0.980698412698\n",
      "precision is: \t 0.624149133663\n",
      "recall is: \t 0.999380728264\n",
      "f1 score is: \t 0.768403009237\n",
      "test time is: 0.3419487476348877\n",
      "test accuracy is: 0.974\n",
      "precision is: \t 0.511800404585\n",
      "recall is: \t 0.994757536042\n",
      "f1 score is: \t 0.675868210151\n",
      "total array shape: (300000, 135)\n",
      "train_x shape: (270000, 134)\n",
      "test_x shape: (30000, 134)\n",
      "test_y shape: (30000,)\n",
      "train_y shape: (270000,)\n",
      "155.08946704864502\n",
      "train_accuracy is: 0.974377777778\n",
      "precision is: \t 0.532860520095\n",
      "recall is: \t 0.999746546699\n",
      "f1 score is: \t 0.695188579485\n",
      "test time is: 0.3699636459350586\n",
      "test accuracy is: 0.974566666667\n",
      "precision is: \t 0.649122807018\n",
      "recall is: \t 0.997870830376\n",
      "f1 score is: \t 0.786573426573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# data_stability test\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "ticker_ind=1\n",
    "f1_list=[]\n",
    "size_list=np.linspace(100000,300000,11)\n",
    "random_ratio=0.7\n",
    "\n",
    "for size in size_list:\n",
    "    \n",
    "# combine the feature and response array to random sample\n",
    "    total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "    total_array=total_array[random_choice(list(range(int(size))),int(size*random_ratio)),:]\n",
    "\n",
    "\n",
    "    train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "    print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "    #split the data to train and test data set\n",
    "    train_x=total_array[:train_num_index,:134]\n",
    "    test_x=total_array[train_num_index:,:134]\n",
    "    train_y=total_array[:train_num_index,134]\n",
    "    test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "    # the y data need to reshape to size (n,) not (n,1)\n",
    "    test_y=test_y.reshape(len(test_y),)\n",
    "    train_y=train_y.reshape(len(train_y),)\n",
    "    print(\"train_x shape:\",train_x.shape)\n",
    "    print(\"test_x shape:\",test_x.shape)\n",
    "    print(\"test_y shape:\",test_y.shape)\n",
    "    print(\"train_y shape:\",train_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # scale data\n",
    "    #%%\n",
    "\n",
    "    # can use the processing.scale function to scale the data\n",
    "    from sklearn import preprocessing\n",
    "    # note that we need to transfer the data type to float\n",
    "    # remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "    # use scale for zero mean and one std\n",
    "    scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "    train_x_scale=scaler.transform(train_x)\n",
    "    test_x_scale=scaler.transform(test_x)\n",
    "    \n",
    "\n",
    "    # training\n",
    "\n",
    "    # change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "    time_rf=time.time()\n",
    "    clf =  RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345)\n",
    "    clf.fit(train_x_scale,train_y)\n",
    "\n",
    "    print(time.time()-time_rf)\n",
    "\n",
    "    #testing5\n",
    "    # test the training error\n",
    "    predict_y=np.array(clf.predict(train_x_scale))\n",
    "    print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "    # test the score for the train data\n",
    "    from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                                 f1_score)\n",
    "    precision= precision_score(predict_y,train_y)\n",
    "    recall = recall_score(predict_y,train_y)\n",
    "    f1=f1_score(predict_y,train_y)\n",
    "    print(\"precision is: \\t %s\" % precision)\n",
    "    print(\"recall is: \\t %s\" % recall)\n",
    "    print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "    # define a function to prefict the result by threshold\n",
    "    # note: logistic model will return two probability\n",
    "    def predict_threshold(predict_proba, threshold):\n",
    "        res=[]\n",
    "        for i in range(len(predict_proba)):\n",
    "            res.append(int(predict_proba[i][1]>threshold))\n",
    "        return res\n",
    "\n",
    "    t=time.time()\n",
    "    predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "    print(\"test time is:\", time.time()-t)\n",
    "    predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "    # test the score for the test data\n",
    "    from sklearn.metrics import (precision_score, recall_score,\n",
    "                                 f1_score)\n",
    "    print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "    precision= precision_score(predict_y_test,test_y)\n",
    "    recall = recall_score(predict_y_test,test_y)\n",
    "    f1=f1_score(predict_y_test,test_y)\n",
    "    print(\"precision is: \\t %s\" % precision)\n",
    "    print(\"recall is: \\t %s\" % recall)\n",
    "    print(\"f1 score is: \\t %s\" %f1)\n",
    "    f1_list.append(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.80913539967373571,\n",
       " 0.66482758620689653,\n",
       " 0.80948851000741295,\n",
       " 0.72080291970802912,\n",
       " 0.74459567654123304,\n",
       " 0.79210711768851305,\n",
       " 0.74155995343422587,\n",
       " 0.76234425473004164,\n",
       " 0.79891514916698969,\n",
       " 0.67586821015138032,\n",
       " 0.78657342657342666]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/matplotlib/axes/_axes.py:519: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "# draw the chart for the f1 score\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(size_list,f1_list,\"b.-\")\n",
    "plt.xlabel(\"Data NUmber\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.legend(bbox_to_anchor=[1.4, 1])\n",
    "plt.title(\"Data sample stability for \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"data_sample_stability_\"+ticker_list[ticker_ind]+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total array shape: (200000, 135)\n",
      "train_x shape: (180000, 134)\n",
      "test_x shape: (20000, 134)\n",
      "test_y shape: (20000,)\n",
      "train_y shape: (180000,)\n",
      "[ -2.49e-14  -6.84e-15   8.16e-15  -2.93e-15   5.65e-14  -8.17e-15\n",
      "   7.91e-16  -3.78e-15  -9.80e-15   5.28e-15  -1.78e-14  -2.89e-15\n",
      "  -1.95e-14   6.96e-16  -4.47e-15   1.40e-16   1.16e-14   5.49e-15\n",
      "  -4.54e-15   1.54e-16  -8.17e-15   5.94e-15   1.69e-14  -2.13e-15\n",
      "   1.37e-14   1.71e-15   1.05e-14  -3.25e-16  -1.27e-14  -4.07e-15\n",
      "  -1.04e-14  -3.04e-15  -1.31e-14  -3.29e-16  -1.77e-14   2.82e-15\n",
      "  -1.84e-14   7.14e-15   1.50e-14  -2.07e-15   8.03e-15  -1.01e-14\n",
      "   1.20e-14   6.82e-14   3.98e-14  -5.62e-15   4.73e-14  -3.89e-14\n",
      "  -3.96e-14   2.38e-14   1.19e-14  -9.37e-15  -1.80e-14  -1.04e-14\n",
      "   2.05e-15   3.73e-15  -1.94e-14   1.57e-14  -8.15e-15  -1.37e-14\n",
      "   4.79e-14  -2.62e-14   2.36e-15  -5.17e-14  -2.14e-14  -5.17e-14\n",
      "   1.32e-13  -1.19e-13   7.01e-14  -2.75e-14   3.90e-14  -7.57e-14\n",
      "  -1.70e-13   1.30e-13   7.90e-14   1.06e-13  -1.60e-13  -1.89e-13\n",
      "  -1.38e-15   2.09e-14  -1.86e-13  -5.00e-14  -1.61e-15   1.33e-17\n",
      "   3.66e-15  -7.45e-15  -9.37e-15   6.72e-15  -4.39e-16  -4.24e-15\n",
      "   2.78e-15  -1.71e-15   3.79e-15   2.22e-15  -4.03e-15   7.97e-15\n",
      "  -3.31e-15  -8.21e-15  -3.56e-15   1.75e-15   5.37e-15  -2.99e-15\n",
      "  -4.16e-15   2.41e-15  -1.19e-14  -1.37e-15  -8.00e-15  -3.71e-16\n",
      "   4.21e-16   1.75e-15   5.65e-15   9.68e-15   1.81e-15   1.41e-15\n",
      "   6.05e-15   1.35e-15   3.22e-15  -1.28e-15   2.54e-15  -3.35e-15\n",
      "  -5.45e-15  -2.64e-16   4.58e-15   3.33e-15  -2.47e-15   8.62e-16\n",
      "   1.76e-14  -2.47e-15   1.36e-14   1.38e-15  -1.93e-15   2.58e-16\n",
      "  -2.59e-15   1.85e-15]\n",
      "[ -4.34e-01   4.06e-02  -4.34e-01  -3.42e-02  -4.34e-01   1.00e-02\n",
      "  -4.34e-01  -2.02e-02  -4.35e-01  -3.75e-02  -4.34e-01  -1.97e-02\n",
      "  -4.35e-01  -1.87e-02  -4.33e-01  -8.33e-03  -4.34e-01  -2.04e-02\n",
      "  -4.33e-01  -1.58e-02  -4.35e-01  -3.51e-02  -4.33e-01  -2.81e-02\n",
      "  -4.35e-01  -3.26e-02  -4.32e-01  -3.16e-02  -4.35e-01   3.66e-03\n",
      "  -4.32e-01  -1.35e-02  -4.35e-01   1.34e-02  -4.31e-01  -1.82e-02\n",
      "  -4.35e-01   2.51e-02  -4.31e-01  -1.64e-02  -3.32e-02  -9.06e-02\n",
      "  -1.39e-01  -1.64e-01  -1.80e-01  -1.94e-01  -2.04e-01  -2.11e-01\n",
      "  -2.15e-01  -2.20e-01  -4.34e-01  -4.34e-01  -4.34e-01  -4.34e-01\n",
      "  -4.34e-01  -4.34e-01  -4.34e-01  -4.33e-01  -4.33e-01  -4.33e-01\n",
      "  -5.73e-02  -7.21e-02  -4.81e-02  -6.64e-02  -9.83e-02  -9.62e-02\n",
      "  -9.57e-02  -9.75e-02  -1.02e-01  -4.25e-02  -6.60e-02  -8.96e-02\n",
      "  -9.40e-02  -9.24e-02  -1.04e-01  -9.88e-02  -8.54e-02  -9.62e-02\n",
      "  -4.35e-01  -4.33e-01  -1.49e-02  -6.47e-02  -1.96e-01   5.89e-02\n",
      "   1.10e-02   2.18e-02   2.35e-02   2.23e-02   2.33e-02   2.31e-02\n",
      "   2.76e-02   2.17e-02   2.26e-02   2.41e-02   2.05e-02   4.57e-03\n",
      "   9.11e-03   8.79e-03   1.07e-02   1.01e-02   6.72e-03   1.10e-02\n",
      "   7.54e-03   8.56e-03   1.08e-02  -1.41e-02  -5.03e-03  -9.58e-03\n",
      "  -2.07e-02  -2.56e-03   6.12e-03   1.49e-02  -1.82e-02   2.29e-03\n",
      "  -6.82e-03  -1.49e-03  -4.02e-03  -3.91e-04  -8.66e-03  -6.84e-04\n",
      "   9.17e-03   7.79e-04   1.05e-02  -1.41e-02   3.31e-03   1.02e-02\n",
      "   3.07e-02  -1.60e-02   1.69e-02   6.80e-03  -9.01e-03  -1.27e-03\n",
      "   2.80e-02  -9.13e-03]\n"
     ]
    }
   ],
   "source": [
    "#time series split\n",
    "#%%--------------------------------------------------------------------------------------------\n",
    "\n",
    "ticker_ind=1\n",
    "size=200000\n",
    "random_ratio=0.7\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "total_array=total_array[random_choice(list(range(size)),int(size*random_ratio)),:]\n",
    "\n",
    "\n",
    "train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x=total_array[:train_num_index,:134]\n",
    "test_x=total_array[train_num_index:,:134]\n",
    "train_y=total_array[:train_num_index,134]\n",
    "test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "print(\"train_x shape:\",train_x.shape)\n",
    "print(\"test_x shape:\",test_x.shape)\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "# scale data\n",
    "#%%\n",
    "\n",
    "# can use the processing.scale function to scale the data\n",
    "from sklearn import preprocessing\n",
    "# note that we need to transfer the data type to float\n",
    "# remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "# use scale for zero mean and one std\n",
    "scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "train_x_scale=scaler.transform(train_x)\n",
    "test_x_scale=scaler.transform(test_x)\n",
    "\n",
    "print(np.mean(train_x_scale,0))\n",
    "print(np.mean(test_x_scale,0))\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 239627,\n",
       " 15142,\n",
       " 160838,\n",
       " 154407,\n",
       " 264244,\n",
       " 138646,\n",
       " 200007,\n",
       " 101229,\n",
       " 197488,\n",
       " 188375,\n",
       " 12,\n",
       " 257201,\n",
       " 187708,\n",
       " 185246,\n",
       " 202697,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 30325,\n",
       " 161726,\n",
       " 98310,\n",
       " 23,\n",
       " 296024,\n",
       " 136810,\n",
       " 283155,\n",
       " 34003,\n",
       " 158462,\n",
       " 29,\n",
       " 30,\n",
       " 64780,\n",
       " 32,\n",
       " 14320,\n",
       " 34,\n",
       " 167159,\n",
       " 259438,\n",
       " 256405,\n",
       " 79632,\n",
       " 39,\n",
       " 40,\n",
       " 53202,\n",
       " 122895,\n",
       " 43,\n",
       " 44,\n",
       " 166565,\n",
       " 24570,\n",
       " 114136,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 138225,\n",
       " 257704,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 120476,\n",
       " 164430,\n",
       " 274902,\n",
       " 59,\n",
       " 297161,\n",
       " 82603,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 31989,\n",
       " 279281,\n",
       " 67,\n",
       " 125159,\n",
       " 158339,\n",
       " 241930,\n",
       " 84104,\n",
       " 29542,\n",
       " 33420,\n",
       " 116146,\n",
       " 195439,\n",
       " 76899,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 265771,\n",
       " 81,\n",
       " 82,\n",
       " 219886,\n",
       " 26214,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 157342,\n",
       " 290573,\n",
       " 180637,\n",
       " 161258,\n",
       " 92,\n",
       " 166363,\n",
       " 86596,\n",
       " 112799,\n",
       " 96,\n",
       " 77990,\n",
       " 297662,\n",
       " 108625,\n",
       " 291809,\n",
       " 99378,\n",
       " 174615,\n",
       " 101238,\n",
       " 121936,\n",
       " 108803,\n",
       " 42284,\n",
       " 107,\n",
       " 124,\n",
       " 226506,\n",
       " 50760,\n",
       " 280461,\n",
       " 97866,\n",
       " 82374,\n",
       " 154086,\n",
       " 48675,\n",
       " 116,\n",
       " 58257,\n",
       " 74324,\n",
       " 50382,\n",
       " 142831,\n",
       " 219194,\n",
       " 279126,\n",
       " 134541,\n",
       " 129709,\n",
       " 105055,\n",
       " 147085,\n",
       " 38115,\n",
       " 167886,\n",
       " 139876,\n",
       " 83281,\n",
       " 215006,\n",
       " 103189,\n",
       " 131793,\n",
       " 134,\n",
       " 143144,\n",
       " 12701,\n",
       " 184241,\n",
       " 111322,\n",
       " 114694,\n",
       " 230525,\n",
       " 141,\n",
       " 27779,\n",
       " 198550,\n",
       " 144,\n",
       " 146265,\n",
       " 297870,\n",
       " 94358,\n",
       " 148,\n",
       " 270694,\n",
       " 298665,\n",
       " 151,\n",
       " 152,\n",
       " 275614,\n",
       " 18931,\n",
       " 279916,\n",
       " 23905,\n",
       " 146232,\n",
       " 158,\n",
       " 212130,\n",
       " 31422,\n",
       " 161,\n",
       " 52621,\n",
       " 117204,\n",
       " 239706,\n",
       " 165,\n",
       " 263357,\n",
       " 140905,\n",
       " 32355,\n",
       " 151398,\n",
       " 170,\n",
       " 171,\n",
       " 163059,\n",
       " 32366,\n",
       " 238339,\n",
       " 175,\n",
       " 221053,\n",
       " 28794,\n",
       " 190983,\n",
       " 87816,\n",
       " 219152,\n",
       " 181,\n",
       " 182,\n",
       " 218144,\n",
       " 184,\n",
       " 178693,\n",
       " 186,\n",
       " 250602,\n",
       " 164802,\n",
       " 217126,\n",
       " 291400,\n",
       " 165624,\n",
       " 134729,\n",
       " 193,\n",
       " 194,\n",
       " 238446,\n",
       " 163450,\n",
       " 240953,\n",
       " 198,\n",
       " 205824,\n",
       " 12935,\n",
       " 201,\n",
       " 212273,\n",
       " 203,\n",
       " 204,\n",
       " 215941,\n",
       " 228017,\n",
       " 163062,\n",
       " 289133,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 271747,\n",
       " 111762,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 107830,\n",
       " 189069,\n",
       " 230259,\n",
       " 49553,\n",
       " 283586,\n",
       " 272630,\n",
       " 4285,\n",
       " 177002,\n",
       " 225,\n",
       " 226,\n",
       " 251558,\n",
       " 50454,\n",
       " 78377,\n",
       " 56985,\n",
       " 231,\n",
       " 232,\n",
       " 76207,\n",
       " 275808,\n",
       " 235,\n",
       " 61438,\n",
       " 56393,\n",
       " 265477,\n",
       " 200783,\n",
       " 298547,\n",
       " 2322,\n",
       " 253049,\n",
       " 243,\n",
       " 244,\n",
       " 80409,\n",
       " 299562,\n",
       " 278060,\n",
       " 3257,\n",
       " 181919,\n",
       " 250,\n",
       " 115691,\n",
       " 252,\n",
       " 69600,\n",
       " 254,\n",
       " 230056,\n",
       " 247587,\n",
       " 107220,\n",
       " 258,\n",
       " 259,\n",
       " 92672,\n",
       " 11452,\n",
       " 262,\n",
       " 263,\n",
       " 76994,\n",
       " 255036,\n",
       " 173951,\n",
       " 267,\n",
       " 218448,\n",
       " 269,\n",
       " 56982,\n",
       " 271,\n",
       " 272,\n",
       " 212092,\n",
       " 277579,\n",
       " 80777,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 102723,\n",
       " 85609,\n",
       " 90890,\n",
       " 282,\n",
       " 193485,\n",
       " 284,\n",
       " 35879,\n",
       " 286,\n",
       " 149878,\n",
       " 51218,\n",
       " 289,\n",
       " 290,\n",
       " 144625,\n",
       " 198516,\n",
       " 114461,\n",
       " 191568,\n",
       " 295,\n",
       " 207036,\n",
       " 297,\n",
       " 96418,\n",
       " 187880,\n",
       " 300,\n",
       " 144511,\n",
       " 302,\n",
       " 217446,\n",
       " 304,\n",
       " 183064,\n",
       " 111267,\n",
       " 189928,\n",
       " 104889,\n",
       " 309,\n",
       " 188438,\n",
       " 237501,\n",
       " 276214,\n",
       " 166869,\n",
       " 29004,\n",
       " 298011,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 25530,\n",
       " 286584,\n",
       " 321,\n",
       " 206972,\n",
       " 169487,\n",
       " 239077,\n",
       " 299306,\n",
       " 225486,\n",
       " 148505,\n",
       " 141190,\n",
       " 329,\n",
       " 246677,\n",
       " 157763,\n",
       " 332,\n",
       " 2163,\n",
       " 169345,\n",
       " 264208,\n",
       " 125016,\n",
       " 234876,\n",
       " 107045,\n",
       " 229038,\n",
       " 136031,\n",
       " 22977,\n",
       " 9318,\n",
       " 343,\n",
       " 44156,\n",
       " 266450,\n",
       " 251060,\n",
       " 228413,\n",
       " 32306,\n",
       " 160165,\n",
       " 71522,\n",
       " 127940,\n",
       " 70879,\n",
       " 353,\n",
       " 354,\n",
       " 289456,\n",
       " 356,\n",
       " 151476,\n",
       " 358,\n",
       " 16876,\n",
       " 87846,\n",
       " 29528,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 78224,\n",
       " 55919,\n",
       " 149222,\n",
       " 156841,\n",
       " 370,\n",
       " 262128,\n",
       " 278830,\n",
       " 6753,\n",
       " 95426,\n",
       " 27922,\n",
       " 150309,\n",
       " 133738,\n",
       " 378,\n",
       " 76506,\n",
       " 78491,\n",
       " 182676,\n",
       " 175717,\n",
       " 99338,\n",
       " 290314,\n",
       " 385,\n",
       " 134874,\n",
       " 387,\n",
       " 282882,\n",
       " 166610,\n",
       " 62377,\n",
       " 162348,\n",
       " 244976,\n",
       " 393,\n",
       " 211858,\n",
       " 272160,\n",
       " 396,\n",
       " 397,\n",
       " 155300,\n",
       " 157041,\n",
       " 400,\n",
       " 165804,\n",
       " 402,\n",
       " 403,\n",
       " 217330,\n",
       " 102178,\n",
       " 34747,\n",
       " 168107,\n",
       " 157478,\n",
       " 244416,\n",
       " 70887,\n",
       " 190246,\n",
       " 412,\n",
       " 120617,\n",
       " 262030,\n",
       " 9543,\n",
       " 130500,\n",
       " 153703,\n",
       " 7521,\n",
       " 204557,\n",
       " 420,\n",
       " 120574,\n",
       " 150757,\n",
       " 20750,\n",
       " 424,\n",
       " 224494,\n",
       " 88012,\n",
       " 202889,\n",
       " 69118,\n",
       " 290081,\n",
       " 102041,\n",
       " 431,\n",
       " 57717,\n",
       " 36383,\n",
       " 434,\n",
       " 166483,\n",
       " 436,\n",
       " 19415,\n",
       " 111455,\n",
       " 439,\n",
       " 111855,\n",
       " 441,\n",
       " 29420,\n",
       " 64222,\n",
       " 39541,\n",
       " 257379,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 950,\n",
       " 95836,\n",
       " 70659,\n",
       " 285706,\n",
       " 35808,\n",
       " 264001,\n",
       " 34562,\n",
       " 8595,\n",
       " 457,\n",
       " 458,\n",
       " 160772,\n",
       " 281840,\n",
       " 269149,\n",
       " 462,\n",
       " 218959,\n",
       " 23237,\n",
       " 121638,\n",
       " 466,\n",
       " 79590,\n",
       " 132322,\n",
       " 469,\n",
       " 31933,\n",
       " 284382,\n",
       " 472,\n",
       " 473,\n",
       " 118186,\n",
       " 150614,\n",
       " 476,\n",
       " 48245,\n",
       " 24477,\n",
       " 214207,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 199089,\n",
       " 37780,\n",
       " 485,\n",
       " 295762,\n",
       " 487,\n",
       " 488,\n",
       " 97660,\n",
       " 180665,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 197991,\n",
       " 294116,\n",
       " 11688,\n",
       " 221020,\n",
       " 498,\n",
       " 152291,\n",
       " 185988,\n",
       " 162680,\n",
       " 291606,\n",
       " 32191,\n",
       " 59999,\n",
       " 254939,\n",
       " 189696,\n",
       " 40157,\n",
       " 149095,\n",
       " 509,\n",
       " 244981,\n",
       " 511,\n",
       " 281828,\n",
       " 513,\n",
       " 114635,\n",
       " 515,\n",
       " 153253,\n",
       " 517,\n",
       " 142010,\n",
       " 149697,\n",
       " 520,\n",
       " 521,\n",
       " 166814,\n",
       " 523,\n",
       " 13312,\n",
       " 157408,\n",
       " 113671,\n",
       " 251198,\n",
       " 77993,\n",
       " 529,\n",
       " 2364,\n",
       " 531,\n",
       " 298719,\n",
       " 5569,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 32819,\n",
       " 256250,\n",
       " 87260,\n",
       " 540,\n",
       " 233882,\n",
       " 542,\n",
       " 118462,\n",
       " 544,\n",
       " 69247,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 24777,\n",
       " 87805,\n",
       " 210044,\n",
       " 115626,\n",
       " 119,\n",
       " 114563,\n",
       " 104213,\n",
       " 54690,\n",
       " 293455,\n",
       " 558,\n",
       " 559,\n",
       " 4936,\n",
       " 201512,\n",
       " 36180,\n",
       " 208786,\n",
       " 259466,\n",
       " 565,\n",
       " 566,\n",
       " 157364,\n",
       " 121580,\n",
       " 27368,\n",
       " 43548,\n",
       " 136892,\n",
       " 13241,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 191346,\n",
       " 137116,\n",
       " 77822,\n",
       " 579,\n",
       " 13199,\n",
       " 581,\n",
       " 26069,\n",
       " 583,\n",
       " 129678,\n",
       " 256608,\n",
       " 29422,\n",
       " 92942,\n",
       " 588,\n",
       " 138850,\n",
       " 58863,\n",
       " 187156,\n",
       " 4891,\n",
       " 593,\n",
       " 26844,\n",
       " 172785,\n",
       " 82128,\n",
       " 283806,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 126969,\n",
       " 97997,\n",
       " 33602,\n",
       " 125029,\n",
       " 605,\n",
       " 214341,\n",
       " 607,\n",
       " 10771,\n",
       " 188440,\n",
       " 25152,\n",
       " 125525,\n",
       " 187003,\n",
       " 613,\n",
       " 95522,\n",
       " 236215,\n",
       " 214282,\n",
       " 617,\n",
       " 618,\n",
       " 207717,\n",
       " 620,\n",
       " 289830,\n",
       " 26053,\n",
       " 181417,\n",
       " 243999,\n",
       " 625,\n",
       " 60452,\n",
       " 98681,\n",
       " 628,\n",
       " 64810,\n",
       " 42093,\n",
       " 631,\n",
       " 173203,\n",
       " 110833,\n",
       " 634,\n",
       " 83528,\n",
       " 146925,\n",
       " 37544,\n",
       " 236157,\n",
       " 227896,\n",
       " 640,\n",
       " 641,\n",
       " 47437,\n",
       " 163857,\n",
       " 89936,\n",
       " 99435,\n",
       " 268145,\n",
       " 207274,\n",
       " 46824,\n",
       " 649,\n",
       " 121655,\n",
       " 243003,\n",
       " 652,\n",
       " 193802,\n",
       " 220070,\n",
       " 296032,\n",
       " 113562,\n",
       " 156140,\n",
       " 125750,\n",
       " 298754,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 31681,\n",
       " 664,\n",
       " 111344,\n",
       " 40542,\n",
       " 203293,\n",
       " 43606,\n",
       " 44728,\n",
       " 178484,\n",
       " 92403,\n",
       " 155229,\n",
       " 145957,\n",
       " 251460,\n",
       " 675,\n",
       " 676,\n",
       " 137091,\n",
       " 86723,\n",
       " 261717,\n",
       " 180000,\n",
       " 681,\n",
       " 11218,\n",
       " 264270,\n",
       " 229192,\n",
       " 685,\n",
       " 148531,\n",
       " 687,\n",
       " 688,\n",
       " 68042,\n",
       " 199513,\n",
       " 829,\n",
       " 692,\n",
       " 163487,\n",
       " 694,\n",
       " 149203,\n",
       " 31235,\n",
       " 49960,\n",
       " 698,\n",
       " 26075,\n",
       " 229449,\n",
       " 241473,\n",
       " 29146,\n",
       " 273697,\n",
       " 65307,\n",
       " 705,\n",
       " 294621,\n",
       " 19949,\n",
       " 708,\n",
       " 142493,\n",
       " 50838,\n",
       " 8335,\n",
       " 220568,\n",
       " 713,\n",
       " 99256,\n",
       " 715,\n",
       " 147058,\n",
       " 167797,\n",
       " 718,\n",
       " 236323,\n",
       " 123815,\n",
       " 721,\n",
       " 23545,\n",
       " 117290,\n",
       " 111645,\n",
       " 725,\n",
       " 158351,\n",
       " 251493,\n",
       " 76886,\n",
       " 729,\n",
       " 281431,\n",
       " 731,\n",
       " 27921,\n",
       " 733,\n",
       " 734,\n",
       " 11726,\n",
       " 126890,\n",
       " 139892,\n",
       " 16985,\n",
       " 156172,\n",
       " 241693,\n",
       " 293909,\n",
       " 742,\n",
       " 34423,\n",
       " 124009,\n",
       " 205002,\n",
       " 746,\n",
       " 159667,\n",
       " 748,\n",
       " 128472,\n",
       " 64003,\n",
       " 147310,\n",
       " 752,\n",
       " 205342,\n",
       " 754,\n",
       " 177557,\n",
       " 170981,\n",
       " 757,\n",
       " 244919,\n",
       " 58254,\n",
       " 760,\n",
       " 9606,\n",
       " 208740,\n",
       " 39779,\n",
       " 223904,\n",
       " 280550,\n",
       " 766,\n",
       " 299288,\n",
       " 282126,\n",
       " 113998,\n",
       " 276035,\n",
       " 137408,\n",
       " 772,\n",
       " 91819,\n",
       " 13046,\n",
       " 198642,\n",
       " 144811,\n",
       " 777,\n",
       " 3736,\n",
       " 162661,\n",
       " 97910,\n",
       " 90575,\n",
       " 782,\n",
       " 167484,\n",
       " 784,\n",
       " 785,\n",
       " 70406,\n",
       " 787,\n",
       " 99279,\n",
       " 145770,\n",
       " 38087,\n",
       " 3673,\n",
       " 792,\n",
       " 29989,\n",
       " 274583,\n",
       " 289493,\n",
       " 128021,\n",
       " 38109,\n",
       " 39805,\n",
       " 799,\n",
       " 266142,\n",
       " 72136,\n",
       " 802,\n",
       " 803,\n",
       " 133036,\n",
       " 29563,\n",
       " 39057,\n",
       " 70276,\n",
       " 149403,\n",
       " 15616,\n",
       " 86179,\n",
       " 275975,\n",
       " 143468,\n",
       " 70630,\n",
       " 278257,\n",
       " 270784,\n",
       " 236091,\n",
       " 178873,\n",
       " 114553,\n",
       " 99237,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 252413,\n",
       " 36781,\n",
       " 14631,\n",
       " 826,\n",
       " 827,\n",
       " 175155,\n",
       " 124830,\n",
       " 92699,\n",
       " 831,\n",
       " 55950,\n",
       " 833,\n",
       " 254594,\n",
       " 34873,\n",
       " 277783,\n",
       " 94894,\n",
       " 838,\n",
       " 839,\n",
       " 119139,\n",
       " 154663,\n",
       " 154815,\n",
       " 171568,\n",
       " 274351,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 208450,\n",
       " 849,\n",
       " 850,\n",
       " 57028,\n",
       " 256945,\n",
       " 53020,\n",
       " 854,\n",
       " 855,\n",
       " 46038,\n",
       " 156111,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 54486,\n",
       " 862,\n",
       " 863,\n",
       " 137613,\n",
       " 122383,\n",
       " 277621,\n",
       " 191736,\n",
       " 108025,\n",
       " 266892,\n",
       " 109863,\n",
       " 871,\n",
       " 872,\n",
       " 139918,\n",
       " 174294,\n",
       " 210911,\n",
       " 127624,\n",
       " 162707,\n",
       " 175470,\n",
       " 105585,\n",
       " 203589,\n",
       " 15555,\n",
       " 228386,\n",
       " 883,\n",
       " 884,\n",
       " 40876,\n",
       " 886,\n",
       " 887,\n",
       " 237333,\n",
       " 7190,\n",
       " 280937,\n",
       " 891,\n",
       " 77100,\n",
       " 210403,\n",
       " 35979,\n",
       " 44710,\n",
       " 122671,\n",
       " 897,\n",
       " 201563,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 178462,\n",
       " 139021,\n",
       " 905,\n",
       " 53351,\n",
       " 121764,\n",
       " 28686,\n",
       " 109060,\n",
       " 910,\n",
       " 128272,\n",
       " 239561,\n",
       " 13311,\n",
       " 244321,\n",
       " 915,\n",
       " 204540,\n",
       " 917,\n",
       " 173544,\n",
       " 225576,\n",
       " 244892,\n",
       " 19653,\n",
       " 922,\n",
       " 134271,\n",
       " 149193,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 151772,\n",
       " 109827,\n",
       " 121613,\n",
       " 932,\n",
       " 275452,\n",
       " 245740,\n",
       " 281189,\n",
       " 71997,\n",
       " 937,\n",
       " 186945,\n",
       " 281698,\n",
       " 234052,\n",
       " 941,\n",
       " 942,\n",
       " 49024,\n",
       " 19585,\n",
       " 945,\n",
       " 89590,\n",
       " 947,\n",
       " 948,\n",
       " 119761,\n",
       " 99918,\n",
       " 204935,\n",
       " 105840,\n",
       " 276443,\n",
       " 293976,\n",
       " 30111,\n",
       " 956,\n",
       " 274156,\n",
       " 156174,\n",
       " 278574,\n",
       " 215822,\n",
       " 961,\n",
       " 215882,\n",
       " 963,\n",
       " 139952,\n",
       " 163665,\n",
       " 260912,\n",
       " 125805,\n",
       " 968,\n",
       " 142527,\n",
       " 970,\n",
       " 971,\n",
       " 22731,\n",
       " 293316,\n",
       " 153401,\n",
       " 198253,\n",
       " 63314,\n",
       " 150111,\n",
       " 157999,\n",
       " 255129,\n",
       " 980,\n",
       " 141340,\n",
       " 119825,\n",
       " 89067,\n",
       " 164455,\n",
       " 127810,\n",
       " 50441,\n",
       " 151680,\n",
       " 153885,\n",
       " 255265,\n",
       " 187252,\n",
       " 116920,\n",
       " 256331,\n",
       " 218055,\n",
       " 98126,\n",
       " 995,\n",
       " 996,\n",
       " 124800,\n",
       " 998,\n",
       " 132084,\n",
       " ...]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_choice(list(range(size)),int(size*random_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.21855902671814\n",
      "train_accuracy is: 0.9958\n",
      "precision is: \t 0.802986638722\n",
      "recall is: \t 0.998696643858\n",
      "f1 score is: \t 0.890212024397\n",
      "test time is: 0.2569611072540283\n",
      "test accuracy is: 0.99515\n",
      "precision is: \t 0.781395348837\n",
      "recall is: \t 0.991150442478\n",
      "f1 score is: \t 0.873862158648\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "time_rf=time.time()\n",
    "clf =  RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-time_rf)\n",
    "\n",
    "#testing5\n",
    "# test the training error\n",
    "predict_y=np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "print(\"test time is:\", time.time()-t)\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "# test the score for the test data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[9825    0]\n",
      " [   0  175]]\n"
     ]
    }
   ],
   "source": [
    "## confusion matrix plot\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Model build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x_scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#----------------\n",
    "# logistic l1\n",
    "#-----------------\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "        \n",
    "        # set the random state to make sure that each time get the same results\n",
    "\n",
    "time_logistic=time.time()\n",
    "clf = linear_model.LogisticRegression(C=1, penalty='l1', tol=1e-6,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "time_logistic=time.time()-time_logistic    \n",
    "\n",
    "print(time_logistic)\n",
    "\n",
    "# test the training error\n",
    "predict_y_logistic =np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y_logistic==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y_logistic,train_y)\n",
    "recall = recall_score(predict_y_logistic,train_y)\n",
    "f1=f1_score(predict_y_logistic,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n",
    "#----------------\n",
    "# logistic l2\n",
    "#-----------------\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "        \n",
    "        # set the random state to make sure that each time get the same results\n",
    "\n",
    "time_logistic=time.time()\n",
    "clf = linear_model.LogisticRegression(C=1, penalty='l2', tol=1e-6,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "time_logistic=time.time()-time_logistic    \n",
    "\n",
    "print(time_logistic)\n",
    "\n",
    "# test the training error\n",
    "predict_y_logistic =np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y_logistic==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y_logistic,train_y)\n",
    "recall = recall_score(predict_y_logistic,train_y)\n",
    "f1=f1_score(predict_y_logistic,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------------------\n",
    "# SVM_poly_2\n",
    "#---------------------\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "\n",
    "import time \n",
    "from sklearn import svm\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf = svm.SVC(C=1.0,kernel='poly',degree=2,max_iter=5000,shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y =np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "#draw the crosstab chart\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#---------------\n",
    "# decision tree\n",
    "#-----------------\n",
    "\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  tree.DecisionTreeClassifier(max_depth=10,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y=np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "print(\"test time is:\", time.time()-t)\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "#draw the crosstab chart\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-59e42b63bcfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mtime_ada\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m987612345\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_scale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime_ada\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                 sample_weight)\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    460\u001b[0m         \"\"\"\n\u001b[0;32m    461\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    348\u001b[0m                                            max_leaf_nodes)\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#-----------------------------------------\n",
    "# Adaboost \n",
    "#-----------------------------------------\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "time_ada=time.time()\n",
    "clf =  AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10),n_estimators=100,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-time_ada)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y=np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "#draw the crosstab chart\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## see the feature_importances\n",
    "\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_x_scale[:100,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multi-class predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.201947450637817\n"
     ]
    }
   ],
   "source": [
    "## load the arbitrage time txt data\n",
    "\n",
    "ask_low_time_list=[]\n",
    "bid_high_time_list=[]\n",
    "no_arbi_time_list=[] \n",
    "time_list=[1,5,10,15,20]\n",
    "import time \n",
    "t=time.time()\n",
    "for ticker_ind in range(5):  \n",
    "    ask_low_time_list.append([])\n",
    "    bid_high_time_list.append([])\n",
    "    no_arbi_time_list.append([])\n",
    "    for time_ind in range(len(time_list)):\n",
    "        ask_low_time_list[ticker_ind].append(\n",
    "            np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_ask_low_time_'+str(time_list[time_ind])+'.txt',header=-1)))\n",
    "        bid_high_time_list[ticker_ind].append(\n",
    "            np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_bid_high_time_'+str(time_list[time_ind])+'.txt',header=-1)))\n",
    "        no_arbi_time_list[ticker_ind].append(\n",
    "            np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_no_arbi_time_'+str(time_list[time_ind])+'.txt',header=-1)))\n",
    "        \n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the total response is:\n",
      "\n",
      "(400236, 1)\n",
      "(269571, 1)\n",
      "(147766, 1)\n",
      "(622641, 1)\n",
      "(667701, 1)\n",
      "The shape of the reduced response is:\n",
      "\n",
      "(309538, 1)\n",
      "(218710, 1)\n",
      "(118877, 1)\n",
      "(458160, 1)\n",
      "(511299, 1)\n"
     ]
    }
   ],
   "source": [
    "# Deal with the data\n",
    "def build_y(ask_low,bid_high,no_arbi,option):\n",
    "    if (option==1):\n",
    "        return ask_low\n",
    "    elif option==2:\n",
    "        return bid_high\n",
    "    elif option==3:\n",
    "        return no_arbi\n",
    "    elif option==4:\n",
    "        return ask_low-bid_high\n",
    "    else:\n",
    "        print(\"option should be 1,2,3,4\")\n",
    "        \n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    response=build_y(ask_low_time_list[ticker_ind][1],bid_high_time_list[ticker_ind][1],\\\n",
    "                                 no_arbi_time_list[ticker_ind][1],option=4)\n",
    "    np.savetxt(path_save+ticker_list[ticker_ind]+'_multiresponse.txt',response)\n",
    "\n",
    "response_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    response_list.append((np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_multiresponse.txt',header=-1))))\n",
    "\n",
    "    ## print the shape of the response\n",
    "## note it is the total response\n",
    "print(\"The shape of the total response is:\\n\")\n",
    "\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_list[ticker_ind].shape)\n",
    "    \n",
    "# need to get the response from 10 to 15:30\n",
    "# the shape of the response and the feature array should be equal \n",
    "response_reduced_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    first_ind = np.where(time_index_list[ticker_ind]>=start_ind)[0][0]\n",
    "    last_ind=np.where(time_index_list[ticker_ind]<=end_ind)[0][-1]\n",
    "    response_reduced_list.append(response_list[ticker_ind][first_ind:last_ind+1])\n",
    "    \n",
    "print(\"The shape of the reduced response is:\\n\")\n",
    "\n",
    "## print the shape of reduced response\n",
    "## response reduced is used for testing and training the model\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_reduced_list[ticker_ind].shape)\n",
    "    # random split data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random split\n",
    "#split the data to train and test data set\n",
    "import random\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "ticker_ind=1\n",
    "size=100000\n",
    "\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "\n",
    "print(\"total shape:\",total_array.shape)\n",
    "\n",
    "train_x, test_x, train_y, test_y =train_test_split(\\\n",
    "total_array[:,:134],total_array[:,134], test_size=0.1, random_state=42)\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "\n",
    "print(\"test shape:\",test_y.shape)\n",
    "print(\"train shape:\",train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total array shape: (100000, 136)\n",
      "train_x shape: (90000, 134)\n",
      "test_x shape: (10000, 134)\n",
      "test_y shape: (10000,)\n",
      "train_y shape: (90000,)\n",
      "[  1.47e-14   1.59e-14   1.94e-14  -1.54e-14  -2.02e-14  -2.04e-14\n",
      "  -2.54e-15  -5.21e-15   3.67e-14   9.90e-15  -2.27e-14   1.67e-15\n",
      "   3.38e-14   9.92e-15  -2.45e-14  -7.93e-15   2.51e-14  -1.43e-14\n",
      "   2.77e-15   2.77e-15  -1.94e-14  -4.49e-15   1.18e-14  -5.56e-15\n",
      "   7.73e-15  -2.83e-14  -3.09e-14   1.65e-14  -3.00e-14  -5.33e-15\n",
      "   2.26e-14  -1.43e-15   3.01e-14   8.56e-15   1.89e-14   5.45e-15\n",
      "  -2.64e-14   3.28e-14   3.86e-14   2.19e-15   1.80e-15  -2.63e-14\n",
      "  -3.62e-14  -6.76e-16   2.26e-13  -3.16e-14   1.26e-14  -7.05e-15\n",
      "   2.32e-14  -9.53e-15   1.90e-14  -3.56e-15  -3.01e-15   4.29e-15\n",
      "   1.59e-14   4.27e-14   6.31e-15   2.57e-14  -1.12e-14   7.84e-15\n",
      "  -5.33e-15  -5.86e-15   1.34e-14  -2.79e-14  -3.39e-14   7.09e-14\n",
      "   5.04e-14  -2.79e-14  -2.80e-14   2.39e-14   1.51e-14   5.99e-14\n",
      "  -2.98e-14  -1.22e-13  -1.97e-14   2.31e-14  -1.84e-13  -9.51e-14\n",
      "   2.75e-14   2.03e-14  -5.72e-14   1.34e-14  -2.33e-15  -4.38e-16\n",
      "  -1.23e-14   1.61e-15  -5.85e-16  -3.65e-15   7.58e-15  -5.44e-15\n",
      "   1.74e-14   6.00e-15  -2.95e-15   9.59e-15   4.25e-15  -8.90e-15\n",
      "   4.31e-15   7.76e-15   5.19e-15  -9.50e-15  -1.25e-15   3.25e-15\n",
      "   5.98e-15   2.16e-15   4.34e-15   4.12e-15   2.51e-16   1.96e-15\n",
      "   1.78e-15   3.79e-15  -3.33e-15   1.30e-16  -3.21e-15  -5.38e-15\n",
      "   5.16e-15  -6.60e-15   7.95e-16   1.92e-15  -2.54e-15  -1.25e-15\n",
      "   3.59e-15   1.02e-14  -3.20e-15   8.53e-15  -3.30e-15  -3.40e-15\n",
      "  -1.36e-14  -1.49e-15  -1.62e-15   2.27e-15   4.08e-15   1.07e-14\n",
      "   5.10e-15   4.39e-16]\n",
      "[-0.52 -0.23 -0.52 -0.06 -0.52 -0.24 -0.52 -0.05 -0.53 -0.16 -0.51 -0.03\n",
      " -0.53 -0.08 -0.51 -0.   -0.54 -0.06 -0.5   0.   -0.54 -0.07 -0.5  -0.01\n",
      " -0.54 -0.04 -0.49 -0.01 -0.55  0.02 -0.48 -0.02 -0.55 -0.01 -0.48 -0.03\n",
      " -0.56 -0.05 -0.47 -0.05  0.13  0.04 -0.08 -0.16 -0.24 -0.29 -0.34 -0.37\n",
      " -0.4  -0.41 -0.52 -0.52 -0.52 -0.52 -0.52 -0.52 -0.52 -0.52 -0.52 -0.52\n",
      " -0.09 -0.19 -0.15 -0.18 -0.18 -0.19 -0.19 -0.18 -0.17 -0.11 -0.16 -0.18\n",
      " -0.22 -0.21 -0.24 -0.26 -0.24 -0.23 -0.54 -0.5  -0.24 -0.08 -0.29  0.03\n",
      "  0.04  0.03  0.03  0.05  0.05  0.05  0.04  0.05  0.05  0.07 -0.01 -0.    0.\n",
      "  0.    0.   -0.    0.01  0.02  0.03  0.02 -0.02 -0.01  0.02 -0.01  0.\n",
      " -0.03  0.05 -0.04  0.07 -0.06 -0.01  0.    0.01 -0.   -0.01  0.01  0.\n",
      " -0.03  0.02 -0.01 -0.12 -0.08 -0.09 -0.1  -0.09 -0.08 -0.01  0.02 -0.01\n",
      " -0.02]\n"
     ]
    }
   ],
   "source": [
    "#time series split\n",
    "#%%--------------------------------------------------------------------------------------------\n",
    "\n",
    "ticker_ind=1\n",
    "size=100000\n",
    "random_ratio=0.3\n",
    "\n",
    "time_index=time_index_list[ticker_ind]\n",
    "# combine the feature and response array to random sample\n",
    "time_index_reduced=time_index[(time_index>=start_ind)&(time_index<=end_ind)]\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                            time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)[:size,:]\n",
    "\n",
    "total_array=total_array[random_choice(list(range(size)),int(size*random_ratio)),:]\n",
    "\n",
    "train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x=total_array[:train_num_index,:134]\n",
    "test_x=total_array[train_num_index:,:134]\n",
    "train_y=total_array[:train_num_index,134]\n",
    "test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "print(\"train_x shape:\",train_x.shape)\n",
    "print(\"test_x shape:\",test_x.shape)\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)\n",
    "\n",
    "# scale the data\n",
    "# can use the processing.scale function to scale the data\n",
    "from sklearn import preprocessing\n",
    "# note that we need to transfer the data type to float\n",
    "# remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "# use scale for zero mean and one std\n",
    "scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "train_x_scale=scaler.transform(train_x)\n",
    "test_x_scale=scaler.transform(test_x)\n",
    "\n",
    "print(np.mean(train_x_scale,0))\n",
    "print(np.mean(test_x_scale,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one vs one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.8912947177887\n",
      "train accuracy is: 0.995022222222\n",
      "test time is : 0.7838070392608643\n",
      "test accuracy is: 0.9922\n",
      "Confusion matrix, without normalization\n",
      "[[  74   35    0]\n",
      " [   0 9790    0]\n",
      " [   0   43   58]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAEpCAYAAADWEjokAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmpkV5P8D09seO4ejZHYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcfbd2a8cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# only run for random forest method\n",
    "# one vs one case\n",
    "# random forest\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "## sample weights\n",
    "#sample_weights=[]\n",
    "#ratio=len(train_y)/sum(train_y==1)/10\n",
    "#for i in range(len(train_x)):\n",
    "#    if train_y[i]==0:\n",
    "#        sample_weights.append(1)\n",
    "#    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  OneVsOneClassifier(RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345))\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "%matplotlib inline\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_one.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311.88875126838684\n",
      "train accuracy is: 0.999811111111\n",
      "test time is : 0.25420355796813965\n",
      "test accuracy is: 0.9966\n",
      "Confusion matrix, without normalization\n",
      "[[  89   20    0]\n",
      " [   0 9790    0]\n",
      " [   0   14   87]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAEpCAYAAADWEjokAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XuYHVWZ7/Hvr7mFS8AoJEqCgSMkEh6EROkRUYijBvES\nGM9RQZQgcZgxoCjHUaLMgBcG8ZwjqAgzOpwQvIUwDkM8kyExhwMBBRIgECQhRHgSSCAduchdzOU9\nf9TqpNLsvXt3elfvqs7vw7Ofrlp7VdXaTT9v3r1q1VqKCMzMrPU62t0AM7PBygHWzKwgDrBmZgVx\ngDUzK4gDrJlZQRxgzcwK4gC7g5M0RNKvJP1R0rX9OM8nJN3Yyra1i6R3Slre7nZY9cnjYKtB0ieA\nLwJvBp4D7gX+MSJ+08/zfhI4Gzg6doA/BkmbgYMj4pF2t8UGP2ewFSDpXOC7wLeA4cAbgR8CH27B\n6UcDD+0IwTVp+Dkl7TRQDbEdQET4VeIXsDfwPPCRBnV2BS4D1gJrgEuBXdJ7xwGPAecCXanOlPTe\nhcArwJ/JsuJPAxcAP8mdezSwGehI+6cDD6f6DwOnpPIpwK25494BLAKeAe4ky5C73/t/wDeA29J5\nbgReW+ezdbf/73LtPxE4AVgBPAlMz9U/Cvhtuu5a4AfAzum9W9JneSFd96O5838ZeAKY2V2Wjvkv\nwFPAkWl/f2A9cGy7/zb8Kv/LGWz5HQ3sBvx7gzrnA53AW4Aj0vb5ufdfDwwlCw6fAa6QtE9EXAj8\nIzArIvaOiBmpfs8sLwAk7QF8Dzg+IvYmC6L31qg3DPg/ZEH/dWQB/z9SebdTyILyfunzfanB53s9\n2T8i+5P9A/Bj4FRgPHAs8PeSRqe6m4AvAK8l+939JTANICKOS3UOT5/3utz5X0P2zeDM/GeJrCvh\ny8BPJe0OzABmRMTCBu01A9xFUAWvA56MiM0N6nwC+HpEPBURTwFfBz6Ve//PwDcjYlNE/CdZBjd2\nO9uzCThc0pCI6IqIWjeDPkjW7fDziNgcEbOAB9m2S2NGRDwcEa8As4EjG1zzz2T9zZuAWcC+wGUR\n8VJELAOWkf3DQkTcExGLIvMo8COyjDRPNT7TBRGxIbVnGxFxFfB7skx8BNv+42VWlwNs+T0F7Cup\n0f+r/YFHc/urU9mWc/QI0C8Be/W1IRHxEvBx4LPAE2n0Qa1AvX9qQ95qYGRuf10f2vNURHRn1S+n\nn+tz77/cfbykQ1K7npD0R+AisoDcyB8iYkMvdf4FOAz4QRN1zQAH2Cq4nayf9KQGddaS9ZV2Gw08\nvp3XexHYI7f/hvybEfHriJhE9rV6BVmG2NPjwIE9yt6Y2lm0K4HlwJsi4jXA13h1xtpTbze+9iTr\n7rgKuFDSa1rRUBv8HGBLLiKeI+t3/KGkEyXtLmlnSSdI+naqNgs4X9K+kvYF/h74yXZe8l7gWEkH\nSNoHOK/7DUnDJU1OfbEbyLoaanVdzAUOkXSypJ0kfRw4FPjVdrapL4YCz0XES5LeTJZt560ju3HV\nF98HFkXEmWSf7Z/730zbETjAVkBEfJdsFMD5ZF+NHyW7cdN94+tbwF3AUuC+tH1Ro1M2uNYC4Np0\nrsVsGxQ7UjvWkt29P5ZXBzAi4mngQ2Q3rp5MPz8YEc/0dv0m1bwJl3wJOFXSc2SBcFaPuhcC10h6\nWtJ/6+1CkiYDk0g3ysg+/3hJp2xPw23H4gcNzMwK4gzWzKwgDrBmZgVxgDUzK8jO7W5AT5LcKWxW\nYRHR27C4pmnXvYMNz/flkNURcWCrrt9fpbvJJSlefKXRQ0vtddE3L+Rrf39hu5tRV0dHy/62C/Gt\nb1zI+f9wYbubUVll//3tvotaG2ClGHLkWU3X/9O9P2zp9furdBmsmdk2Gj7EWG4OsGZWbipNQtpn\nDrB99K5jJ7a7CZV27HET292EStshf38VzmDdBzvIlL0P1ga3Qvpg3/bFpuv/6a5L3QdrZta0Cmew\nDrBmVm4d1V3FxwHWzMrNN7nMzAriLgIzs4I4gzUzK4gzWDOzgjiDNTMriDNYM7OCOMCamRWkwk8n\nVvefBjPbMaij+Ve9U0jnSLo/vT6fyoZJmi9phaR5aRXl7vrTJa2UtFzSpFz5BElLJT0k6bLemu4A\na2blJjX/qnm4DgOmAm8DjgQ+JOlNZEvSL4iIscBNwPRUfxzwMbKl5k8ArpC2nPxKYGpEjAHGSDq+\nUdMdYM2s3PqfwR4K3BkRr0TEJmAh8BFgMjAz1ZkJnJS2JwOzImJjRKwCVgKdkl4PDI2IxaneNblj\nanKANbNy62cGC/wOeFfqEtgD+ABwADAiIroAImIdMDzVHwk8ljt+bSobCazJla9JZXX5JpeZlVuD\nvtVNTz/M5mcebnh4RDwo6RLg18ALwBJgU62q/WhlTQ6wZlZuDWbT2mnfMey075gt+5seWVCzXkTM\nAGYASLqILEPtkjQiIrrS1//1qfpasgy326hUVq+8ftMbvWlm1nb97yJA0n7p5xuBvwJ+DswBTk9V\npgA3pO05wMmSdpV0EHAwsCh1IzwrqTPd9Dotd0xNzmDNrNxa86DBLyW9FtgATIuI51K3wWxJZwCr\nyUYOEBHLJM0GluXqd3cfnAVcDQwB5kbEjQ2b7iVjBhcvGWPtVMiSMR/4XtP1/zT3HC8ZY2bWND8q\na2ZWEAdYM7OCeLpCM7OCOIM1MyuIM1gzs4I4gzUzK4gzWDOzYsgB1sysGA6wZmZFqW58dYA1s3Lr\n6KjuTa4Ba7mksZJ+K+lPks4dqOuaWbVJavpVNgOZwT4FfI5ellgwM8srY+Bs1oBlsBHxZETcDWwc\nqGua2SCgPrxKxn2wZlZqVc5gSxlgL/rmhVu233XsRI49bmLb2mJm9S285WYW3nJzoddoRYCV9EWy\npbs3A/cDnwb2BK4FRgOrgI9FxLOp/nTgDLJv3OdExPxUPoFtJ9z+QsPrFjnhtqRpwF+TLSb2gYhY\nJ+kC4PmI+G6dYzzhdj94wm1rpyIm3B72yZ81Xf+Zn576qutL2h+4DXhzRPxZ0rXAXGAc8FREfEfS\nV4BhEXGepHHAz4CjyNbdWgAcEhEh6U7g7IhYLGku8L2ImFevPYX2wUbEFRExPiImpPVsujkKmFlT\nWjSKYCdgT0k7A7uTLVZ4IjAzvT+TrTfgJwOzImJjRKwCVgKdaWHEoRGxONW7hl5u2g9YF4GkEcBd\nwFBgs6RzgHER8cJAtcHMKqif6VhEPC7pfwGPAi8B8yNiQfeKsqnOOknD0yEjgdtzp1ibyjYCa3Ll\na1J5XQMWYNMHOaDXimZmOY0y0w3rlrFh3bLejn8NWbY6GngWuE7SqWRdl3kt7y8t5U0uM7NujQLs\nrm84jF3fcNiW/Zfv+2Wtau8FHomIp9P5rgfeAXR1Z7Hp6//6VH8t2yaDo1JZvfK6qvsMmpntEFrQ\nB/so8HZJQ5RVeg/ZktxzgNNTnSnADWl7DnCypF0lHQQcDCxK95GeldSZznNa7pianMGaWbn1vw92\nkaR/BZYAG9LPH5HdD5ot6QxgNfCxVH+ZpNlkQXgDMC22Drc6i22Had3YsOlFDtPaHh6m1T8epmXt\nVMQwreFTZzddf/1VH2vp9fvLGayZlZqf5DIzK0iVpyt0gDWzcqtuAusAa2bl5i4CM7OCOMCamRXE\nAdbMrCjVja8OsGZWbs5gzcwK4gBrZlYQB1gzs4I4wJqZFaW68dUB1szKzRmsmVlBHGDNzApS4fjq\nFQ3MrNw6OtT0qxZJYyQtkXRP+vmspM9LGiZpvqQVkuZJ2id3zHRJKyUtlzQpVz5B0lJJD0m6rNe2\nt+Q3YGZWkP4uGRMRD0XE+IiYALwVeBG4HjgPWBARY4GbgOnpeuPIVjc4FDgBuEJbT34lMDUixgBj\nJB3fqO0OsGZWalLzrya8F3g4Ih4jW2l2ZiqfCZyUticDsyJiY0SsAlYCnWlhxKERsTjVuyZ3TE3u\ngzWzUmvxMkgfB36etkdERBdARKyTNDyVjwRuzx2zNpVtBNbkytek8rqcwZpZqbUqg5W0C1l2el0q\n6rkgYcsXKHQGa2al1miY1our7+PF1fc1e6oTgLsj4sm03yVpRER0pa//61P5WuCA3HGjUlm98roc\nYM2s1BplpnsdeAR7HXjElv0/3PrTRqc6BfhFbn8OcDpwCTAFuCFX/jNJl5J1ARwMLIqISCMQOoHF\nwGnA9xtd0AHWzEqtFQ8aSNqD7AbXmbniS4DZks4AVpONHCAilkmaDSwDNgDTIqK7++As4GpgCDA3\nIm5sdF0HWDMrtVYE2Ih4CdivR9nTZEG3Vv2LgYtrlN8NHN7sdR1gzazUqvwklwOsmZWa5yIwMytI\nheOrA6yZlZszWDOzglQ4vjrAmlm5OYNtsRY/e7xDGXbU2e1uQqU9s/jydjfBeqhyPChlgDUz61bh\nBNYB1szKzV0EZmYFqXB8dYA1s3JzBmtmVpAKx1cHWDMrN2ewZmYFcYA1MytIheOrA6yZlVuVM1gv\nemhmpdaKRQ8l7SPpOknLJT0g6S8kDZM0X9IKSfMk7ZOrP13SylR/Uq58gqSlkh6SdFlvbXeANbNS\nk9T0q4HvkS3xcihwBPAgcB6wICLGAjcB09P1xpEtH3Mo2UKJV2jrya8EpkbEGGCMpOMbXdQB1sxK\nrb8ZrKS9gXdFxAyAiNgYEc8CJwIzU7WZwElpezIwK9VbBawEOtPKs0MjYnGqd03umJocYM2s1Dqk\npl91HAQ8KWmGpHsk/SgtgjgiIroAImIdMDzVHwk8ljt+bSobCazJla9JZXX5JpeZlVqj2bSefuhu\nnl55T2+n2BmYAJwVEXel5bjPA6JHvZ77/eYAa2al1mi2wn3HvpV9x751y/4jc6+qVW0N8FhE3JX2\nf0kWYLskjYiIrvT1f316fy1wQO74UamsXnn9tjd608ys3fp7kyt1AzwmaUwqeg/wADAHOD2VTQFu\nSNtzgJMl7SrpIOBgYFHqRnhWUme66XVa7pianMGaWam1aBjs54GfSdoFeAT4NLATMFvSGcBqspED\nRMQySbOBZcAGYFpEdHcfnAVcDQwhG5VwY6OL1g2w6c5bXRHxXBMfysysX0T/I2xE3AccVeOt99ap\nfzFwcY3yu4HDm71uowz2AbJO3/yn694P4I3NXsTMbHtVeMWY+gE2Ig6o956Z2UAZ9I/KSjpZ0lfT\n9ihJb+3tGDOzVmjFo7Lt0muAlXQ58G7gU6noJeCfimyUmVm3Fjxo0DbNjCJ4R0RMkLQEICKelrRr\nwe0yMwPKmZk2q5kAu0FSB+kpB0mvAzYX2iozs2Sw98H+kOzJh/0kfR24Dbik0FaZmSVV7oPtNYON\niGsk3c3W8WIfjYjfFdssM7NMGftWm9Xsk1w7kT3REPjxWjMbQNUNr82NIvga8Atgf7LJDX4uaXrR\nDTMzg5ZNuN0WzWSwpwHjI+IlAEkXAUuo8RiZmVmr7VThR7maCbBP9Ki3cyozMytcCRPTpjWa7OVS\nsj7Xp4EHJM1L+5OAxfWOMzNrpTJ+9W9Wowy2e6TAA8B/5MrvKK45ZmbbqnAPQcPJXmpODW5mNpCq\nnME2M4rgTZJm5dYCf0jSQwPRODMz9eFV9xzSKkn3SVoiaVEqGyZpvqQVkuZJ2idXf7qklZKWS5qU\nK5+Qi4WX9db2Zsa0Xg3MSO0/AZgNXNvEcWZm/daiyV42AxMjYnxEdKay84AFETEWuAmYDiBpHNnq\nBoeSxbwrtDWNvhKYGhFjgDGSjm/Y9iY+3x4RMQ8gIh6OiPPTRc3MCteiR2XFq+PdicDMtD0TOClt\nTwZmRcTGiFgFrAQ608KIQyOi+yb/NbljamomwL6SJnt5WNLfSvowMLSJ415F0vslPZjS669szznM\nbMfSogcNAvi1pMWSPpPKRqQFEUkLGg5P5SOBx3LHrk1lI8lWqO22JpXV1cw42C8Ce5ItGnYRsA9w\nRhPHbSMF6cvJVnR8HFgs6YaIeLCv5zKzHUeL7nEdExFPSNoPmC9pBWmGwJye+/3WzGQvd6bN59k6\n6fb26ARWRsRqAEmzyFJ0B1gzq6tR3+ra3y3i8Qd6H5YfEU+kn3+Q9O9k8ahL0oiI6Epf/9d3nxbI\nL5k1KpXVK6+r0YMG19MgokfERxqduIaeafcasg9pZlZXowx21OGdjDp8axi5+7orahyvPYCOiHhB\n0p5kD0t9HZgDnE42/eoU4IZ0yByyJb4vJYtbBwOLIiIkPSupk+xhq9OA7zdqe6MM9vJGBxbpW9+4\ncMv2scdN5NjjJrarKWbWwMJbbmbhLTcXeo0WjIMdAVwvKchi3s8iYr6ku4DZks4AVpONHCAilkma\nDSwjm0VwWkR0J5tnkY2sGgLMjYgbG7Z963HFkvR24MKIeH/aPw+IiLikR714ecPAtGkwGnbU2e1u\nQqU9s7htecWgsPsuIiJa9mSApDj735Y1Xf/yj4xr6fX7q9n5YFthMXCwpNFkk8WcDJwygNc3swoa\n7LNptUREbJJ0NjCfbHjYVRGxfKCub2bVVOH42nyAlbRbRLzSn4ul/oqx/TmHme1YBvtcBJ2S7id7\nmgFJR0j6QeEtMzMjy2CbfZVNM09yfR/4EPAUQETcB7y7yEaZmXUb1KvKko0fW90jTd9UUHvMzLYx\n2FeVfSwNrA1JOwGfAzxdoZkNiCovY91MgP0sWTfBG4EuYEEqMzMrXIUT2KbmIlhPNmbVzGzADeou\nAkk/psacBBFxZiEtMjPLqXB8baqLYEFuewjwV2w7aYuZWWHKOPyqWc10EWyzPIyknwC3FdYiM7Oc\nQd1FUMNBZLPTmJkVrsLxtak+2GfY2gfbATxNtliYmVnhBm0XQVpJ8Qi2ztq9OQZqfkMzM2CnCqew\nDcfwpmA6NyI2pZeDq5kNqME+F8G9ksYX3hIzsxpatKoskjok3SNpTtofJmm+pBWS5knaJ1d3uqSV\nkpZLmpQrnyBpaVoZ+7Le2l43wErq7j4YT7YC7IrUuCWS7untxGZmrdDCDPYcsmVgup0HLIiIscBN\nwHQASePIlo85FDgBuEJbo/eVwNSIGAOMkXR8ows26oNdBEwAJvfabDOzgrSiC1bSKOADwEXAuan4\nROC4tD0TuJks6E4GZkXERmCVpJVAp6TVwNCI6F7G9hrgJGBeves2CrACiIiHt+cDmZm1QovGwV4K\n/B2wT65sRER0AUTEOknDU/lI4PZcvbWpbCPZatjd1qTyuhoF2P0knVvvzYj4bqMTm5m1QqOv/iuX\n3MHvl9zR8HhJHwS6IuJeSRMbVG35TfxGAXYnYC9SJmtm1g6NEtgxE97OmAlv37J/44zv16p2DDBZ\n0geA3YGh6YnUdZJGRESXpNcD61P9tcABueNHpbJ65XU1CrBPRMQ3Gh1sZla0jn7meBHxVeCrAJKO\nA/57RHxK0neA04FLgCnADemQOcDPJF1K1gVwMLAoIkLSs2l+7MXAaWRTudbVax+smVk7FficwbeB\n2ZLOAFaTjRwgIpZJmk024mADMC33DMBZwNVkE1/NTQu51tUowL6nf203M+u/Vj5AEBG3ALek7aeB\n99apdzFwcY3yu4HDm71e3QCbLm5m1lY72mxaZmYDpsLx1QHWzMrNGayZWUEqHF8dYM2s3Ko8XaED\nrJmVWnXDqwOsmZWc+2DNzApS3fDqAGtmJVfhBNYB1szKrbeVCsrMAdbMSq2Zda3KygHWzErNGayZ\nWUGqG14dYAedZxZf3u4mVNrmzV6ZvmycwZqZFaTKfbBVbruZ7QAkNf2qc/xuku6UtETS/ZIuSOXD\nJM2XtELSPEn75I6ZLmmlpOWSJuXKJ0haKukhSZf11nYHWDMrNfXhVUtEvAK8OyLGA0cCJ6RlX84D\nFkTEWOAmYDqApHFkqxscCpwAXKGt0ftKYGpEjAHGSDq+UdsdYM2s1KTmX/VExEtpczeyrtEATgRm\npvKZwElpezIwKyI2RsQqYCXQmRZGHBoRi1O9a3LH1OQAa2altpPU9KseSR2SlgDrgF+nIDkiIroA\nImIdMDxVHwk8ljt8bSobCazJla9JZXX5JpeZlZoaDNRauvg33L/4t72eIyI2A+Ml7Q1cL+kwsix2\nm2r9aWctDrBmVmqNvvof0XkMR3Qes2X/F1f+z4bniojnJN0MvB/okjQiIrrS1//1qdpa4IDcYaNS\nWb3yutxFYGal1oGaftUiad/uEQKSdgfeBywH5gCnp2pTgBvS9hzgZEm7SjoIOBhYlLoRnpXUmW56\nnZY7piZnsGZWai14zuANwExJHWRJ5bURMVfSHcBsSWcAq8lGDhARyyTNBpYBG4BpEdHdfXAWcDUw\nBJgbETc2bPvW48pBUry8oVxtsh2Hn+Tqnz136yAiWvbolaSYt2x97xWT48cNb+n1+8sZrJmVWqOb\nXGXnAGtmpdZR3fjqAGtm5eYM1sysIBWeTMsB1szKzRmsmVlB3AdrZlYQZ7BmZgVxH6yZWUEqHF8d\nYM2s3BpNQ1h2DrBmVm7Vja8OsGZWbr7JZWZWkAr3EDjAmlm5VTi+OsCaWclVOMI6wJpZqVW5D9ZL\nxphZqfV32W5JoyTdJOkBSfdL+nwqHyZpvqQVkuZ1LyuT3psuaaWk5ZIm5conSFoq6SFJl/XWdgdY\nMys19eFVx0bg3Ig4DDgaOEvSm4HzgAURMRa4CZgOIGkc2fIxhwInAFekNbgArgSmRsQYYIyk4xu1\n3QHWzMqtnxE2ItZFxL1p+wWyBQ9HAScCM1O1mcBJaXsyMCsiNkbEKmAl0JlWnh0aEYtTvWtyx9Tk\nPlgzK7VW9sFKOhA4ErgDGBERXZAFYUnDU7WRwO25w9amso3Amlz5mlRelwOsmZVao3Gwd91+K3fd\ncWuT59FewL8C50TEC5J6rnDZ8hUvHWDNrNQa5a9HHf0ujjr6XVv2//myb9c+h7QzWXD9SUTckIq7\nJI2IiK709b97+dq1wAG5w0elsnrldbkP1szKrQV3uYD/DSyLiO/lyuYAp6ftKcANufKTJe0q6SDg\nYGBRRKwDnpXUmW56nZY7piZnsGZWah39fFZW0jHAqcD9kpaQdQV8FbgEmC3pDGA12cgBImKZpNnA\nMmADMC0iursPzgKuBoYAcyPixobX3npcOUiKlzeUq02249i82X97/bHnbh1ERMvuSkmK+x97vun6\nhx8wtKXX768B7SKQdJWkLklLB/K6ZlZhrekiaIuB7oOdATQcmGtmlqc+/Fc2A9oHGxG3SRo9kNc0\ns2rzdIVmZgWpcHwtZ4D91jcu3LJ97HETOfa4iW1ri5nVt/CWm7l14c3FXqTCEXbARxGkLoJfRcRb\n6rzvUQTWNh5F0D9FjCJY/viLTdc/dP89SzWKoB0ZbEnv95lZGVW5D3agh2n9HPgt2TRfj0r69EBe\n38yqp8KjtAZ8FMEnBvJ6ZjYIlDFyNqmUN7nMzLqVcXxrsxxgzazUqtwH6wBrZqVW4fjqAGtmJVfh\nCOsAa2al1t/pCtvJAdbMSq264dUrGphZ2fVzIGytaVIlDZM0X9IKSfMk7ZN7b7qklZKWS5qUK58g\naamkhyRd1kzTHWDNrNRaMF1hrWlSzwMWRMRY4CZgOoCkcWQrGxwKnABckZaHAbgSmBoRY8gelup1\n6lUHWDMrNan5Vy0RcRvwTI/iE4GZaXsmcFLangzMioiNEbEKWAl0pkURh0bE4lTvmtwxdbkP1sxK\nraA+2OER0QUQEeskDU/lI4Hbc/XWprKNwJpc+ZpU3pADrJmVWqNBBLffdgu337awFZcpZBo1B1gz\nK7n6Efbod07k6HdO3LJ/6XcuavakXZJGRERX+vq/PpWvBQ7I1RuVyuqVN+Q+WDMrtf72wXafhm0j\n9Rzg9LQ9BbghV36ypF0lHQQcDCyKiHXAs5I6002v03LH1OUM1sxKrb99sGma1InA6yQ9ClwAfBu4\nTtIZwGqykQNExDJJs4FlwAZgWmxdleAs4GpgCDA3Im7s9doDvaJBb7yigbWTVzTonyJWNHj8j680\nXX//1+y2w69oYGbWNE9XaGZWlOrGVwdYMyu3CsdXB1gzKzfPpmVmVpTqxlcHWDMrtwrHVwdYMyu3\nCvcQOMCaWbl5mJaZWUGqnMF6LgIzs4I4gzWzUqtyBusAa2al5j5YM7OCOIM1MytIheOrA6yZlVyF\nI6wDrJmVWpX7YD1Mq48W3nJzu5tQaf799c+O+PtrxZIxkt4v6UFJD0n6ykC13QG2j3bEP/BW8u+v\nf25deHO7mzDg+htgJXUAlwPHA4cBp0h680C03QHWzEpNffivjk5gZUSsjogNwCzgxIFouwOsmZVa\nC7oIRgKP5fbXpLLClXLRw3a3wcy2X4sXPVwFjO7DIV0R8foe5/ivwPERcWba/yTQGRGfb1U76ynd\nKIIyrQhpZu0VEQe24DRrgTfm9kelssK5i8DMBrvFwMGSRkvaFTgZmDMQFy5dBmtm1koRsUnS2cB8\nsqTyqohYPhDXLl0frJnZYOEugj6QNFbSbyX9SdK57W5PlbRroPdgIekqSV2Slra7LdY8B9i+eQr4\nHPA/2t2QKmnnQO9BZAbZ788qxAG2DyLiyYi4G9jY7rZUTNsGeg8WEXEb8Ey722F94wBrA6FtA73N\n2skB1sysIA6wvZA0TdISSfdIen3vR1gNbRvobdZODrC9iIgrImJ8REyIiHW5t/zEWfPaNtB7kBH+\nu6sUj4PtA0kjgLuAocBm4AVgXES80NaGVYCk9wPfY+tA72+3uUmVIunnwETgdUAXcEFEzGhro6xX\nDrBmZgVxF4GZWUEcYM3MCuIAa2ZWEAdYM7OCOMCamRXEAdbMrCAOsIOcpE3pKbT7JV0raUg/znWc\npF+l7Q9L+nKDuvtI+ux2XOOCWlNB1ivvUWeGpI/04VqjJd3f1zaaNcsBdvB7MT2FdjiwAfjbnhWk\nButxvloARMSvIuI7DeoNA6b1qaXt4YHgVhgH2B3LrWx9ZPVBSTNTBjdK0vvSZOJ3pUx3D9gyUfZy\nSXcBW7JDSVMk/SBtD5f0b5LuTfM2vB24GHhTyp4vSfW+JGlRqndB7lxfk7RC0kJgbG8fQtJn0nmW\nSLquR1bop9sBAAACb0lEQVT+PkmL0+f7YKrfIek7ku5M1/7rfv8mzZrgADv4CUDSzsAJQPdX4kOA\ny1Nm+xJwPvCeiHgbcDdwrqTdgB8BH0zlPSe76c7+vg/cHBFHAhOAB4DzgN+n7Pkrkt4HHBIRncB4\n4G2S3ilpAvAx4C3AB4GjmvhMv4yIzogYDzwITM29NzoijgI+BPxTmvtgKvDHiPgLsrlpz5TUl6Wg\nzbaLFz0c/HaXdE/avhW4imwu1lURsTiVvx0YB/wmdRfsAtwOvBl4JCIeSfV+CtTK/v4S+BRAZM9e\nPy/ptT3qTCLLLu8hC/p7kgX5vYHrI+IV4BVJzUwC8xZJ3wRek84zL/fe7NSO30t6OH2GScDhkj6a\n6uydrr2yiWuZbTcH2MHvpYiYkC9IXa4v5ouA+RFxao96R9Dc7E3N9GMKuDgiftzjGuc0cWxPM4DJ\nEfE7SVOA4+q0RWlfwOci4tc9ru0s1grlLoLBr16AzJffARwj6U0AkvaQdAjZ1+/Rkg5K9U6pc67/\nS7qhlfo79waeJ5t1rNs84AxJe6Z6+0vaD1gInCRpN0lDgQ838Zn2AtZJ2gU4tcd7H1XmTcBBwIp0\n7WmpmwRJh0javcbvwaylnMEOfvWyyy3lEfGkpNOBX6R+1wDOj4iVkv4GmCvpRbIuhr1qnOsLwI8k\nTSVbr+yzEXFnumm2FPjP1A97KHB7yqCfBz4ZEUskzQaWkk3Dt6iJz/QPqd564E62DeSPpveGAn8T\nEX+W9C/AgcA9qQtkPXBSL78fs37zdIVmZgVxF4GZWUEcYM3MCuIAa2ZWEAdYM7OCOMCamRXEAdbM\nrCAOsGZmBfn/3DkeQPBnjB8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcfc2f987b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# only run for random forest method\n",
    "# one vs one case\n",
    "# adaboosting\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "## sample weights\n",
    "#sample_weights=[]\n",
    "#ratio=len(train_y)/sum(train_y==1)/10\n",
    "#for i in range(len(train_x)):\n",
    "#    if train_y[i]==0:\n",
    "#        sample_weights.append(1)\n",
    "#    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10),n_estimators=100,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_one.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------\n",
    "# one vs one case\n",
    "# svm\n",
    "#-------------------\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "\n",
    "\n",
    "## sample weights\n",
    "#sample_weights=[]\n",
    "#ratio=len(train_y)/sum(train_y==1)/10\n",
    "#for i in range(len(train_x)):\n",
    "#    if train_y[i]==0:\n",
    "#        sample_weights.append(1)\n",
    "#    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  OneVsOneClassifier(svm.SVC(C=1.0,kernel='poly',degree=2,max_iter=5000,shrinking=True, tol=0.001, verbose=False)\n",
    ")\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_one.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# only run for random forest method\n",
    "# one vs rest case\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  OneVsRestClassifier(RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345))\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_rest.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.P&L calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_index(index, value):\n",
    "    i=0\n",
    "    while index[i] <value:\n",
    "        i=i+1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## for AMZN\n",
    "ticker_ind =1\n",
    "train_ratio=0.9\n",
    "\n",
    "data_order=data_order_list[ticker_ind]\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "time_index=data_mess[:,0]\n",
    "data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "total_array_old=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 40)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "data_order_test=data_order_reduced[int(size*train_ratio):size,:]\n",
    "time_index_test=time_index_reduced[int(size*train_ratio):size]\n",
    "\n",
    "test_y_unrandom=total_array_old[int(size*train_ratio):size,134]\n",
    "print(data_order_test.shape)\n",
    "print(time_index_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg4AAAEZCAYAAAD/gK2HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTLxxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcfbd2fb1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(time_index_test[:10000],data_order_test[:10000,0],\"r-\",label=\"Ask price\")\n",
    "plt.plot(time_index_test[:10000],data_order_test[:10000,2],\"b-\",label=\"Bid price\")\n",
    "\n",
    "x_ask_low_choose=time_index_test[test_y_unrandom==1]\n",
    "y_ask_low_choose=data_order_test[test_y_unrandom==1,0]\n",
    "x_bid_high_choose=time_index_test[test_y_unrandom==-1]\n",
    "y_bid_high_choose=data_order_test[test_y_unrandom==-1,2]\n",
    "\n",
    "plt.plot(x_ask_low_choose[:30],y_ask_low_choose[:30],\"gv\",markersize=8,label=\"Ask low\")\n",
    "plt.plot(x_bid_high_choose[:30],y_bid_high_choose[:30],\"r^\",markersize=8,label=\"Bid high\")\n",
    "plt.xlabel(\"Time(s)\")\n",
    "plt.ylabel(\"Price($10^{-4}$\\$)\")\n",
    "plt.legend(bbox_to_anchor=[1.4, 1])\n",
    "plt.title(\"Arbitrage opportunities for \"+ticker_list[ticker_ind]+\"(5s)\")\n",
    "plt.savefig(\"arbitrage_plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_index_test=total_array[:,135][int(size*train_ratio):size]\n",
    "# find the arbitrage occuring index\n",
    "arbi_index=list(np.where(predict_y_test!=0)[0])\n",
    "# find the index that 5 seconds later\n",
    "arbi_future_index=[]\n",
    "for i in arbi_index:\n",
    "    arbi_future_index.append(get_index(time_index_reduced,time_index_test[i]+5))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_price=data_order_reduced[arbi_future_index,0]\n",
    "current_price=total_array_test[arbi_index,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2239300.0\n",
      "2239300.0\n"
     ]
    }
   ],
   "source": [
    "print(current_price[234])\n",
    "print(future_price[234])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_array_test=total_array[int(size*train_ratio):size,:]\n",
    "future_price=[]\n",
    "current_price=[]\n",
    "pnl=[]\n",
    "for i in range(len(arbi_index)):\n",
    "    #ask low\n",
    "    if predict_y_test[arbi_index[i]]==1 :\n",
    "        future_price=data_order_reduced[arbi_future_index[i],0]\n",
    "        current_price=total_array_test[arbi_index[i],2]\n",
    "        pnl.append(current_price-future_price)\n",
    "    # bid high\n",
    "    else: \n",
    "        future_price=data_order_reduced[arbi_future_index[i],2]\n",
    "        current_price=total_array_test[arbi_index[i],0]\n",
    "        pnl.append(future_price-current_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,False]\n",
    "a.index(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([234]),)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.where(predict_y_test!=0)[0]\n",
    "np.where(a==7976)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4384, 7976, 9836]),)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(((test_y==0) & (predict_y_test==1))==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAEZCAYAAADR8/HkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkBBCiFSQAxJCCJEK/x9Cyr1rGHwPrgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcfbd0b5080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pnl=np.array(pnl)\n",
    "predict_arbi=predict_y_test[predict_y_test!=0]\n",
    "plt.plot(pnl[predict_arbi==1],\"b.\",label=\"Ask low PnL\")\n",
    "plt.plot(pnl[predict_arbi==-1],\"r.\",label=\"Bid High PnL\")\n",
    "\n",
    "plt.xlabel(\"Arbitrage Index\")\n",
    "plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "plt.title(\"PnL for \"+ticker_list[ticker_ind])\n",
    "plt.legend()\n",
    "plt.savefig(ticker_list[ticker_ind]+\"_pnl.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEZCAYAAADYGFGeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5JxzLi/8/0V1QSklsf6IAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcfbd1640b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cum_pnl=np.cumsum(pnl)\n",
    "plt.plot(cum_pnl,\"b.\",label=\"Cumulative P&L\")\n",
    "plt.xlabel(\"Arbitrage Index\")\n",
    "plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "plt.title(\"Cumulative PnL for \"+ticker_list[ticker_ind])\n",
    "plt.legend()\n",
    "plt.savefig(ticker_list[ticker_ind]+\"_cum_pnl.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop for all stock to plot the pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#time series split\n",
    "#%%--------------------------------------------------------------------------------------------\n",
    "\n",
    "size =100000\n",
    "for ticker_ind in range(2,5):\n",
    "    # combine the feature and response array to random sample\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    time_index=data_mess[:,0]\n",
    "    data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    total_array_old=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                    time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)\n",
    "\n",
    "    total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)[:size,:]\n",
    "    total_array=total_array[np.random.randint(len(total_array),size=len(total_array)),:]\n",
    "\n",
    "    train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "    print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "    #split the data to train and test data set\n",
    "    train_x=total_array[:train_num_index,:134]\n",
    "    test_x=total_array[train_num_index:,:134]\n",
    "    train_y=total_array[:train_num_index,134]\n",
    "    test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "    # the y data need to reshape to size (n,) not (n,1)\n",
    "    test_y=test_y.reshape(len(test_y),)\n",
    "    train_y=train_y.reshape(len(train_y),)\n",
    "    print(\"train_x shape:\",train_x.shape)\n",
    "    print(\"test_x shape:\",test_x.shape)\n",
    "    print(\"test_y shape:\",test_y.shape)\n",
    "    print(\"train_y shape:\",train_y.shape)\n",
    "\n",
    "\n",
    "    # scale the data\n",
    "    # can use the processing.scale function to scale the data\n",
    "    from sklearn import preprocessing\n",
    "    # note that we need to transfer the data type to float\n",
    "    # remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "    # use scale for zero mean and one std\n",
    "    scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "    train_x_scale=scaler.transform(train_x)\n",
    "    test_x_scale=scaler.transform(test_x)\n",
    "\n",
    "    print(np.mean(train_x_scale,0))\n",
    "    print(np.mean(test_x_scale,0))\n",
    "\n",
    "    from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "    # change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "    t=time.time()\n",
    "    clf =  OneVsRestClassifier(RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345))\n",
    "    clf.fit(train_x_scale,train_y)\n",
    "\n",
    "    print(time.time()-t)\n",
    "\n",
    "    predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "    print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "    # define a function to prefict the result by threshold\n",
    "    # note: logistic model will return two probability\n",
    "    def predict_threshold(predict_proba, threshold):\n",
    "        res=[]\n",
    "        for i in range(len(predict_proba)):\n",
    "            res.append(int(predict_proba[i][1]>threshold))\n",
    "        return res\n",
    "\n",
    "    t=time.time()\n",
    "    predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "    print(\"test time is :\",time.time()-t)\n",
    "    print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "    # # test the score for the train data\n",
    "    # from sklearn.metrics import (precision_score, recall_score,\n",
    "    #                              f1_score)\n",
    "    # print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "    # precision= precision_score(predict_y_test,test_y)\n",
    "    # recall = recall_score(predict_y_test,test_y)\n",
    "    # f1=f1_score(predict_y_test,test_y)\n",
    "    # print(\"precision is: \\t %s\" % precision)\n",
    "    # print(\"recall is: \\t %s\" % recall)\n",
    "    # print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "    # #draw the crosstab chart\n",
    "    # %matplotlib inline\n",
    "    # ## draw chart for the cross table\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(3)\n",
    "        plt.xticks(tick_marks, [-1,0,1])\n",
    "        plt.yticks(tick_marks, [-1,0,1])\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(test_y, predict_y_test)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm)\n",
    "    plt.savefig(\"one_vs_rest.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    def get_index(index, value):\n",
    "        i=0\n",
    "        while index[i] <value:\n",
    "            i=i+1\n",
    "        return i\n",
    "\n",
    "\n",
    "    train_ratio=0.9\n",
    "    time_index=data_mess[:,0]\n",
    "    data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    total_array_old=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                    time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "\n",
    "    time_index_test=total_array[:,135][int(size*train_ratio):size]\n",
    "    # find the arbitrage occuring index\n",
    "    arbi_index=list(np.where(predict_y_test!=0)[0])\n",
    "    # find the index that 5 seconds later\n",
    "    arbi_future_index=[]\n",
    "    for i in arbi_index:\n",
    "        arbi_future_index.append(get_index(time_index_reduced,time_index_test[i]+5))\n",
    "\n",
    "    total_array_test=total_array[int(size*train_ratio):size,:]\n",
    "    future_price=[]\n",
    "    current_price=[]\n",
    "    pnl=[]\n",
    "    for i in range(len(arbi_index)):\n",
    "        #ask low\n",
    "        if predict_y_test[arbi_index[i]]==1 :\n",
    "            future_price=data_order_reduced[arbi_future_index[i],0]\n",
    "            current_price=total_array_test[arbi_index[i],2]\n",
    "            pnl.append(current_price-future_price)\n",
    "        # bid high\n",
    "        else: \n",
    "            future_price=data_order_reduced[arbi_future_index[i],2]\n",
    "            current_price=total_array_test[arbi_index[i],0]\n",
    "            pnl.append(future_price-current_price)\n",
    "\n",
    "    pnl=np.array(pnl)\n",
    "    predict_arbi=predict_y_test[predict_y_test!=0]\n",
    "    plt.plot(pnl[predict_arbi==1],\"b.\",label=\"Ask low PnL\")\n",
    "    plt.plot(pnl[predict_arbi==-1],\"r.\",label=\"Bid High PnL\")\n",
    "\n",
    "    plt.xlabel(\"Arbitrage Index\")\n",
    "    plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "    plt.title(\"PnL for \"+ticker_list[ticker_ind])\n",
    "    plt.legend()\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_pnl.png\")\n",
    "    plt.show()\n",
    "\n",
    "    cum_pnl=np.cumsum(pnl)\n",
    "    plt.plot(cum_pnl,\"b.\",label=\"Cumulative P&L\")\n",
    "    plt.xlabel(\"Arbitrage Index\")\n",
    "    plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "    plt.title(\"Cumulative PnL for \"+ticker_list[ticker_ind])\n",
    "    plt.legend()\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_cum_pnl.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the order book type\n",
    "\n",
    "use the data_mess data set to plot the chart of the order book type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Plot the order book types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "order_type_list=[]\n",
    "t=time.time()\n",
    "for ticker_ind in range(5):\n",
    "    order_type=[]\n",
    "    for i in [1,2,3,4,5]:\n",
    "        order_type.append(sum(data_mess_list[ticker_ind][:,1]==i))\n",
    "    order_type_list.append(order_type)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(order_type_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# n_groups = 5\n",
    "\n",
    "# means_men = (20, 35, 30, 35, 27)\n",
    "# std_men = (2, 3, 4, 1, 2)\n",
    "\n",
    "# means_women = (25, 32, 34, 20, 25)\n",
    "# std_women = (3, 5, 2, 3, 3)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# index = np.arange(n_groups)\n",
    "# bar_width = 0.35\n",
    "\n",
    "# opacity = 0.4\n",
    "# error_config = {'ecolor': '0.3'}\n",
    "\n",
    "# rects1 = plt.bar(index, means_men, bar_width,\n",
    "#                  alpha=opacity,\n",
    "#                  color='b',\n",
    "#                  yerr=std_men,\n",
    "#                  error_kw=error_config,\n",
    "#                  label='Men')\n",
    "\n",
    "# rects2 = plt.bar(index + bar_width, means_women, bar_width,\n",
    "#                  alpha=opacity,\n",
    "#                  color='r',\n",
    "#                  yerr=std_women,\n",
    "#                  error_kw=error_config,\n",
    "#                  label='Women')\n",
    "\n",
    "# plt.xlabel('Group')\n",
    "# plt.ylabel('Scores')\n",
    "# plt.title('Scores by group and gender')\n",
    "# plt.xticks(index + bar_width, ('A', 'B', 'C', 'D', 'E'))\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "order_type_array=np.array(order_type_list)\n",
    "\n",
    "\n",
    "n_groups=7.5\n",
    "index = np.arange(n_groups,step=1.5)    # the x locations for the groups\n",
    "ticker_list=['AAPL', 'AMZN', 'GOOG', 'INTC','MSFT']\n",
    "color_list=['red','yellow','green','blue','darkmagenta']\n",
    "type_list=['1:Order_book','2:Cancel_part','3:Delete_all','4:Execution_visible','5:Execution_hidden']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "bar_width = 0.25\n",
    "\n",
    "opacity = 0.6\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, order_type_array[:,0], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[0],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[0])\n",
    "\n",
    "rects2 = plt.bar(index + 1*bar_width, order_type_array[:,1], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[1],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[1])\n",
    "\n",
    "\n",
    "rects3 = plt.bar(index + 2*bar_width, order_type_array[:,2], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[2],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[2])\n",
    "\n",
    "rects4 = plt.bar(index + 3*bar_width, order_type_array[:,3], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[3],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[3])\n",
    "\n",
    "rects5 = plt.bar(index + 4*bar_width, order_type_array[:,4], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[4],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[4])\n",
    "\n",
    "\n",
    "plt.xlabel('Stock Ticker')\n",
    "plt.ylabel('Numbers')\n",
    "plt.title('Order Book Types')\n",
    "plt.xticks(index + bar_width*2.5, ticker_list)\n",
    "plt.yticks(np.arange(0, 700000,50000))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Plot the arbitrage situation (bid high, ask low and no arbitrage)\n",
    "\n",
    "Take the first stock which is AAPL as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_order_reduced=data_order_list[0][(time_index_list[0]>= start_ind) & (time_index_list[0]<= end_ind)]\n",
    "data_mess_reduced=data_mess_list[0][(time_index_list[0]>= start_ind) & (time_index_list[0]<= end_ind)]\n",
    "time_index_reduced=time_index_list[0][(time_index_list[0]>= start_ind) & (time_index_list[0]<= end_ind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_ind=np.where(ask_low_time_list[0][1]==1)[0][0]\n",
    "last_ind=np.where(time_index_reduced>time_index_reduced[first_ind]+5)[0][0]\n",
    "print(\"first_ind:\",first_ind)\n",
    "print(\"last_ind:\",last_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "time_index=time_index_reduced[first_ind:last_ind+1]\n",
    "ask_price=data_order_reduced[first_ind:last_ind+1,0]\n",
    "bid_price=data_order_reduced[first_ind:last_ind+1,2]\n",
    "print(ask_pirce[1])\n",
    "print(bid_price[1])\n",
    "plt.plot(time_index,ask_price,'r.-',label=\"Ask Price\")\n",
    "plt.plot(time_index,bid_price,'b.-',label=\"Bid Price\")\n",
    "\n",
    "plt.xticks=time_index\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Ask Low Arbitrage Example\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.where(bid_high_time_list[0][1]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the bid high case\n",
    "\n",
    "first_ind=np.where(bid_high_time_list[0][1]==1)[0][20]\n",
    "last_ind=np.where(time_index_list[0]>time_index_list[0][first_ind]+5)[0][0]\n",
    "print(\"first_ind:\",first_ind)\n",
    "print(\"last_ind:\",last_ind)\n",
    "\n",
    "%matplotlib qt\n",
    "time_index=time_index_list[0][first_ind:last_ind+1]\n",
    "ask_price=data_order_list[0][first_ind:last_ind+1,0]\n",
    "bid_price=data_order_list[0][first_ind:last_ind+1,2]\n",
    "print(ask_pirce[1])\n",
    "print(bid_price[1])\n",
    "plt.plot(time_index,ask_price,'r.-',label=\"Ask Price\")\n",
    "plt.plot(time_index,bid_price,'b.-',label=\"Bid Price\")\n",
    "\n",
    "plt.xticks=time_index\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Bid High Arbitrage Example\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the no arbitrage case\n",
    "first_ind=np.where(no_arbi_time_list[0][1]==1)[0][20]\n",
    "last_ind=np.where(time_index_list[0]>time_index_list[0][first_ind]+5)[0][0]\n",
    "print(\"first_ind:\",first_ind)\n",
    "print(\"last_ind:\",last_ind)\n",
    "\n",
    "%matplotlib qt\n",
    "time_index=time_index_list[0][first_ind:last_ind+1]\n",
    "ask_price=data_order_list[0][first_ind:last_ind+1,0]\n",
    "bid_price=data_order_list[0][first_ind:last_ind+1,2]\n",
    "print(ask_pirce[1])\n",
    "print(bid_price[1])\n",
    "plt.plot(time_index,ask_price,'r.-',label=\"Ask Price\")\n",
    "plt.plot(time_index,bid_price,'b.-',label=\"Bid Price\")\n",
    "\n",
    "plt.xticks=time_index\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"No Arbitrage Example\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.plot the statistical properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) cumulative distribution function for arrival time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticker_ind=2\n",
    "data=data_mess_list[ticker_ind]\n",
    "# we use the market order\n",
    "data_order=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "\n",
    "arrival_time=data_order[1:,0]-data_order[0:-1,0]\n",
    "#delete the zero intra arrival time\n",
    "arrival_time=arrival_time[arrival_time>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu_log=np.mean(np.log(arrival_time))\n",
    "std_log=np.std(np.log(arrival_time))\n",
    "data_log=np.random.lognormal(mu_log,std_log,arrival_time.shape)\n",
    "\n",
    "mu_exp=np.mean(arrival_time)\n",
    "data_exp=np.random.exponential(mu_exp,arrival_time.shape)\n",
    "\n",
    "data_weibull=np.random.weibull(0.38,arrival_time.shape)\n",
    "beta=np.var(arrival_time)/np.mean(arrival_time)\n",
    "alpha=np.mean(arrival_time)/beta\n",
    "data_gamma=np.random.gamma(alpha,beta,arrival_time.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.stats import lognorm\n",
    "ecdf = sm.distributions.ECDF(arrival_time,)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"b\",label=\"Original data\")\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_log)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"g\",label=\"Lognormal Distribution\")\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_exp)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"y\",label=\"Exponential distribution\")\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_weibull)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"r\",label=\"Weibull distribution\")\n",
    "\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_t)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"purple\",label=\"Gamma distribution\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Intra-arrival time\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Cumulative distribution function of order arrival time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) loop for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2, 2,figsize=(13, 13))\n",
    "for tickerb_ind in range(1,5):\n",
    "    data=data_mess_list[ticker_ind]\n",
    "    # we use the market order\n",
    "    data_order=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "\n",
    "    arrival_time=data_order[1:,0]-data_order[0:-1,0]\n",
    "    #delete the zero intra arrival time\n",
    "    arrival_time=arrival_time[arrival_time>0]\n",
    "    mu_log=np.mean(np.log(arrival_time))\n",
    "    std_log=np.std(np.log(arrival_time))\n",
    "    data_log=np.random.lognormal(mu_log,std_log,arrival_time.shape)\n",
    "\n",
    "    mu_exp=np.mean(arrival_time)\n",
    "    data_exp=np.random.exponential(mu_exp,arrival_time.shape)\n",
    "\n",
    "    data_weibull=np.random.weibull(0.38,arrival_time.shape)\n",
    "    beta=np.var(arrival_time)/np.mean(arrival_time)\n",
    "    alpha=np.mean(arrival_time)/beta\n",
    "    data_gamma=np.random.gamma(alpha,beta,arrival_time.shape)\n",
    "    ecdf = sm.distributions.ECDF(arrival_time,)\n",
    "   \n",
    "  \n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"b\",label=\"Original data\")\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_log)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"g\",label=\"Lognormal Distribution\")\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_exp)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"y\",label=\"Exponential distribution\")\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_weibull)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"r\",label=\"Weibull distribution\")\n",
    "\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_t)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"purple\",label=\"Gamma distribution\")\n",
    "    \n",
    "    \n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlabel(\"Intra-arrival time\")\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_ylabel(\"Probability\")\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].legend(loc=\"lower right\")\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_title(\"Cumulative distribution function of \\n order arrival time  for stock \"+ticker_list[ticker_ind])\n",
    "\n",
    "plt.savefig('arrival_time.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) volume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.stats import lognorm  \n",
    "import seaborn as sns\n",
    "ticker_ind=0\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_AAPL.png\")\n",
    "plt.show()\n",
    "\n",
    "ticker_ind=1\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_AMZN.png\")\n",
    "plt.show()\n",
    "\n",
    "ticker_ind=3\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale)+1.2, shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_INTC.png\")\n",
    "plt.show()\n",
    "\n",
    "ticker_ind=4\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale)+1.2, shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_MSFT.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3) Intraday seasonality\n",
    "\n",
    "observe the volume during the whole day under 5 minutes time bins. show the result of seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticker_ind=0\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "data_mess_limit=data_mess[data_mess[:,1]==1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calute the volume of limit order book in each time interval\n",
    "\n",
    "time_interval=np.linspace(data_mess_limit[:,0].min(),data_mess_limit[:,0].max(),78)\n",
    "vol=0\n",
    "vol_time=[]\n",
    "j=1\n",
    "\n",
    "for i in range(len(data_mess_limit)):\n",
    "    if  data_mess_limit[i,0]<=time_interval[j]:\n",
    "        vol=vol+data_mess_limit[i,3]\n",
    "    else: \n",
    "        j=j+1\n",
    "        vol_time.append(vol)\n",
    "        vol=data_mess_limit[i,3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the quadratic fit and vol_time\n",
    "x=range(76)\n",
    "plt.plot(x,vol_time,label=ticker_list[ticker_ind])\n",
    "qua_fit=np.poly1d(np.polyfit(x, vol_time, 2))(x)\n",
    "plt.plot(x,qua_fit,label=ticker_list[ticker_ind]+\" quadratic fit\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "xticks=np.arange(34200,57600,2400)\n",
    "plt.xticks(x[::8],xticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for limit order \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "for ticker_ind in range(1,5):\n",
    "\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_mess_limit=data_mess[data_mess[:,1]==1,:]\n",
    "    # calute the volume of limit order book in each time interval\n",
    "\n",
    "    time_interval=np.linspace(data_mess_limit[:,0].min(),data_mess_limit[:,0].max(),78)\n",
    "    vol=0\n",
    "    vol_time=[]\n",
    "    j=1\n",
    "\n",
    "    for i in range(len(data_mess_limit)):\n",
    "        if  data_mess_limit[i,0]<=time_interval[j]:\n",
    "            vol=vol+data_mess_limit[i,3]\n",
    "        else: \n",
    "            j=j+1\n",
    "            vol_time.append(vol)\n",
    "            vol=data_mess_limit[i,3]\n",
    "    # plot the quadratic fit and vol_time\n",
    "    x=range(76)\n",
    "    plt.plot(x,vol_time,\"+-\",label=ticker_list[ticker_ind])\n",
    "    qua_fit=np.poly1d(np.polyfit(x, vol_time, 2))(x)\n",
    "    plt.plot(x,qua_fit,\"--\",label=ticker_list[ticker_ind]+\" quadratic fit\")\n",
    "    plt.legend(loc=\"upper center\")\n",
    "    xticks=np.arange(34200,57600,2400)\n",
    "    plt.xticks(x[::8],xticks)\n",
    "    plt.title(\"Number of limit orders in a 5-minute interval for \"+ticker_list[ticker_ind])\n",
    "    plt.xlabel(\"Time of day(seconds)\")\n",
    "    plt.ylabel(\"Number of limit orders submitted in $\\Delta_t=5$ minutes\")\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_limit_vol_time.png\",bbox_inches='tight')\n",
    "\n",
    "    plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# market order\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "for ticker_ind in range(1,5):\n",
    "\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_mess_market=data_mess[(data_mess[:,1]==4) | (data_mess[:,1]==5),:]\n",
    "    # calute the volume of limit order book in each time interval\n",
    "\n",
    "    time_interval=np.linspace(data_mess_market[:,0].min(),data_mess_market[:,0].max(),78)\n",
    "    vol=0\n",
    "    vol_time=[]\n",
    "    j=1\n",
    "\n",
    "    for i in range(len(data_mess_market)):\n",
    "        if  data_mess_market[i,0]<=time_interval[j]:\n",
    "            vol=vol+data_mess_market[i,3]\n",
    "        else: \n",
    "            j=j+1\n",
    "            vol_time.append(vol)\n",
    "            vol=data_mess_market[i,3]\n",
    "    # plot the quadratic fit and vol_time\n",
    "    x=range(76)\n",
    "    plt.plot(x,vol_time,\"+-\",label=ticker_list[ticker_ind])\n",
    "    qua_fit=np.poly1d(np.polyfit(x, vol_time, 2))(x)\n",
    "    plt.plot(x,qua_fit,\"--\",label=ticker_list[ticker_ind]+\" quadratic fit\")\n",
    "    plt.legend(loc=\"upper center\")\n",
    "    xticks=np.arange(34200,57600,2400)\n",
    "    plt.xticks(x[::8],xticks)\n",
    "    plt.title(\"Number of market orders in a 5-minute interval for \"+ticker_list[ticker_ind])\n",
    "    plt.xlabel(\"Time of day(seconds)\")\n",
    "    plt.ylabel(\"Number of market orders submitted in $\\Delta_t=5$ minutes\")\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_market_vol_time.png\",bbox_inches='tight')\n",
    "    plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) average shape of the order books\n",
    "find the total volume for all each price level and see the volume trend based on the price levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 4) average shape of the order books\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "ticker_ind=1\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "data_order=data_order_list[ticker_ind]\n",
    "data_order_limit_ask_vol=data_order[data_mess[:,1]==1,1:40:4]\n",
    "data_order_limit_bid_vol=data_order[data_mess[:,1]==1,3:40:4]\n",
    "\n",
    "vol_ask=np.sum(data_order_limit_ask_vol,axis=0)/np.mean(np.sum(data_order_limit_ask_vol,axis=0))\n",
    "vol_bid=np.sum(data_order_limit_bid_vol,axis=0)/np.mean(np.sum(data_order_limit_bid_vol,axis=0))\n",
    "plt.plot(list(range(-10,0)),vol_bid)\n",
    "plt.plot(list(range(1,11)),vol_ask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop the stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "marker_list=[\"s\",\"D\",\"^\",\"8\"]\n",
    "color_list=[\"g\",\"b\",\"r\",\"y\"]\n",
    "for ticker_ind in range(1,5):\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "    data_order_limit_ask_vol=data_order[:,1:40:4]\n",
    "    data_order_limit_bid_vol=data_order[:,3:40:4]\n",
    "\n",
    "    vol_ask=np.sum(data_order_limit_ask_vol,axis=0)/np.mean(np.sum(data_order_limit_ask_vol,axis=0))\n",
    "    vol_bid=np.sum(data_order_limit_bid_vol,axis=0)/np.mean(np.sum(data_order_limit_bid_vol,axis=0))\n",
    "    plt.plot(list(range(-10,0)),vol_bid,\n",
    "             \"--\",marker=marker_list[ticker_ind-1],color=color_list[ticker_ind-1],label=\n",
    "            ticker_list[ticker_ind])\n",
    "    plt.plot(list(range(1,11)),vol_ask,\"--\",marker=marker_list[ticker_ind-1],color=color_list[ticker_ind-1])\n",
    "plt.ylim([0.6,1.6])\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Average quantity offered in the market order book\")\n",
    "plt.xlabel(\"Price level of limit orders (negative axis : bids ; positive axis : asks)\")\n",
    "plt.ylabel(\"Average numbers of shares(Normalized by mean)\")\n",
    "plt.savefig(\"level_quantity.png\",bbox_inches='tight')    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) placement of orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticker_ind=2\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "data_order=data_order_list[ticker_ind]\n",
    "\n",
    "data_mess_limit=data_mess[data_mess[:,1]==1,:]\n",
    "data_order_limit=data_order[data_mess[:,1]==1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spread_list=[]\n",
    "for i in range(1,len(data_mess_limit)):\n",
    "    if data_mess_limit[i,5]==-1:\n",
    "        spread=data_mess_limit[i,4]-data_order_limit[i-1,0]\n",
    "    else:\n",
    "        spread=data_order_limit[i-1,2]-data_mess_limit[i,4]\n",
    "    spread_list.append(spread)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import math\n",
    "sns.kdeplot(np.array(spread_list), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "mu = 0\n",
    "variance = np.var(spread_list)\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(min(spread_list), max(spread_list), 100)\n",
    "plt.plot(x,mlab.normpdf(x, mu, sigma),\"r--\",label=\"Gaussian\")\n",
    "plt.xlim([-10000,10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ticker_ind in range(1,5):\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "\n",
    "    data_mess_limit=data_mess[data_mess[:,1]==1,:]\n",
    "    data_order_limit=data_order[data_mess[:,1]==1,:]\n",
    "\n",
    "    spread_list=[]\n",
    "    for i in range(1,len(data_mess_limit)):\n",
    "        if data_mess_limit[i,5]==-1:\n",
    "            spread=data_mess_limit[i,4]-data_order_limit[i-1,0]\n",
    "        else:\n",
    "            spread=data_order_limit[i-1,2]-data_mess_limit[i,4]\n",
    "        spread_list.append(spread)\n",
    "\n",
    "\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import matplotlib.mlab as mlab\n",
    "    import math\n",
    "    sns.kdeplot(np.array(spread_list), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "    mu = 0\n",
    "    variance = np.var(spread_list)\n",
    "    sigma = math.sqrt(variance)\n",
    "    x = np.linspace(min(spread_list), max(spread_list), 100)\n",
    "    plt.plot(x,mlab.normpdf(x, mu, sigma),\"r--\",label=\"Gaussian\")\n",
    "    plt.xlim([min(spread_list)*0.8,max(spread_list)*0.8])\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Placement of limit orders using the\\n same best quote reference for \"+ticker_list[ticker_ind])\n",
    "    plt.xlabel(\"Price diference\")\n",
    "    plt.ylabel(\"Probability density function\")\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_placement.png\",bbox_inches='tight')    \n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
