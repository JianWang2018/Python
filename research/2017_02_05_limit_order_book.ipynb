{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit order book\n",
    "\n",
    "author: Jian Wang\n",
    "\n",
    "time: 2016-02-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Model prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for importing the AAPL data is: 2.2476625442504883\n",
      "The shape of the order data is:  (400391, 40)  of message data is:  (400391, 6)\n",
      "Time for importing the AMZN data is: 1.3033385276794434\n",
      "The shape of the order data is:  (269748, 40)  of message data is:  (269748, 6)\n",
      "Time for importing the GOOG data is: 0.8174479007720947\n",
      "The shape of the order data is:  (147916, 40)  of message data is:  (147916, 6)\n",
      "Time for importing the INTC data is: 3.5862882137298584\n",
      "The shape of the order data is:  (624040, 40)  of message data is:  (624040, 6)\n",
      "Time for importing the MSFT data is: 3.8824493885040283\n",
      "The shape of the order data is:  (668765, 40)  of message data is:  (668765, 6)\n",
      "Check the original data:\n",
      "\n",
      "The first five sampe of AAPL is:  [[  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86100000e+06   2.00000000e+02   5.85100000e+06   5.00000000e+00\n",
      "    5.86890000e+06   3.00000000e+02   5.85010000e+06   8.90000000e+01\n",
      "    5.86950000e+06   5.00000000e+01   5.84970000e+06   5.00000000e+00\n",
      "    5.87000000e+06   1.00000000e+02   5.84930000e+06   3.00000000e+02\n",
      "    5.87100000e+06   1.00000000e+01   5.84650000e+06   3.00000000e+02\n",
      "    5.87390000e+06   1.00000000e+02   5.84530000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84380000e+06   2.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84270000e+06   3.00000000e+02]\n",
      " [  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85320000e+06   1.80000000e+01\n",
      "    5.86100000e+06   2.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86890000e+06   3.00000000e+02   5.85100000e+06   5.00000000e+00\n",
      "    5.86950000e+06   5.00000000e+01   5.85010000e+06   8.90000000e+01\n",
      "    5.87000000e+06   1.00000000e+02   5.84970000e+06   5.00000000e+00\n",
      "    5.87100000e+06   1.00000000e+01   5.84930000e+06   3.00000000e+02\n",
      "    5.87390000e+06   1.00000000e+02   5.84650000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84530000e+06   3.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84380000e+06   2.00000000e+02]\n",
      " [  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85320000e+06   1.80000000e+01\n",
      "    5.86100000e+06   2.00000000e+02   5.85310000e+06   1.80000000e+01\n",
      "    5.86890000e+06   3.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86950000e+06   5.00000000e+01   5.85100000e+06   5.00000000e+00\n",
      "    5.87000000e+06   1.00000000e+02   5.85010000e+06   8.90000000e+01\n",
      "    5.87100000e+06   1.00000000e+01   5.84970000e+06   5.00000000e+00\n",
      "    5.87390000e+06   1.00000000e+02   5.84930000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84650000e+06   3.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84530000e+06   3.00000000e+02]]\n",
      "\n",
      "The first five sampe of AMZN is:  [[  2.23950000e+06   1.00000000e+02   2.23180000e+06   1.00000000e+02\n",
      "    2.23990000e+06   1.00000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24400000e+06   5.47000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24540000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.24890000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.26770000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29430000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02\n",
      "    2.29800000e+06   1.00000000e+02   2.18970000e+06   1.00000000e+02]\n",
      " [  2.23950000e+06   1.00000000e+02   2.23810000e+06   2.10000000e+01\n",
      "    2.23990000e+06   1.00000000e+02   2.23180000e+06   1.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24400000e+06   5.47000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24540000e+06   1.00000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24890000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.26770000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.29430000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29800000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02]\n",
      " [  2.23950000e+06   1.00000000e+02   2.23810000e+06   2.10000000e+01\n",
      "    2.23960000e+06   2.00000000e+01   2.23180000e+06   1.00000000e+02\n",
      "    2.23990000e+06   1.00000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24400000e+06   5.47000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24540000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.24890000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.26770000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29430000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02]]\n",
      "\n",
      "The first five sampe of GOOG is:  [[  5.80230000e+06   1.00000000e+02   5.79400000e+06   4.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]\n",
      " [  5.80230000e+06   1.00000000e+02   5.79400000e+06   1.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]\n",
      " [  5.80230000e+06   1.00000000e+02   5.79400000e+06   1.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]]\n",
      "\n",
      "The first five sampe of INTC is:  [[  2.75200000e+05   6.60000000e+01   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74600000e+05   7.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74500000e+05   9.00000000e+02\n",
      "    2.76100000e+05   2.30000000e+03   2.74400000e+05   2.80000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76300000e+05   2.00000000e+03   2.74200000e+05   4.06300000e+03]\n",
      " [  2.75200000e+05   1.66000000e+02   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74600000e+05   7.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74500000e+05   9.00000000e+02\n",
      "    2.76100000e+05   2.30000000e+03   2.74400000e+05   2.80000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76300000e+05   2.00000000e+03   2.74200000e+05   4.06300000e+03]\n",
      " [  2.75200000e+05   1.66000000e+02   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75500000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74600000e+05   7.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74500000e+05   9.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74400000e+05   2.80000000e+03\n",
      "    2.76100000e+05   2.30000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74200000e+05   4.06300000e+03]]\n",
      "\n",
      "The first five sampe of MSFT is:  [[  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10500000e+05   1.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10600000e+05   1.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08800000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08500000e+05   4.00000000e+02\n",
      "    3.11400000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]\n",
      " [  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10500000e+05   2.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10600000e+05   1.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08800000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08500000e+05   4.00000000e+02\n",
      "    3.11400000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]\n",
      " [  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10400000e+05   1.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10500000e+05   2.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10600000e+05   1.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08800000e+05   2.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08500000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]]\n",
      "82.3326678276062\n",
      "The shape of the total response is:\n",
      "\n",
      "(400236, 1)\n",
      "(269571, 1)\n",
      "(147766, 1)\n",
      "(622641, 1)\n",
      "(667701, 1)\n",
      "The shape of the reduced response is:\n",
      "\n",
      "(309538, 1)\n",
      "(218710, 1)\n",
      "(118877, 1)\n",
      "(458160, 1)\n",
      "(511299, 1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Aug 26 00:03:47 2016\n",
    "\n",
    "@author: jianwang\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set default parameters\n",
    "ticker_list=[\"AAPL\",\"AMZN\",\"GOOG\",\"INTC\",\"MSFT\"]\n",
    "start_ind=10*3600\n",
    "end_ind=15.5*3600\n",
    "data_order_list=[]\n",
    "data_mess_list=[]\n",
    "time_index_list=[]\n",
    "path_save='/media/jianwang/Study/Research/order_book/'\n",
    "path_load=\"/media/jianwang/Study/Research/order_book/\"\n",
    "\n",
    "## set random seed to produce the same results\n",
    "\n",
    "np.random.seed(987612345)\n",
    "\n",
    "#read the stock ticker\n",
    "#totally 5 dataset\n",
    "\n",
    "for i in range(len(ticker_list)):\n",
    "    #get the path for the csv files\n",
    "    # name_order is for the order book and name_mess for the message book\n",
    "    name_order='_2012-06-21_34200000_57600000_orderbook_10.csv'\n",
    "    name_mess='_2012-06-21_34200000_57600000_message_10.csv'\n",
    "    # calculate the cputime for reading the data\n",
    "    t=time.time()\n",
    "    # header =-1 means that the first line is not the header, otherwise, the first line will be header\n",
    "    # data_order is for order book and data mess is for message book\n",
    "    data_order_list.append(np.array(pd.read_csv(path_load+ticker_list[i]+name_order,header=-1),dtype=\"float64\"))\n",
    "    data_mess_list.append(np.array(pd.read_csv(path_load+ticker_list[i]+name_mess,header=-1),dtype=\"float64\"))\n",
    "    print(\"Time for importing the \"+ticker_list[i]+\" data is:\",time.time()-t)\n",
    "    print(\"The shape of the order data is: \",data_order_list[i].shape, \" of message data is: \", data_mess_list[i].shape)\n",
    "    # get the time index\n",
    "    time_index_list.append(data_mess＿list[i][:,0])\n",
    "\n",
    "\n",
    "#print the sample of data\n",
    "print(\"Check the original data:\")\n",
    "\n",
    "for i in range(len(ticker_list)):\n",
    "    print()\n",
    "    print(\"The first five sampe of \"+ticker_list[i]+\" is: \",data_order_list[i][:3])\n",
    "\n",
    "    # -*- coding: utf-8 -*-\n",
    "\n",
    "# # save the feature array\n",
    "# ##get the original order,message and time index data, header =-1 means that did not\n",
    "# ##read the first column as the name\n",
    "#%%\n",
    "# # use a loop to read data\n",
    "# for ticker_ind in range(len(ticker_list)):\n",
    "#     data_order=data_order_list[ticker_ind]\n",
    "#     data_mess=data_mess_list[ticker_ind]\n",
    "#     time_index=data_mess[:,0]\n",
    "#     # obtain the reduced order message and time_index dataset, half an hour after the\n",
    "#     # 9:30 and half an hour before 16:00\n",
    "#     # data_reduced is used to install the data from 10 to 15:30, take half hour for auction\n",
    "#     data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "#     data_mess_reduced=data_mess[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "#     time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "\n",
    "#     test_lower=0\n",
    "#     # test up is the up index of the original data to construct the test data\n",
    "#     test_upper=len(data_order_reduced)\n",
    "#     # data_test is the subset of data_reduced from the lower index to upper index\n",
    "#     data_order_test=data_order_reduced[test_lower:test_upper,:]\n",
    "#     data_mess_test=data_mess_reduced[test_lower:test_upper,:]\n",
    "#     t=time.time()\n",
    "#     feature_array=get_features (data_order, data_mess,data_order_test,data_mess_test)\n",
    "#     np.savetxt(path_save+ticker_list[ticker_ind]+'_feature_array.txt',feature_array,delimiter=' ')\n",
    "#     print (\"Time for building \"+ticker_list[ticker_ind]+\" is:\",time.time()-t)\n",
    "\n",
    "\n",
    "# load the feature\n",
    "#%%\n",
    "import time\n",
    "t=time.time()\n",
    "feature_array_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    feature_array_list.append(np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_feature_array.txt',\\\n",
    "                                                   sep=' ',header=-1)))\n",
    "print(time.time()-t)\n",
    "\n",
    "## save y data\n",
    "#%%\n",
    "#time_ind=1\n",
    "#option_ind=1\n",
    "#for ticker_ind in range(len(ticker_list)):\n",
    "#    response=build_y(ask_low_time_list[ticker_ind][time_ind],bid_high_time_list[ticker_ind][time_ind],\\\n",
    "#                                 no_arbi_time_list[ticker_ind][time_ind],option=option_ind)\n",
    "#    np.savetxt(path_save+ticker_list[ticker_ind]+'_response.txt',response)\n",
    "\n",
    "\n",
    "\n",
    "## load y data\n",
    "#%%\n",
    "response_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    response_list.append((np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_response.txt',header=-1))))\n",
    "\n",
    "\n",
    "## print the shape of the response\n",
    "## note it is the total response\n",
    "#%%\n",
    "print(\"The shape of the total response is:\\n\")\n",
    "\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_list[ticker_ind].shape)\n",
    "\n",
    "# need to get the response from 10 to 15:30\n",
    "# the shape of the response and the feature array should be equal\n",
    "response_reduced_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    first_ind = np.where(time_index_list[ticker_ind]>=start_ind)[0][0]\n",
    "    last_ind=np.where(time_index_list[ticker_ind]<=end_ind)[0][-1]\n",
    "    response_reduced_list.append(response_list[ticker_ind][first_ind:last_ind+1])\n",
    "\n",
    "print(\"The shape of the reduced response is:\\n\")\n",
    "\n",
    "## print the shape of reduced response\n",
    "## response reduced is used for testing and training the model\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_reduced_list[ticker_ind].shape)\n",
    "\n",
    "    \n",
    "    # random generate a given \n",
    "def random_choice(num, key):\n",
    "    temp=np.random.choice(num,size=key,replace=False)\n",
    "    temp_sort=sorted(temp)\n",
    "    for i in range(len(temp)):\n",
    "        num[temp_sort[i]]=temp[i]\n",
    "    \n",
    "    return num\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.train and test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# Random split\n",
    "#%%---------------------------------------------------------------------\n",
    "import random\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "ticker_ind=1\n",
    "size=100000\n",
    "\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x, test_x, train_y, test_y =train_test_split(\\\n",
    "total_array[:,:134],total_array[:,134], test_size=0.1, random_state=42)\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.,   100.,   200.,   300.,   400.,   500.,   600.,   700.,\n",
       "         800.,   900.,  1000.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0,1000,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total array shape: (100000, 135)\n",
      "train_x shape: (90000, 134)\n",
      "test_x shape: (10000, 134)\n",
      "test_y shape: (10000,)\n",
      "train_y shape: (90000,)\n",
      "34.42207956314087\n",
      "train_accuracy is: 0.998077777778\n",
      "precision is: \t 0.915330661323\n",
      "recall is: \t 0.99781540142\n",
      "f1 score is: \t 0.954794878495\n",
      "test time is:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.10674309730529785\n",
      "test accuracy is: 0.997\n",
      "precision is: \t 0.817610062893\n",
      "recall is: \t 0.992366412214\n",
      "f1 score is: \t 0.896551724138\n",
      "total array shape: (110000, 135)\n",
      "train_x shape: (99000, 134)\n",
      "test_x shape: (11000, 134)\n",
      "test_y shape: (11000,)\n",
      "train_y shape: (99000,)\n",
      "40.31476354598999\n",
      "train_accuracy is: 0.996838383838\n",
      "precision is: \t 0.872093023256\n",
      "recall is: \t 0.997624703088\n",
      "f1 score is: \t 0.9306448039\n",
      "test time is: 0.12062287330627441\n",
      "test accuracy is: 0.994909090909\n",
      "precision is: \t 0.868932038835\n",
      "recall is: \t 0.994444444444\n",
      "f1 score is: \t 0.927461139896\n",
      "total array shape: (120000, 135)\n",
      "train_x shape: (108000, 134)\n",
      "test_x shape: (12000, 134)\n",
      "test_y shape: (12000,)\n",
      "train_y shape: (108000,)\n",
      "41.00515079498291\n",
      "train_accuracy is: 0.997814814815\n",
      "precision is: \t 0.908231587239\n",
      "recall is: \t 0.998700736249\n",
      "f1 score is: \t 0.951320132013\n",
      "test time is:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.14142680168151855\n",
      "test accuracy is: 0.997333333333\n",
      "precision is: \t 0.905198776758\n",
      "recall is: \t 0.996632996633\n",
      "f1 score is: \t 0.948717948718\n",
      "total array shape: (130000, 135)\n",
      "train_x shape: (117000, 134)\n",
      "test_x shape: (13000, 134)\n",
      "test_y shape: (13000,)\n",
      "train_y shape: (117000,)\n",
      "48.5468385219574\n",
      "train_accuracy is: 0.997307692308\n",
      "precision is: \t 0.886488465763\n",
      "recall is: \t 0.997938994229\n",
      "f1 score is: \t 0.938917975567\n",
      "test time is: 0.15782666206359863\n",
      "test accuracy is: 0.997230769231\n",
      "precision is: \t 0.861111111111\n",
      "recall is: \t 0.995412844037\n",
      "f1 score is: \t 0.923404255319\n",
      "total array shape: (140000, 135)\n",
      "train_x shape: (126000, 134)\n",
      "test_x shape: (14000, 134)\n",
      "test_y shape: (14000,)\n",
      "train_y shape: (126000,)\n",
      "50.54997944831848\n",
      "train_accuracy is: 0.997341269841\n",
      "precision is: \t 0.886114911081\n",
      "recall is: \t 0.999228692634\n",
      "f1 score is: \t 0.939278593438\n",
      "test time is:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.16013145446777344\n",
      "test accuracy is: 0.996\n",
      "precision is: \t 0.8\n",
      "recall is: \t 1.0\n",
      "f1 score is: \t 0.888888888889\n",
      "total array shape: (150000, 135)\n",
      "train_x shape: (135000, 134)\n",
      "test_x shape: (15000, 134)\n",
      "test_y shape: (15000,)\n",
      "train_y shape: (135000,)\n",
      "55.0678505897522\n",
      "train_accuracy is: 0.996903703704\n",
      "precision is: \t 0.866451612903\n",
      "recall is: \t 0.998513011152\n",
      "f1 score is: \t 0.92780656304\n",
      "test time is: 0.167219877243042\n",
      "test accuracy is: 0.9956\n",
      "precision is: \t 0.788273615635\n",
      "recall is: \t 0.995884773663\n",
      "f1 score is: \t 0.88\n",
      "total array shape: (160000, 135)\n",
      "train_x shape: (144000, 134)\n",
      "test_x shape: (16000, 134)\n",
      "test_y shape: (16000,)\n",
      "train_y shape: (144000,)\n",
      "63.08138370513916\n",
      "train_accuracy is: 0.996375\n",
      "precision is: \t 0.834710743802\n",
      "recall is: \t 0.999238964992\n",
      "f1 score is: \t 0.909594735019\n",
      "test time is:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.1772010326385498\n",
      "test accuracy is: 0.9958125\n",
      "precision is: \t 0.767527675277\n",
      "recall is: \t 0.981132075472\n",
      "f1 score is: \t 0.861283643892\n",
      "total array shape: (170000, 135)\n",
      "train_x shape: (153000, 134)\n",
      "test_x shape: (17000, 134)\n",
      "test_y shape: (17000,)\n",
      "train_y shape: (153000,)\n",
      "67.66211152076721\n",
      "train_accuracy is: 0.995718954248\n",
      "precision is: \t 0.800184162063\n",
      "recall is: \t 0.998468019916\n",
      "f1 score is: \t 0.888396660419\n",
      "test time is: 0.18916845321655273\n",
      "test accuracy is: 0.994882352941\n",
      "precision is: \t 0.715231788079\n",
      "recall is: \t 0.995391705069\n",
      "f1 score is: \t 0.832369942197\n",
      "total array shape: (180000, 135)\n",
      "train_x shape: (162000, 134)\n",
      "test_x shape: (18000, 134)\n",
      "test_y shape: (18000,)\n",
      "train_y shape: (162000,)\n",
      "71.98944616317749\n",
      "train_accuracy is: 0.995290123457\n",
      "precision is: \t 0.779994217982\n",
      "recall is: \t 0.999259259259\n",
      "f1 score is: \t 0.876116252638\n",
      "test time is: 0.19916534423828125\n",
      "test accuracy is: 0.993388888889\n",
      "precision is: \t 0.727688787185"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall is: \t 1.0\n",
      "f1 score is: \t 0.84238410596\n",
      "total array shape: (190000, 135)\n",
      "train_x shape: (171000, 134)\n",
      "test_x shape: (19000, 134)\n",
      "test_y shape: (19000,)\n",
      "train_y shape: (171000,)\n",
      "79.27789831161499\n",
      "train_accuracy is: 0.995555555556\n",
      "precision is: \t 0.788692975091\n",
      "recall is: \t 0.998228834573\n",
      "f1 score is: \t 0.881175734834\n",
      "test time is: 0.21183562278747559\n",
      "test accuracy is: 0.994631578947\n",
      "precision is: \t 0.722991689751\n",
      "recall is: \t 0.992395437262\n",
      "f1 score is: \t 0.836538461538\n",
      "total array shape: (200000, 135)\n",
      "train_x shape: (180000, 134)\n",
      "test_x shape: (20000, 134)\n",
      "test_y shape: (20000,)\n",
      "train_y shape: (180000,)\n",
      "85.01419425010681\n",
      "train_accuracy is: 0.995772222222\n",
      "precision is: \t 0.801366263794\n",
      "recall is: \t 0.998363338789\n",
      "f1 score is: \t 0.88908322402\n",
      "test time is: 0.22783470153808594\n",
      "test accuracy is: 0.99415\n",
      "precision is: \t 0.741496598639\n",
      "recall is: \t 0.990909090909\n",
      "f1 score is: \t 0.848249027237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# data_stability test\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "ticker_ind=1\n",
    "f1_list=[]\n",
    "size_list=np.linspace(100000,200000,11)\n",
    "random_ratio=0.7\n",
    "\n",
    "for size in size_list:\n",
    "    \n",
    "# combine the feature and response array to random sample\n",
    "    total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "    total_array=total_array[random_choice(list(range(int(size))),int(size*random_ratio)),:]\n",
    "\n",
    "\n",
    "    train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "    print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "    #split the data to train and test data set\n",
    "    train_x=total_array[:train_num_index,:134]\n",
    "    test_x=total_array[train_num_index:,:134]\n",
    "    train_y=total_array[:train_num_index,134]\n",
    "    test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "    # the y data need to reshape to size (n,) not (n,1)\n",
    "    test_y=test_y.reshape(len(test_y),)\n",
    "    train_y=train_y.reshape(len(train_y),)\n",
    "    print(\"train_x shape:\",train_x.shape)\n",
    "    print(\"test_x shape:\",test_x.shape)\n",
    "    print(\"test_y shape:\",test_y.shape)\n",
    "    print(\"train_y shape:\",train_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # scale data\n",
    "    #%%\n",
    "\n",
    "    # can use the processing.scale function to scale the data\n",
    "    from sklearn import preprocessing\n",
    "    # note that we need to transfer the data type to float\n",
    "    # remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "    # use scale for zero mean and one std\n",
    "    scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "    train_x_scale=scaler.transform(train_x)\n",
    "    test_x_scale=scaler.transform(test_x)\n",
    "    \n",
    "\n",
    "    # training\n",
    "\n",
    "    # change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "    time_rf=time.time()\n",
    "    clf =  RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345)\n",
    "    clf.fit(train_x_scale,train_y)\n",
    "\n",
    "    print(time.time()-time_rf)\n",
    "\n",
    "    #testing5\n",
    "    # test the training error\n",
    "    predict_y=np.array(clf.predict(train_x_scale))\n",
    "    print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "    # test the score for the train data\n",
    "    from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                                 f1_score)\n",
    "    precision= precision_score(predict_y,train_y)\n",
    "    recall = recall_score(predict_y,train_y)\n",
    "    f1=f1_score(predict_y,train_y)\n",
    "    print(\"precision is: \\t %s\" % precision)\n",
    "    print(\"recall is: \\t %s\" % recall)\n",
    "    print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "    # define a function to prefict the result by threshold\n",
    "    # note: logistic model will return two probability\n",
    "    def predict_threshold(predict_proba, threshold):\n",
    "        res=[]\n",
    "        for i in range(len(predict_proba)):\n",
    "            res.append(int(predict_proba[i][1]>threshold))\n",
    "        return res\n",
    "\n",
    "    t=time.time()\n",
    "    predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "    print(\"test time is:\", time.time()-t)\n",
    "    predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "    # test the score for the test data\n",
    "    from sklearn.metrics import (precision_score, recall_score,\n",
    "                                 f1_score)\n",
    "    print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "    precision= precision_score(predict_y_test,test_y)\n",
    "    recall = recall_score(predict_y_test,test_y)\n",
    "    f1=f1_score(predict_y_test,test_y)\n",
    "    print(\"precision is: \\t %s\" % precision)\n",
    "    print(\"recall is: \\t %s\" % recall)\n",
    "    print(\"f1 score is: \\t %s\" %f1)\n",
    "    f1_list.append(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.80913539967373571,\n",
       " 0.66482758620689653,\n",
       " 0.80948851000741295,\n",
       " 0.72080291970802912,\n",
       " 0.74459567654123304,\n",
       " 0.79210711768851305,\n",
       " 0.74155995343422587,\n",
       " 0.76234425473004164,\n",
       " 0.79891514916698969,\n",
       " 0.67586821015138032,\n",
       " 0.78657342657342666]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianwang/anaconda3/lib/python3.5/site-packages/matplotlib/axes/_axes.py:519: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "# draw the chart for the f1 score\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(size_list,f1_list,\"b.-\")\n",
    "plt.xlabel(\"Data NUmber\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.legend(bbox_to_anchor=[1.4, 1])\n",
    "plt.title(\"Data sample stability for \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"data_sample_stability_\"+ticker_list[ticker_ind]+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total array shape: (200000, 135)\n",
      "train_x shape: (180000, 134)\n",
      "test_x shape: (20000, 134)\n",
      "test_y shape: (20000,)\n",
      "train_y shape: (180000,)\n",
      "[ -2.49e-14  -6.84e-15   8.16e-15  -2.93e-15   5.65e-14  -8.17e-15\n",
      "   7.91e-16  -3.78e-15  -9.80e-15   5.28e-15  -1.78e-14  -2.89e-15\n",
      "  -1.95e-14   6.96e-16  -4.47e-15   1.40e-16   1.16e-14   5.49e-15\n",
      "  -4.54e-15   1.54e-16  -8.17e-15   5.94e-15   1.69e-14  -2.13e-15\n",
      "   1.37e-14   1.71e-15   1.05e-14  -3.25e-16  -1.27e-14  -4.07e-15\n",
      "  -1.04e-14  -3.04e-15  -1.31e-14  -3.29e-16  -1.77e-14   2.82e-15\n",
      "  -1.84e-14   7.14e-15   1.50e-14  -2.07e-15   8.03e-15  -1.01e-14\n",
      "   1.20e-14   6.82e-14   3.98e-14  -5.62e-15   4.73e-14  -3.89e-14\n",
      "  -3.96e-14   2.38e-14   1.19e-14  -9.37e-15  -1.80e-14  -1.04e-14\n",
      "   2.05e-15   3.73e-15  -1.94e-14   1.57e-14  -8.15e-15  -1.37e-14\n",
      "   4.79e-14  -2.62e-14   2.36e-15  -5.17e-14  -2.14e-14  -5.17e-14\n",
      "   1.32e-13  -1.19e-13   7.01e-14  -2.75e-14   3.90e-14  -7.57e-14\n",
      "  -1.70e-13   1.30e-13   7.90e-14   1.06e-13  -1.60e-13  -1.89e-13\n",
      "  -1.38e-15   2.09e-14  -1.86e-13  -5.00e-14  -1.61e-15   1.33e-17\n",
      "   3.66e-15  -7.45e-15  -9.37e-15   6.72e-15  -4.39e-16  -4.24e-15\n",
      "   2.78e-15  -1.71e-15   3.79e-15   2.22e-15  -4.03e-15   7.97e-15\n",
      "  -3.31e-15  -8.21e-15  -3.56e-15   1.75e-15   5.37e-15  -2.99e-15\n",
      "  -4.16e-15   2.41e-15  -1.19e-14  -1.37e-15  -8.00e-15  -3.71e-16\n",
      "   4.21e-16   1.75e-15   5.65e-15   9.68e-15   1.81e-15   1.41e-15\n",
      "   6.05e-15   1.35e-15   3.22e-15  -1.28e-15   2.54e-15  -3.35e-15\n",
      "  -5.45e-15  -2.64e-16   4.58e-15   3.33e-15  -2.47e-15   8.62e-16\n",
      "   1.76e-14  -2.47e-15   1.36e-14   1.38e-15  -1.93e-15   2.58e-16\n",
      "  -2.59e-15   1.85e-15]\n",
      "[ -4.34e-01   4.06e-02  -4.34e-01  -3.42e-02  -4.34e-01   1.00e-02\n",
      "  -4.34e-01  -2.02e-02  -4.35e-01  -3.75e-02  -4.34e-01  -1.97e-02\n",
      "  -4.35e-01  -1.87e-02  -4.33e-01  -8.33e-03  -4.34e-01  -2.04e-02\n",
      "  -4.33e-01  -1.58e-02  -4.35e-01  -3.51e-02  -4.33e-01  -2.81e-02\n",
      "  -4.35e-01  -3.26e-02  -4.32e-01  -3.16e-02  -4.35e-01   3.66e-03\n",
      "  -4.32e-01  -1.35e-02  -4.35e-01   1.34e-02  -4.31e-01  -1.82e-02\n",
      "  -4.35e-01   2.51e-02  -4.31e-01  -1.64e-02  -3.32e-02  -9.06e-02\n",
      "  -1.39e-01  -1.64e-01  -1.80e-01  -1.94e-01  -2.04e-01  -2.11e-01\n",
      "  -2.15e-01  -2.20e-01  -4.34e-01  -4.34e-01  -4.34e-01  -4.34e-01\n",
      "  -4.34e-01  -4.34e-01  -4.34e-01  -4.33e-01  -4.33e-01  -4.33e-01\n",
      "  -5.73e-02  -7.21e-02  -4.81e-02  -6.64e-02  -9.83e-02  -9.62e-02\n",
      "  -9.57e-02  -9.75e-02  -1.02e-01  -4.25e-02  -6.60e-02  -8.96e-02\n",
      "  -9.40e-02  -9.24e-02  -1.04e-01  -9.88e-02  -8.54e-02  -9.62e-02\n",
      "  -4.35e-01  -4.33e-01  -1.49e-02  -6.47e-02  -1.96e-01   5.89e-02\n",
      "   1.10e-02   2.18e-02   2.35e-02   2.23e-02   2.33e-02   2.31e-02\n",
      "   2.76e-02   2.17e-02   2.26e-02   2.41e-02   2.05e-02   4.57e-03\n",
      "   9.11e-03   8.79e-03   1.07e-02   1.01e-02   6.72e-03   1.10e-02\n",
      "   7.54e-03   8.56e-03   1.08e-02  -1.41e-02  -5.03e-03  -9.58e-03\n",
      "  -2.07e-02  -2.56e-03   6.12e-03   1.49e-02  -1.82e-02   2.29e-03\n",
      "  -6.82e-03  -1.49e-03  -4.02e-03  -3.91e-04  -8.66e-03  -6.84e-04\n",
      "   9.17e-03   7.79e-04   1.05e-02  -1.41e-02   3.31e-03   1.02e-02\n",
      "   3.07e-02  -1.60e-02   1.69e-02   6.80e-03  -9.01e-03  -1.27e-03\n",
      "   2.80e-02  -9.13e-03]\n"
     ]
    }
   ],
   "source": [
    "#time series split\n",
    "#%%--------------------------------------------------------------------------------------------\n",
    "\n",
    "ticker_ind=1\n",
    "size=200000\n",
    "random_ratio=0.7\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "total_array=total_array[random_choice(list(range(size)),int(size*random_ratio)),:]\n",
    "\n",
    "\n",
    "train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x=total_array[:train_num_index,:134]\n",
    "test_x=total_array[train_num_index:,:134]\n",
    "train_y=total_array[:train_num_index,134]\n",
    "test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "print(\"train_x shape:\",train_x.shape)\n",
    "print(\"test_x shape:\",test_x.shape)\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "# scale data\n",
    "#%%\n",
    "\n",
    "# can use the processing.scale function to scale the data\n",
    "from sklearn import preprocessing\n",
    "# note that we need to transfer the data type to float\n",
    "# remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "# use scale for zero mean and one std\n",
    "scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "train_x_scale=scaler.transform(train_x)\n",
    "test_x_scale=scaler.transform(test_x)\n",
    "\n",
    "print(np.mean(train_x_scale,0))\n",
    "print(np.mean(test_x_scale,0))\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 239627,\n",
       " 15142,\n",
       " 160838,\n",
       " 154407,\n",
       " 264244,\n",
       " 138646,\n",
       " 200007,\n",
       " 101229,\n",
       " 197488,\n",
       " 188375,\n",
       " 12,\n",
       " 257201,\n",
       " 187708,\n",
       " 185246,\n",
       " 202697,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 30325,\n",
       " 161726,\n",
       " 98310,\n",
       " 23,\n",
       " 296024,\n",
       " 136810,\n",
       " 283155,\n",
       " 34003,\n",
       " 158462,\n",
       " 29,\n",
       " 30,\n",
       " 64780,\n",
       " 32,\n",
       " 14320,\n",
       " 34,\n",
       " 167159,\n",
       " 259438,\n",
       " 256405,\n",
       " 79632,\n",
       " 39,\n",
       " 40,\n",
       " 53202,\n",
       " 122895,\n",
       " 43,\n",
       " 44,\n",
       " 166565,\n",
       " 24570,\n",
       " 114136,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 138225,\n",
       " 257704,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 120476,\n",
       " 164430,\n",
       " 274902,\n",
       " 59,\n",
       " 297161,\n",
       " 82603,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 31989,\n",
       " 279281,\n",
       " 67,\n",
       " 125159,\n",
       " 158339,\n",
       " 241930,\n",
       " 84104,\n",
       " 29542,\n",
       " 33420,\n",
       " 116146,\n",
       " 195439,\n",
       " 76899,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 265771,\n",
       " 81,\n",
       " 82,\n",
       " 219886,\n",
       " 26214,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 157342,\n",
       " 290573,\n",
       " 180637,\n",
       " 161258,\n",
       " 92,\n",
       " 166363,\n",
       " 86596,\n",
       " 112799,\n",
       " 96,\n",
       " 77990,\n",
       " 297662,\n",
       " 108625,\n",
       " 291809,\n",
       " 99378,\n",
       " 174615,\n",
       " 101238,\n",
       " 121936,\n",
       " 108803,\n",
       " 42284,\n",
       " 107,\n",
       " 124,\n",
       " 226506,\n",
       " 50760,\n",
       " 280461,\n",
       " 97866,\n",
       " 82374,\n",
       " 154086,\n",
       " 48675,\n",
       " 116,\n",
       " 58257,\n",
       " 74324,\n",
       " 50382,\n",
       " 142831,\n",
       " 219194,\n",
       " 279126,\n",
       " 134541,\n",
       " 129709,\n",
       " 105055,\n",
       " 147085,\n",
       " 38115,\n",
       " 167886,\n",
       " 139876,\n",
       " 83281,\n",
       " 215006,\n",
       " 103189,\n",
       " 131793,\n",
       " 134,\n",
       " 143144,\n",
       " 12701,\n",
       " 184241,\n",
       " 111322,\n",
       " 114694,\n",
       " 230525,\n",
       " 141,\n",
       " 27779,\n",
       " 198550,\n",
       " 144,\n",
       " 146265,\n",
       " 297870,\n",
       " 94358,\n",
       " 148,\n",
       " 270694,\n",
       " 298665,\n",
       " 151,\n",
       " 152,\n",
       " 275614,\n",
       " 18931,\n",
       " 279916,\n",
       " 23905,\n",
       " 146232,\n",
       " 158,\n",
       " 212130,\n",
       " 31422,\n",
       " 161,\n",
       " 52621,\n",
       " 117204,\n",
       " 239706,\n",
       " 165,\n",
       " 263357,\n",
       " 140905,\n",
       " 32355,\n",
       " 151398,\n",
       " 170,\n",
       " 171,\n",
       " 163059,\n",
       " 32366,\n",
       " 238339,\n",
       " 175,\n",
       " 221053,\n",
       " 28794,\n",
       " 190983,\n",
       " 87816,\n",
       " 219152,\n",
       " 181,\n",
       " 182,\n",
       " 218144,\n",
       " 184,\n",
       " 178693,\n",
       " 186,\n",
       " 250602,\n",
       " 164802,\n",
       " 217126,\n",
       " 291400,\n",
       " 165624,\n",
       " 134729,\n",
       " 193,\n",
       " 194,\n",
       " 238446,\n",
       " 163450,\n",
       " 240953,\n",
       " 198,\n",
       " 205824,\n",
       " 12935,\n",
       " 201,\n",
       " 212273,\n",
       " 203,\n",
       " 204,\n",
       " 215941,\n",
       " 228017,\n",
       " 163062,\n",
       " 289133,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 271747,\n",
       " 111762,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 107830,\n",
       " 189069,\n",
       " 230259,\n",
       " 49553,\n",
       " 283586,\n",
       " 272630,\n",
       " 4285,\n",
       " 177002,\n",
       " 225,\n",
       " 226,\n",
       " 251558,\n",
       " 50454,\n",
       " 78377,\n",
       " 56985,\n",
       " 231,\n",
       " 232,\n",
       " 76207,\n",
       " 275808,\n",
       " 235,\n",
       " 61438,\n",
       " 56393,\n",
       " 265477,\n",
       " 200783,\n",
       " 298547,\n",
       " 2322,\n",
       " 253049,\n",
       " 243,\n",
       " 244,\n",
       " 80409,\n",
       " 299562,\n",
       " 278060,\n",
       " 3257,\n",
       " 181919,\n",
       " 250,\n",
       " 115691,\n",
       " 252,\n",
       " 69600,\n",
       " 254,\n",
       " 230056,\n",
       " 247587,\n",
       " 107220,\n",
       " 258,\n",
       " 259,\n",
       " 92672,\n",
       " 11452,\n",
       " 262,\n",
       " 263,\n",
       " 76994,\n",
       " 255036,\n",
       " 173951,\n",
       " 267,\n",
       " 218448,\n",
       " 269,\n",
       " 56982,\n",
       " 271,\n",
       " 272,\n",
       " 212092,\n",
       " 277579,\n",
       " 80777,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 102723,\n",
       " 85609,\n",
       " 90890,\n",
       " 282,\n",
       " 193485,\n",
       " 284,\n",
       " 35879,\n",
       " 286,\n",
       " 149878,\n",
       " 51218,\n",
       " 289,\n",
       " 290,\n",
       " 144625,\n",
       " 198516,\n",
       " 114461,\n",
       " 191568,\n",
       " 295,\n",
       " 207036,\n",
       " 297,\n",
       " 96418,\n",
       " 187880,\n",
       " 300,\n",
       " 144511,\n",
       " 302,\n",
       " 217446,\n",
       " 304,\n",
       " 183064,\n",
       " 111267,\n",
       " 189928,\n",
       " 104889,\n",
       " 309,\n",
       " 188438,\n",
       " 237501,\n",
       " 276214,\n",
       " 166869,\n",
       " 29004,\n",
       " 298011,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 25530,\n",
       " 286584,\n",
       " 321,\n",
       " 206972,\n",
       " 169487,\n",
       " 239077,\n",
       " 299306,\n",
       " 225486,\n",
       " 148505,\n",
       " 141190,\n",
       " 329,\n",
       " 246677,\n",
       " 157763,\n",
       " 332,\n",
       " 2163,\n",
       " 169345,\n",
       " 264208,\n",
       " 125016,\n",
       " 234876,\n",
       " 107045,\n",
       " 229038,\n",
       " 136031,\n",
       " 22977,\n",
       " 9318,\n",
       " 343,\n",
       " 44156,\n",
       " 266450,\n",
       " 251060,\n",
       " 228413,\n",
       " 32306,\n",
       " 160165,\n",
       " 71522,\n",
       " 127940,\n",
       " 70879,\n",
       " 353,\n",
       " 354,\n",
       " 289456,\n",
       " 356,\n",
       " 151476,\n",
       " 358,\n",
       " 16876,\n",
       " 87846,\n",
       " 29528,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 78224,\n",
       " 55919,\n",
       " 149222,\n",
       " 156841,\n",
       " 370,\n",
       " 262128,\n",
       " 278830,\n",
       " 6753,\n",
       " 95426,\n",
       " 27922,\n",
       " 150309,\n",
       " 133738,\n",
       " 378,\n",
       " 76506,\n",
       " 78491,\n",
       " 182676,\n",
       " 175717,\n",
       " 99338,\n",
       " 290314,\n",
       " 385,\n",
       " 134874,\n",
       " 387,\n",
       " 282882,\n",
       " 166610,\n",
       " 62377,\n",
       " 162348,\n",
       " 244976,\n",
       " 393,\n",
       " 211858,\n",
       " 272160,\n",
       " 396,\n",
       " 397,\n",
       " 155300,\n",
       " 157041,\n",
       " 400,\n",
       " 165804,\n",
       " 402,\n",
       " 403,\n",
       " 217330,\n",
       " 102178,\n",
       " 34747,\n",
       " 168107,\n",
       " 157478,\n",
       " 244416,\n",
       " 70887,\n",
       " 190246,\n",
       " 412,\n",
       " 120617,\n",
       " 262030,\n",
       " 9543,\n",
       " 130500,\n",
       " 153703,\n",
       " 7521,\n",
       " 204557,\n",
       " 420,\n",
       " 120574,\n",
       " 150757,\n",
       " 20750,\n",
       " 424,\n",
       " 224494,\n",
       " 88012,\n",
       " 202889,\n",
       " 69118,\n",
       " 290081,\n",
       " 102041,\n",
       " 431,\n",
       " 57717,\n",
       " 36383,\n",
       " 434,\n",
       " 166483,\n",
       " 436,\n",
       " 19415,\n",
       " 111455,\n",
       " 439,\n",
       " 111855,\n",
       " 441,\n",
       " 29420,\n",
       " 64222,\n",
       " 39541,\n",
       " 257379,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 950,\n",
       " 95836,\n",
       " 70659,\n",
       " 285706,\n",
       " 35808,\n",
       " 264001,\n",
       " 34562,\n",
       " 8595,\n",
       " 457,\n",
       " 458,\n",
       " 160772,\n",
       " 281840,\n",
       " 269149,\n",
       " 462,\n",
       " 218959,\n",
       " 23237,\n",
       " 121638,\n",
       " 466,\n",
       " 79590,\n",
       " 132322,\n",
       " 469,\n",
       " 31933,\n",
       " 284382,\n",
       " 472,\n",
       " 473,\n",
       " 118186,\n",
       " 150614,\n",
       " 476,\n",
       " 48245,\n",
       " 24477,\n",
       " 214207,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 199089,\n",
       " 37780,\n",
       " 485,\n",
       " 295762,\n",
       " 487,\n",
       " 488,\n",
       " 97660,\n",
       " 180665,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 197991,\n",
       " 294116,\n",
       " 11688,\n",
       " 221020,\n",
       " 498,\n",
       " 152291,\n",
       " 185988,\n",
       " 162680,\n",
       " 291606,\n",
       " 32191,\n",
       " 59999,\n",
       " 254939,\n",
       " 189696,\n",
       " 40157,\n",
       " 149095,\n",
       " 509,\n",
       " 244981,\n",
       " 511,\n",
       " 281828,\n",
       " 513,\n",
       " 114635,\n",
       " 515,\n",
       " 153253,\n",
       " 517,\n",
       " 142010,\n",
       " 149697,\n",
       " 520,\n",
       " 521,\n",
       " 166814,\n",
       " 523,\n",
       " 13312,\n",
       " 157408,\n",
       " 113671,\n",
       " 251198,\n",
       " 77993,\n",
       " 529,\n",
       " 2364,\n",
       " 531,\n",
       " 298719,\n",
       " 5569,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 32819,\n",
       " 256250,\n",
       " 87260,\n",
       " 540,\n",
       " 233882,\n",
       " 542,\n",
       " 118462,\n",
       " 544,\n",
       " 69247,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 24777,\n",
       " 87805,\n",
       " 210044,\n",
       " 115626,\n",
       " 119,\n",
       " 114563,\n",
       " 104213,\n",
       " 54690,\n",
       " 293455,\n",
       " 558,\n",
       " 559,\n",
       " 4936,\n",
       " 201512,\n",
       " 36180,\n",
       " 208786,\n",
       " 259466,\n",
       " 565,\n",
       " 566,\n",
       " 157364,\n",
       " 121580,\n",
       " 27368,\n",
       " 43548,\n",
       " 136892,\n",
       " 13241,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 191346,\n",
       " 137116,\n",
       " 77822,\n",
       " 579,\n",
       " 13199,\n",
       " 581,\n",
       " 26069,\n",
       " 583,\n",
       " 129678,\n",
       " 256608,\n",
       " 29422,\n",
       " 92942,\n",
       " 588,\n",
       " 138850,\n",
       " 58863,\n",
       " 187156,\n",
       " 4891,\n",
       " 593,\n",
       " 26844,\n",
       " 172785,\n",
       " 82128,\n",
       " 283806,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 126969,\n",
       " 97997,\n",
       " 33602,\n",
       " 125029,\n",
       " 605,\n",
       " 214341,\n",
       " 607,\n",
       " 10771,\n",
       " 188440,\n",
       " 25152,\n",
       " 125525,\n",
       " 187003,\n",
       " 613,\n",
       " 95522,\n",
       " 236215,\n",
       " 214282,\n",
       " 617,\n",
       " 618,\n",
       " 207717,\n",
       " 620,\n",
       " 289830,\n",
       " 26053,\n",
       " 181417,\n",
       " 243999,\n",
       " 625,\n",
       " 60452,\n",
       " 98681,\n",
       " 628,\n",
       " 64810,\n",
       " 42093,\n",
       " 631,\n",
       " 173203,\n",
       " 110833,\n",
       " 634,\n",
       " 83528,\n",
       " 146925,\n",
       " 37544,\n",
       " 236157,\n",
       " 227896,\n",
       " 640,\n",
       " 641,\n",
       " 47437,\n",
       " 163857,\n",
       " 89936,\n",
       " 99435,\n",
       " 268145,\n",
       " 207274,\n",
       " 46824,\n",
       " 649,\n",
       " 121655,\n",
       " 243003,\n",
       " 652,\n",
       " 193802,\n",
       " 220070,\n",
       " 296032,\n",
       " 113562,\n",
       " 156140,\n",
       " 125750,\n",
       " 298754,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 31681,\n",
       " 664,\n",
       " 111344,\n",
       " 40542,\n",
       " 203293,\n",
       " 43606,\n",
       " 44728,\n",
       " 178484,\n",
       " 92403,\n",
       " 155229,\n",
       " 145957,\n",
       " 251460,\n",
       " 675,\n",
       " 676,\n",
       " 137091,\n",
       " 86723,\n",
       " 261717,\n",
       " 180000,\n",
       " 681,\n",
       " 11218,\n",
       " 264270,\n",
       " 229192,\n",
       " 685,\n",
       " 148531,\n",
       " 687,\n",
       " 688,\n",
       " 68042,\n",
       " 199513,\n",
       " 829,\n",
       " 692,\n",
       " 163487,\n",
       " 694,\n",
       " 149203,\n",
       " 31235,\n",
       " 49960,\n",
       " 698,\n",
       " 26075,\n",
       " 229449,\n",
       " 241473,\n",
       " 29146,\n",
       " 273697,\n",
       " 65307,\n",
       " 705,\n",
       " 294621,\n",
       " 19949,\n",
       " 708,\n",
       " 142493,\n",
       " 50838,\n",
       " 8335,\n",
       " 220568,\n",
       " 713,\n",
       " 99256,\n",
       " 715,\n",
       " 147058,\n",
       " 167797,\n",
       " 718,\n",
       " 236323,\n",
       " 123815,\n",
       " 721,\n",
       " 23545,\n",
       " 117290,\n",
       " 111645,\n",
       " 725,\n",
       " 158351,\n",
       " 251493,\n",
       " 76886,\n",
       " 729,\n",
       " 281431,\n",
       " 731,\n",
       " 27921,\n",
       " 733,\n",
       " 734,\n",
       " 11726,\n",
       " 126890,\n",
       " 139892,\n",
       " 16985,\n",
       " 156172,\n",
       " 241693,\n",
       " 293909,\n",
       " 742,\n",
       " 34423,\n",
       " 124009,\n",
       " 205002,\n",
       " 746,\n",
       " 159667,\n",
       " 748,\n",
       " 128472,\n",
       " 64003,\n",
       " 147310,\n",
       " 752,\n",
       " 205342,\n",
       " 754,\n",
       " 177557,\n",
       " 170981,\n",
       " 757,\n",
       " 244919,\n",
       " 58254,\n",
       " 760,\n",
       " 9606,\n",
       " 208740,\n",
       " 39779,\n",
       " 223904,\n",
       " 280550,\n",
       " 766,\n",
       " 299288,\n",
       " 282126,\n",
       " 113998,\n",
       " 276035,\n",
       " 137408,\n",
       " 772,\n",
       " 91819,\n",
       " 13046,\n",
       " 198642,\n",
       " 144811,\n",
       " 777,\n",
       " 3736,\n",
       " 162661,\n",
       " 97910,\n",
       " 90575,\n",
       " 782,\n",
       " 167484,\n",
       " 784,\n",
       " 785,\n",
       " 70406,\n",
       " 787,\n",
       " 99279,\n",
       " 145770,\n",
       " 38087,\n",
       " 3673,\n",
       " 792,\n",
       " 29989,\n",
       " 274583,\n",
       " 289493,\n",
       " 128021,\n",
       " 38109,\n",
       " 39805,\n",
       " 799,\n",
       " 266142,\n",
       " 72136,\n",
       " 802,\n",
       " 803,\n",
       " 133036,\n",
       " 29563,\n",
       " 39057,\n",
       " 70276,\n",
       " 149403,\n",
       " 15616,\n",
       " 86179,\n",
       " 275975,\n",
       " 143468,\n",
       " 70630,\n",
       " 278257,\n",
       " 270784,\n",
       " 236091,\n",
       " 178873,\n",
       " 114553,\n",
       " 99237,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 252413,\n",
       " 36781,\n",
       " 14631,\n",
       " 826,\n",
       " 827,\n",
       " 175155,\n",
       " 124830,\n",
       " 92699,\n",
       " 831,\n",
       " 55950,\n",
       " 833,\n",
       " 254594,\n",
       " 34873,\n",
       " 277783,\n",
       " 94894,\n",
       " 838,\n",
       " 839,\n",
       " 119139,\n",
       " 154663,\n",
       " 154815,\n",
       " 171568,\n",
       " 274351,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 208450,\n",
       " 849,\n",
       " 850,\n",
       " 57028,\n",
       " 256945,\n",
       " 53020,\n",
       " 854,\n",
       " 855,\n",
       " 46038,\n",
       " 156111,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 54486,\n",
       " 862,\n",
       " 863,\n",
       " 137613,\n",
       " 122383,\n",
       " 277621,\n",
       " 191736,\n",
       " 108025,\n",
       " 266892,\n",
       " 109863,\n",
       " 871,\n",
       " 872,\n",
       " 139918,\n",
       " 174294,\n",
       " 210911,\n",
       " 127624,\n",
       " 162707,\n",
       " 175470,\n",
       " 105585,\n",
       " 203589,\n",
       " 15555,\n",
       " 228386,\n",
       " 883,\n",
       " 884,\n",
       " 40876,\n",
       " 886,\n",
       " 887,\n",
       " 237333,\n",
       " 7190,\n",
       " 280937,\n",
       " 891,\n",
       " 77100,\n",
       " 210403,\n",
       " 35979,\n",
       " 44710,\n",
       " 122671,\n",
       " 897,\n",
       " 201563,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 178462,\n",
       " 139021,\n",
       " 905,\n",
       " 53351,\n",
       " 121764,\n",
       " 28686,\n",
       " 109060,\n",
       " 910,\n",
       " 128272,\n",
       " 239561,\n",
       " 13311,\n",
       " 244321,\n",
       " 915,\n",
       " 204540,\n",
       " 917,\n",
       " 173544,\n",
       " 225576,\n",
       " 244892,\n",
       " 19653,\n",
       " 922,\n",
       " 134271,\n",
       " 149193,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 151772,\n",
       " 109827,\n",
       " 121613,\n",
       " 932,\n",
       " 275452,\n",
       " 245740,\n",
       " 281189,\n",
       " 71997,\n",
       " 937,\n",
       " 186945,\n",
       " 281698,\n",
       " 234052,\n",
       " 941,\n",
       " 942,\n",
       " 49024,\n",
       " 19585,\n",
       " 945,\n",
       " 89590,\n",
       " 947,\n",
       " 948,\n",
       " 119761,\n",
       " 99918,\n",
       " 204935,\n",
       " 105840,\n",
       " 276443,\n",
       " 293976,\n",
       " 30111,\n",
       " 956,\n",
       " 274156,\n",
       " 156174,\n",
       " 278574,\n",
       " 215822,\n",
       " 961,\n",
       " 215882,\n",
       " 963,\n",
       " 139952,\n",
       " 163665,\n",
       " 260912,\n",
       " 125805,\n",
       " 968,\n",
       " 142527,\n",
       " 970,\n",
       " 971,\n",
       " 22731,\n",
       " 293316,\n",
       " 153401,\n",
       " 198253,\n",
       " 63314,\n",
       " 150111,\n",
       " 157999,\n",
       " 255129,\n",
       " 980,\n",
       " 141340,\n",
       " 119825,\n",
       " 89067,\n",
       " 164455,\n",
       " 127810,\n",
       " 50441,\n",
       " 151680,\n",
       " 153885,\n",
       " 255265,\n",
       " 187252,\n",
       " 116920,\n",
       " 256331,\n",
       " 218055,\n",
       " 98126,\n",
       " 995,\n",
       " 996,\n",
       " 124800,\n",
       " 998,\n",
       " 132084,\n",
       " ...]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_choice(list(range(size)),int(size*random_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.21855902671814\n",
      "train_accuracy is: 0.9958\n",
      "precision is: \t 0.802986638722\n",
      "recall is: \t 0.998696643858\n",
      "f1 score is: \t 0.890212024397\n",
      "test time is: 0.2569611072540283\n",
      "test accuracy is: 0.99515\n",
      "precision is: \t 0.781395348837\n",
      "recall is: \t 0.991150442478\n",
      "f1 score is: \t 0.873862158648\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "time_rf=time.time()\n",
    "clf =  RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-time_rf)\n",
    "\n",
    "#testing5\n",
    "# test the training error\n",
    "predict_y=np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "print(\"test time is:\", time.time()-t)\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "# test the score for the test data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[9825    0]\n",
      " [   0  175]]\n"
     ]
    }
   ],
   "source": [
    "## confusion matrix plot\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Model build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x_scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#----------------\n",
    "# logistic l1\n",
    "#-----------------\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "        \n",
    "        # set the random state to make sure that each time get the same results\n",
    "\n",
    "time_logistic=time.time()\n",
    "clf = linear_model.LogisticRegression(C=1, penalty='l1', tol=1e-6,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "time_logistic=time.time()-time_logistic    \n",
    "\n",
    "print(time_logistic)\n",
    "\n",
    "# test the training error\n",
    "predict_y_logistic =np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y_logistic==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y_logistic,train_y)\n",
    "recall = recall_score(predict_y_logistic,train_y)\n",
    "f1=f1_score(predict_y_logistic,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n",
    "#----------------\n",
    "# logistic l2\n",
    "#-----------------\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "        \n",
    "        # set the random state to make sure that each time get the same results\n",
    "\n",
    "time_logistic=time.time()\n",
    "clf = linear_model.LogisticRegression(C=1, penalty='l2', tol=1e-6,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "time_logistic=time.time()-time_logistic    \n",
    "\n",
    "print(time_logistic)\n",
    "\n",
    "# test the training error\n",
    "predict_y_logistic =np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y_logistic==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y_logistic,train_y)\n",
    "recall = recall_score(predict_y_logistic,train_y)\n",
    "f1=f1_score(predict_y_logistic,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------------------\n",
    "# SVM_poly_2\n",
    "#---------------------\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "\n",
    "import time \n",
    "from sklearn import svm\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf = svm.SVC(C=1.0,kernel='poly',degree=2,max_iter=5000,shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y =np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "#draw the crosstab chart\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#---------------\n",
    "# decision tree\n",
    "#-----------------\n",
    "\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "    predict_y_test_svm,time_train_svm,time_test_svm,accuracy_train_svm,accuracy_test_svm,precision_svm,recall_svm,f1_svm=\\\n",
    "    LimitOrderBook().svm(train_x=train_x_scale,train_y=train_y,\\\n",
    "                               test_x=test_x_scale,test_y=test_y,max_depth=10,random_state= 987612345,)\n",
    "\n",
    "    print(\"time_train:\",time_train_svm,\"time_test:\",time_test_svm,\"accuracy_train:\",accuracy_train_svm,\"accuracy_test:\",accuracy_test_svm,\\\n",
    "          \"precision:\",precision_svm,\"recall:\",recall_svm,\"f1:\",f1_svm)\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  tree.DecisionTreeClassifier(max_depth=10,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y=np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "print(\"test time is:\", time.time()-t)\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "#draw the crosstab chart\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-59e42b63bcfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mtime_ada\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m987612345\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_scale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime_ada\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                 sample_weight)\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    460\u001b[0m         \"\"\"\n\u001b[0;32m    461\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    348\u001b[0m                                            max_leaf_nodes)\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#-----------------------------------------\n",
    "# Adaboost \n",
    "#-----------------------------------------\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "time_ada=time.time()\n",
    "clf =  AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10),n_estimators=100,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-time_ada)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y=np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "#draw the crosstab chart\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## see the feature_importances\n",
    "\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_x_scale[:100,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multi-class predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.900548696517944\n"
     ]
    }
   ],
   "source": [
    "## load the arbitrage time txt data\n",
    "\n",
    "ask_low_time_list=[]\n",
    "bid_high_time_list=[]\n",
    "no_arbi_time_list=[] \n",
    "time_list=[1,5,10,15,20]\n",
    "import time \n",
    "t=time.time()\n",
    "for ticker_ind in range(5):  \n",
    "    ask_low_time_list.append([])\n",
    "    bid_high_time_list.append([])\n",
    "    no_arbi_time_list.append([])\n",
    "    for time_ind in range(len(time_list)):\n",
    "        ask_low_time_list[ticker_ind].append(\n",
    "            np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_ask_low_time_'+str(time_list[time_ind])+'.txt',header=-1)))\n",
    "        bid_high_time_list[ticker_ind].append(\n",
    "            np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_bid_high_time_'+str(time_list[time_ind])+'.txt',header=-1)))\n",
    "        no_arbi_time_list[ticker_ind].append(\n",
    "            np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_no_arbi_time_'+str(time_list[time_ind])+'.txt',header=-1)))\n",
    "        \n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the total response is:\n",
      "\n",
      "(400236, 1)\n",
      "(269571, 1)\n",
      "(147766, 1)\n",
      "(622641, 1)\n",
      "(667701, 1)\n",
      "The shape of the reduced response is:\n",
      "\n",
      "(309538, 1)\n",
      "(218710, 1)\n",
      "(118877, 1)\n",
      "(458160, 1)\n",
      "(511299, 1)\n"
     ]
    }
   ],
   "source": [
    "# Deal with the data\n",
    "def build_y(ask_low,bid_high,no_arbi,option):\n",
    "    if (option==1):\n",
    "        return ask_low\n",
    "    elif option==2:\n",
    "        return bid_high\n",
    "    elif option==3:\n",
    "        return no_arbi\n",
    "    elif option==4:\n",
    "        return ask_low-bid_high\n",
    "    else:\n",
    "        print(\"option should be 1,2,3,4\")\n",
    "        \n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    response=build_y(ask_low_time_list[ticker_ind][1],bid_high_time_list[ticker_ind][1],\\\n",
    "                                 no_arbi_time_list[ticker_ind][1],option=4)\n",
    "    np.savetxt(path_save+ticker_list[ticker_ind]+'_multiresponse.txt',response)\n",
    "\n",
    "response_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    response_list.append((np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_multiresponse.txt',header=-1))))\n",
    "\n",
    "    ## print the shape of the response\n",
    "## note it is the total response\n",
    "print(\"The shape of the total response is:\\n\")\n",
    "\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_list[ticker_ind].shape)\n",
    "    \n",
    "# need to get the response from 10 to 15:30\n",
    "# the shape of the response and the feature array should be equal \n",
    "response_reduced_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    first_ind = np.where(time_index_list[ticker_ind]>=start_ind)[0][0]\n",
    "    last_ind=np.where(time_index_list[ticker_ind]<=end_ind)[0][-1]\n",
    "    response_reduced_list.append(response_list[ticker_ind][first_ind:last_ind+1])\n",
    "    \n",
    "print(\"The shape of the reduced response is:\\n\")\n",
    "\n",
    "## print the shape of reduced response\n",
    "## response reduced is used for testing and training the model\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_reduced_list[ticker_ind].shape)\n",
    "    # random split data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random split\n",
    "#split the data to train and test data set\n",
    "import random\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "ticker_ind=1\n",
    "size=100000\n",
    "\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "\n",
    "print(\"total shape:\",total_array.shape)\n",
    "\n",
    "train_x, test_x, train_y, test_y =train_test_split(\\\n",
    "total_array[:,:134],total_array[:,134], test_size=0.1, random_state=42)\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "\n",
    "print(\"test shape:\",test_y.shape)\n",
    "print(\"train shape:\",train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total array shape: (100000, 136)\n",
      "train_x shape: (90000, 134)\n",
      "test_x shape: (10000, 134)\n",
      "test_y shape: (10000,)\n",
      "train_y shape: (90000,)\n",
      "[ -1.57e-14   4.17e-15  -1.33e-13   5.47e-15   1.94e-14  -2.43e-16\n",
      "   5.16e-15   4.88e-15  -1.41e-15   3.87e-15   7.65e-14   3.08e-15\n",
      "  -3.86e-14  -3.91e-15   2.83e-14  -3.77e-15   3.57e-14  -1.85e-14\n",
      "  -2.84e-14  -2.15e-15  -1.55e-14   1.65e-14  -4.57e-14   6.52e-15\n",
      "  -2.53e-15   7.03e-15  -2.83e-14  -9.47e-16  -4.45e-14   9.21e-16\n",
      "   6.20e-14  -7.76e-15   4.34e-14   1.46e-15   3.80e-14  -5.02e-15\n",
      "  -2.04e-14  -2.09e-14  -5.27e-16   2.53e-15  -8.67e-15   8.25e-15\n",
      "   1.61e-14  -1.44e-13  -6.35e-14  -4.81e-16  -3.30e-14  -2.21e-13\n",
      "   1.98e-14  -2.08e-14   1.19e-14   2.38e-14   2.76e-14  -2.56e-14\n",
      "  -1.72e-14   2.91e-14   3.08e-14   7.40e-15   4.02e-14   4.51e-14\n",
      "   1.25e-14  -7.39e-16  -7.54e-15  -7.86e-14   7.24e-14   2.41e-14\n",
      "   6.63e-15   2.33e-14   3.33e-15   7.63e-15  -1.26e-14   2.11e-14\n",
      "   1.28e-14  -3.02e-15  -5.79e-14  -1.72e-14   5.22e-14   1.50e-15\n",
      "   3.02e-14  -1.93e-14   3.92e-15   7.73e-14   2.52e-15   8.40e-16\n",
      "  -6.28e-15  -4.18e-15   2.01e-15   1.17e-15   3.63e-15   1.20e-14\n",
      "  -7.13e-15  -3.73e-15  -3.06e-15  -3.22e-15   6.27e-15  -9.03e-15\n",
      "   9.92e-15  -3.80e-15   8.57e-15   5.41e-15  -3.93e-15  -2.19e-15\n",
      "   9.57e-16  -9.40e-15  -1.15e-15  -2.54e-15  -9.32e-16  -6.73e-15\n",
      "  -1.73e-15  -7.43e-16  -1.37e-15  -3.61e-15   8.55e-16  -1.51e-15\n",
      "  -1.60e-15   4.64e-15   4.15e-15   6.88e-16  -2.83e-15  -4.93e-15\n",
      "  -9.06e-15   3.59e-15   2.85e-15  -5.39e-15  -4.45e-15  -4.82e-14\n",
      "  -4.77e-15   1.55e-14  -6.08e-15  -1.40e-14  -2.23e-16  -7.32e-16\n",
      "  -3.22e-15   3.36e-15]\n",
      "[-0.29 -0.05 -0.28  0.12 -0.29 -0.07 -0.28  0.16 -0.29 -0.08 -0.28  0.14\n",
      " -0.28 -0.09 -0.27  0.1  -0.29 -0.12 -0.27  0.06 -0.29 -0.11 -0.27  0.03\n",
      " -0.29 -0.09 -0.27  0.05 -0.29 -0.11 -0.26  0.08 -0.29 -0.12 -0.26  0.07\n",
      " -0.29 -0.13 -0.26  0.11  0.02 -0.   -0.03 -0.04 -0.07 -0.07 -0.09 -0.1\n",
      " -0.12 -0.13 -0.29 -0.28 -0.28 -0.28 -0.28 -0.28 -0.28 -0.28 -0.28 -0.28\n",
      "  0.07  0.04  0.04 -0.03 -0.   -0.01 -0.01 -0.05 -0.05 -0.1  -0.09 -0.08\n",
      " -0.08 -0.04 -0.07 -0.12 -0.11 -0.12 -0.29 -0.27 -0.27  0.27 -0.09 -0.35\n",
      " -0.01 -0.01 -0.   -0.01 -0.02 -0.01 -0.01 -0.   -0.01  0.   -0.01 -0.01\n",
      " -0.02 -0.03 -0.03 -0.02 -0.02 -0.03 -0.02 -0.02 -0.01 -0.    0.01  0.01\n",
      "  0.04 -0.01  0.01 -0.02 -0.01  0.02  0.   -0.01  0.01 -0.03  0.02  0.01\n",
      "  0.   -0.   -0.   -0.03 -0.11 -0.17 -0.12 -0.08 -0.11 -0.16 -0.01 -0.    0.\n",
      "  0.  ]\n"
     ]
    }
   ],
   "source": [
    "#time series split\n",
    "#%%--------------------------------------------------------------------------------------------\n",
    "\n",
    "ticker_ind=0\n",
    "size=100000\n",
    "random_ratio=0.5\n",
    "\n",
    "time_index=time_index_list[ticker_ind]\n",
    "# combine the feature and response array to random sample\n",
    "time_index_reduced=time_index[(time_index>=start_ind)&(time_index<=end_ind)]\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                            time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)[:size,:]\n",
    "\n",
    "total_array=total_array[random_choice(list(range(size)),int(size*random_ratio)),:]\n",
    "\n",
    "train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x=total_array[:train_num_index,:134]\n",
    "test_x=total_array[train_num_index:,:134]\n",
    "train_y=total_array[:train_num_index,134]\n",
    "test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "print(\"train_x shape:\",train_x.shape)\n",
    "print(\"test_x shape:\",test_x.shape)\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)\n",
    "\n",
    "# scale the data\n",
    "# can use the processing.scale function to scale the data\n",
    "from sklearn import preprocessing\n",
    "# note that we need to transfer the data type to float\n",
    "# remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "# use scale for zero mean and one std\n",
    "scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "train_x_scale=scaler.transform(train_x)\n",
    "test_x_scale=scaler.transform(test_x)\n",
    "\n",
    "print(np.mean(train_x_scale,0))\n",
    "print(np.mean(test_x_scale,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one vs one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.6269679069519\n",
      "train accuracy is: 0.982922222222\n",
      "test time is : 0.6610398292541504\n",
      "test accuracy is: 0.9788\n",
      "Confusion matrix, without normalization\n",
      "[[ 172   54    0]\n",
      " [   1 9416    4]\n",
      " [   0  153  200]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAEpCAYAAADWEjokAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa732d8f9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# only run for random forest method\n",
    "# one vs one case\n",
    "# random forest\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "## sample weights\n",
    "#sample_weights=[]\n",
    "#ratio=len(train_y)/sum(train_y==1)/10\n",
    "#for i in range(len(train_x)):\n",
    "#    if train_y[i]==0:\n",
    "#        sample_weights.append(1)\n",
    "#    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  OneVsOneClassifier(RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345))\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "%matplotlib inline\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_one.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-a3ef991f284c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m987612345\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_scale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                 sample_weight)\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    460\u001b[0m         \"\"\"\n\u001b[0;32m    461\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    348\u001b[0m                                            max_leaf_nodes)\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# only run for random forest method\n",
    "# one vs one case\n",
    "# adaboosting\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "## sample weights\n",
    "#sample_weights=[]\n",
    "#ratio=len(train_y)/sum(train_y==1)/10\n",
    "#for i in range(len(train_x)):\n",
    "#    if train_y[i]==0:\n",
    "#        sample_weights.append(1)\n",
    "#    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10),n_estimators=100,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_one.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------\n",
    "# one vs one case\n",
    "# svm\n",
    "#-------------------\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "\n",
    "\n",
    "## sample weights\n",
    "#sample_weights=[]\n",
    "#ratio=len(train_y)/sum(train_y==1)/10\n",
    "#for i in range(len(train_x)):\n",
    "#    if train_y[i]==0:\n",
    "#        sample_weights.append(1)\n",
    "#    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  OneVsOneClassifier(svm.SVC(C=1.0,kernel='poly',degree=2,max_iter=5000,shrinking=True, tol=0.001, verbose=False)\n",
    ")\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_one.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113.79565858840942\n",
      "train accuracy is: 0.980655555556\n",
      "test time is : 0.3234865665435791\n",
      "test accuracy is: 0.9785\n",
      "Confusion matrix, without normalization\n",
      "[[ 166   60    0]\n",
      " [   1 9416    4]\n",
      " [   0  150  203]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAEpCAYAAADWEjokAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6fcc153c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# only run for random forest method\n",
    "# one vs rest case\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  OneVsRestClassifier(RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345))\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_rest.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.P&L calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_index(index, value):\n",
    "    i=0\n",
    "    while index[i] <value:\n",
    "        i=i+1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## for AMZN\n",
    "ticker_ind =0\n",
    "train_ratio=0.9\n",
    "\n",
    "data_order=data_order_list[ticker_ind]\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "time_index=data_mess[:,0]\n",
    "data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "total_array_old=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 40)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "data_order_test=data_order_reduced[int(size*train_ratio):size,:]\n",
    "time_index_test=time_index_reduced[int(size*train_ratio):size]\n",
    "\n",
    "test_y_unrandom=total_array_old[int(size*train_ratio):size,134]\n",
    "print(data_order_test.shape)\n",
    "print(time_index_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiEAAAEZCAYAAABB1vXaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6fca1f0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(time_index_test[:10000],data_order_test[:10000,0],\"r-\",label=\"Ask price\")\n",
    "plt.plot(time_index_test[:10000],data_order_test[:10000,2],\"b-\",label=\"Bid price\")\n",
    "\n",
    "x_ask_low_choose=time_index_test[test_y_unrandom==1]\n",
    "y_ask_low_choose=data_order_test[test_y_unrandom==1,0]\n",
    "x_bid_high_choose=time_index_test[test_y_unrandom==-1]\n",
    "y_bid_high_choose=data_order_test[test_y_unrandom==-1,2]\n",
    "\n",
    "plt.plot(x_ask_low_choose[:30],y_ask_low_choose[:30],\"gv\",markersize=8,label=\"Ask low\")\n",
    "plt.plot(x_bid_high_choose[:30],y_bid_high_choose[:30],\"r^\",markersize=8,label=\"Bid high\")\n",
    "plt.xlabel(\"Time(s)\")\n",
    "plt.ylabel(\"Price($10^{-4}$\\$)\")\n",
    "plt.legend(bbox_to_anchor=[1.4, 1])\n",
    "plt.title(\"Arbitrage opportunities for \"+ticker_list[ticker_ind]+\"(5s)\")\n",
    "plt.savefig(\"arbitrage_plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_index_test=total_array[:,135][int(size*train_ratio):size]\n",
    "# find the arbitrage occuring index\n",
    "arbi_index=list(np.where(predict_y_test!=0)[0])\n",
    "# find the index that 5 seconds later\n",
    "arbi_future_index=[]\n",
    "for i in arbi_index:\n",
    "    arbi_future_index.append(get_index(time_index_reduced,time_index_test[i]+5))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_price=data_order_reduced[arbi_future_index,0]\n",
    "current_price=total_array_test[arbi_index,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2239300.0\n",
      "2239300.0\n"
     ]
    }
   ],
   "source": [
    "print(current_price[234])\n",
    "print(future_price[234])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_array_test=total_array[int(size*train_ratio):size,:]\n",
    "future_price=[]\n",
    "current_price=[]\n",
    "pnl=[]\n",
    "for i in range(len(arbi_index)):\n",
    "    #ask low\n",
    "    if predict_y_test[arbi_index[i]]==1 :\n",
    "        future_price=data_order_reduced[arbi_future_index[i],0]\n",
    "        current_price=total_array_test[arbi_index[i],2]\n",
    "        pnl.append(current_price-future_price)\n",
    "    # bid high\n",
    "    else: \n",
    "        future_price=data_order_reduced[arbi_future_index[i],2]\n",
    "        current_price=total_array_test[arbi_index[i],0]\n",
    "        pnl.append(future_price-current_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,False]\n",
    "a.index(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([234]),)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.where(predict_y_test!=0)[0]\n",
    "np.where(a==7976)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4384, 7976, 9836]),)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(((test_y==0) & (predict_y_test==1))==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAEZCAYAAADR8/HkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYli2JdYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6fd061eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pnl=np.array(pnl)\n",
    "predict_arbi=predict_y_test[predict_y_test!=0]\n",
    "plt.plot(pnl[predict_arbi==1],\"b.\",label=\"Ask low PnL\")\n",
    "plt.plot(pnl[predict_arbi==-1],\"r.\",label=\"Bid High PnL\")\n",
    "\n",
    "plt.xlabel(\"Arbitrage Index\")\n",
    "plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "plt.title(\"PnL for \"+ticker_list[ticker_ind])\n",
    "plt.legend()\n",
    "plt.savefig(ticker_list[ticker_ind]+\"_pnl.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEZCAYAAADYGFGeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+H9Da/fx9l+L2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6fd154f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cum_pnl=np.cumsum(pnl)\n",
    "plt.plot(cum_pnl,\"b.\",label=\"Cumulative P&L\")\n",
    "plt.xlabel(\"Arbitrage Index\")\n",
    "plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "plt.title(\"Cumulative PnL for \"+ticker_list[ticker_ind])\n",
    "plt.legend()\n",
    "plt.savefig(ticker_list[ticker_ind]+\"_cum_pnl.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop for all stock to plot the pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#time series split\n",
    "#%%--------------------------------------------------------------------------------------------\n",
    "\n",
    "size =100000\n",
    "for ticker_ind in range(2,5):\n",
    "    # combine the feature and response array to random sample\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    time_index=data_mess[:,0]\n",
    "    data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    total_array_old=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                    time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)\n",
    "\n",
    "    total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)[:size,:]\n",
    "    total_array=total_array[np.random.randint(len(total_array),size=len(total_array)),:]\n",
    "\n",
    "    train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "    print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "    #split the data to train and test data set\n",
    "    train_x=total_array[:train_num_index,:134]\n",
    "    test_x=total_array[train_num_index:,:134]\n",
    "    train_y=total_array[:train_num_index,134]\n",
    "    test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "    # the y data need to reshape to size (n,) not (n,1)\n",
    "    test_y=test_y.reshape(len(test_y),)\n",
    "    train_y=train_y.reshape(len(train_y),)\n",
    "    print(\"train_x shape:\",train_x.shape)\n",
    "    print(\"test_x shape:\",test_x.shape)\n",
    "    print(\"test_y shape:\",test_y.shape)\n",
    "    print(\"train_y shape:\",train_y.shape)\n",
    "\n",
    "\n",
    "    # scale the data\n",
    "    # can use the processing.scale function to scale the data\n",
    "    from sklearn import preprocessing\n",
    "    # note that we need to transfer the data type to float\n",
    "    # remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "    # use scale for zero mean and one std\n",
    "    scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "    train_x_scale=scaler.transform(train_x)\n",
    "    test_x_scale=scaler.transform(test_x)\n",
    "\n",
    "    print(np.mean(train_x_scale,0))\n",
    "    print(np.mean(test_x_scale,0))\n",
    "\n",
    "    from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "    # change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "    t=time.time()\n",
    "    clf =  OneVsRestClassifier(RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345))\n",
    "    clf.fit(train_x_scale,train_y)\n",
    "\n",
    "    print(time.time()-t)\n",
    "\n",
    "    predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "    print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "    # define a function to prefict the result by threshold\n",
    "    # note: logistic model will return two probability\n",
    "    def predict_threshold(predict_proba, threshold):\n",
    "        res=[]\n",
    "        for i in range(len(predict_proba)):\n",
    "            res.append(int(predict_proba[i][1]>threshold))\n",
    "        return res\n",
    "\n",
    "    t=time.time()\n",
    "    predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "    print(\"test time is :\",time.time()-t)\n",
    "    print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "    # # test the score for the train data\n",
    "    # from sklearn.metrics import (precision_score, recall_score,\n",
    "    #                              f1_score)\n",
    "    # print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "    # precision= precision_score(predict_y_test,test_y)\n",
    "    # recall = recall_score(predict_y_test,test_y)\n",
    "    # f1=f1_score(predict_y_test,test_y)\n",
    "    # print(\"precision is: \\t %s\" % precision)\n",
    "    # print(\"recall is: \\t %s\" % recall)\n",
    "    # print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "    # #draw the crosstab chart\n",
    "    # %matplotlib inline\n",
    "    # ## draw chart for the cross table\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(3)\n",
    "        plt.xticks(tick_marks, [-1,0,1])\n",
    "        plt.yticks(tick_marks, [-1,0,1])\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(test_y, predict_y_test)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm)\n",
    "    plt.savefig(\"one_vs_rest.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    def get_index(index, value):\n",
    "        i=0\n",
    "        while index[i] <value:\n",
    "            i=i+1\n",
    "        return i\n",
    "\n",
    "\n",
    "    train_ratio=0.9\n",
    "    time_index=data_mess[:,0]\n",
    "    data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    total_array_old=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                    time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "\n",
    "    time_index_test=total_array[:,135][int(size*train_ratio):size]\n",
    "    # find the arbitrage occuring index\n",
    "    arbi_index=list(np.where(predict_y_test!=0)[0])\n",
    "    # find the index that 5 seconds later\n",
    "    arbi_future_index=[]\n",
    "    for i in arbi_index:\n",
    "        arbi_future_index.append(get_index(time_index_reduced,time_index_test[i]+5))\n",
    "\n",
    "    total_array_test=total_array[int(size*train_ratio):size,:]\n",
    "    future_price=[]\n",
    "    current_price=[]\n",
    "    pnl=[]\n",
    "    for i in range(len(arbi_index)):\n",
    "        #ask low\n",
    "        if predict_y_test[arbi_index[i]]==1 :\n",
    "            future_price=data_order_reduced[arbi_future_index[i],0]\n",
    "            current_price=total_array_test[arbi_index[i],2]\n",
    "            pnl.append(current_price-future_price)\n",
    "        # bid high\n",
    "        else: \n",
    "            future_price=data_order_reduced[arbi_future_index[i],2]\n",
    "            current_price=total_array_test[arbi_index[i],0]\n",
    "            pnl.append(future_price-current_price)\n",
    "\n",
    "    pnl=np.array(pnl)\n",
    "    predict_arbi=predict_y_test[predict_y_test!=0]\n",
    "    plt.plot(pnl[predict_arbi==1],\"b.\",label=\"Ask low PnL\")\n",
    "    plt.plot(pnl[predict_arbi==-1],\"r.\",label=\"Bid High PnL\")\n",
    "\n",
    "    plt.xlabel(\"Arbitrage Index\")\n",
    "    plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "    plt.title(\"PnL for \"+ticker_list[ticker_ind])\n",
    "    plt.legend()\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_pnl.png\")\n",
    "    plt.show()\n",
    "\n",
    "    cum_pnl=np.cumsum(pnl)\n",
    "    plt.plot(cum_pnl,\"b.\",label=\"Cumulative P&L\")\n",
    "    plt.xlabel(\"Arbitrage Index\")\n",
    "    plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "    plt.title(\"Cumulative PnL for \"+ticker_list[ticker_ind])\n",
    "    plt.legend()\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_cum_pnl.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the order book type\n",
    "\n",
    "use the data_mess data set to plot the chart of the order book type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Plot the order book types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "order_type_list=[]\n",
    "t=time.time()\n",
    "for ticker_ind in range(5):\n",
    "    order_type=[]\n",
    "    for i in [1,2,3,4,5]:\n",
    "        order_type.append(sum(data_mess_list[ticker_ind][:,1]==i))\n",
    "    order_type_list.append(order_type)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(order_type_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# n_groups = 5\n",
    "\n",
    "# means_men = (20, 35, 30, 35, 27)\n",
    "# std_men = (2, 3, 4, 1, 2)\n",
    "\n",
    "# means_women = (25, 32, 34, 20, 25)\n",
    "# std_women = (3, 5, 2, 3, 3)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# index = np.arange(n_groups)\n",
    "# bar_width = 0.35\n",
    "\n",
    "# opacity = 0.4\n",
    "# error_config = {'ecolor': '0.3'}\n",
    "\n",
    "# rects1 = plt.bar(index, means_men, bar_width,\n",
    "#                  alpha=opacity,\n",
    "#                  color='b',\n",
    "#                  yerr=std_men,\n",
    "#                  error_kw=error_config,\n",
    "#                  label='Men')\n",
    "\n",
    "# rects2 = plt.bar(index + bar_width, means_women, bar_width,\n",
    "#                  alpha=opacity,\n",
    "#                  color='r',\n",
    "#                  yerr=std_women,\n",
    "#                  error_kw=error_config,\n",
    "#                  label='Women')\n",
    "\n",
    "# plt.xlabel('Group')\n",
    "# plt.ylabel('Scores')\n",
    "# plt.title('Scores by group and gender')\n",
    "# plt.xticks(index + bar_width, ('A', 'B', 'C', 'D', 'E'))\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "order_type_array=np.array(order_type_list)\n",
    "\n",
    "\n",
    "n_groups=7.5\n",
    "index = np.arange(n_groups,step=1.5)    # the x locations for the groups\n",
    "ticker_list=['AAPL', 'AMZN', 'GOOG', 'INTC','MSFT']\n",
    "color_list=['red','yellow','green','blue','darkmagenta']\n",
    "type_list=['1:Order_book','2:Cancel_part','3:Delete_all','4:Execution_visible','5:Execution_hidden']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "bar_width = 0.25\n",
    "\n",
    "opacity = 0.6\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, order_type_array[:,0], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[0],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[0])\n",
    "\n",
    "rects2 = plt.bar(index + 1*bar_width, order_type_array[:,1], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[1],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[1])\n",
    "\n",
    "\n",
    "rects3 = plt.bar(index + 2*bar_width, order_type_array[:,2], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[2],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[2])\n",
    "\n",
    "rects4 = plt.bar(index + 3*bar_width, order_type_array[:,3], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[3],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[3])\n",
    "\n",
    "rects5 = plt.bar(index + 4*bar_width, order_type_array[:,4], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[4],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[4])\n",
    "\n",
    "\n",
    "plt.xlabel('Stock Ticker')\n",
    "plt.ylabel('Numbers')\n",
    "plt.title('Order Book Types')\n",
    "plt.xticks(index + bar_width*2.5, ticker_list)\n",
    "plt.yticks(np.arange(0, 700000,50000))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Plot the arbitrage situation (bid high, ask low and no arbitrage)\n",
    "\n",
    "Take the first stock which is AAPL as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_order_reduced=data_order_list[0][(time_index_list[0]>= start_ind) & (time_index_list[0]<= end_ind)]\n",
    "data_mess_reduced=data_mess_list[0][(time_index_list[0]>= start_ind) & (time_index_list[0]<= end_ind)]\n",
    "time_index_reduced=time_index_list[0][(time_index_list[0]>= start_ind) & (time_index_list[0]<= end_ind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_ind=np.where(ask_low_time_list[0][1]==1)[0][0]\n",
    "last_ind=np.where(time_index_reduced>time_index_reduced[first_ind]+5)[0][0]\n",
    "print(\"first_ind:\",first_ind)\n",
    "print(\"last_ind:\",last_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "time_index=time_index_reduced[first_ind:last_ind+1]\n",
    "ask_price=data_order_reduced[first_ind:last_ind+1,0]\n",
    "bid_price=data_order_reduced[first_ind:last_ind+1,2]\n",
    "print(ask_pirce[1])\n",
    "print(bid_price[1])\n",
    "plt.plot(time_index,ask_price,'r.-',label=\"Ask Price\")\n",
    "plt.plot(time_index,bid_price,'b.-',label=\"Bid Price\")\n",
    "\n",
    "plt.xticks=time_index\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Ask Low Arbitrage Example\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.where(bid_high_time_list[0][1]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the bid high case\n",
    "\n",
    "first_ind=np.where(bid_high_time_list[0][1]==1)[0][20]\n",
    "last_ind=np.where(time_index_list[0]>time_index_list[0][first_ind]+5)[0][0]\n",
    "print(\"first_ind:\",first_ind)\n",
    "print(\"last_ind:\",last_ind)\n",
    "\n",
    "%matplotlib qt\n",
    "time_index=time_index_list[0][first_ind:last_ind+1]\n",
    "ask_price=data_order_list[0][first_ind:last_ind+1,0]\n",
    "bid_price=data_order_list[0][first_ind:last_ind+1,2]\n",
    "print(ask_pirce[1])\n",
    "print(bid_price[1])\n",
    "plt.plot(time_index,ask_price,'r.-',label=\"Ask Price\")\n",
    "plt.plot(time_index,bid_price,'b.-',label=\"Bid Price\")\n",
    "\n",
    "plt.xticks=time_index\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Bid High Arbitrage Example\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the no arbitrage case\n",
    "first_ind=np.where(no_arbi_time_list[0][1]==1)[0][20]\n",
    "last_ind=np.where(time_index_list[0]>time_index_list[0][first_ind]+5)[0][0]\n",
    "print(\"first_ind:\",first_ind)\n",
    "print(\"last_ind:\",last_ind)\n",
    "\n",
    "%matplotlib qt\n",
    "time_index=time_index_list[0][first_ind:last_ind+1]\n",
    "ask_price=data_order_list[0][first_ind:last_ind+1,0]\n",
    "bid_price=data_order_list[0][first_ind:last_ind+1,2]\n",
    "print(ask_pirce[1])\n",
    "print(bid_price[1])\n",
    "plt.plot(time_index,ask_price,'r.-',label=\"Ask Price\")\n",
    "plt.plot(time_index,bid_price,'b.-',label=\"Bid Price\")\n",
    "\n",
    "plt.xticks=time_index\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"No Arbitrage Example\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.plot the statistical properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) cumulative distribution function for arrival time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticker_ind=2\n",
    "data=data_mess_list[ticker_ind]\n",
    "# we use the market order\n",
    "data_order=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "\n",
    "arrival_time=data_order[1:,0]-data_order[0:-1,0]\n",
    "#delete the zero intra arrival time\n",
    "arrival_time=arrival_time[arrival_time>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu_log=np.mean(np.log(arrival_time))\n",
    "std_log=np.std(np.log(arrival_time))\n",
    "data_log=np.random.lognormal(mu_log,std_log,arrival_time.shape)\n",
    "\n",
    "mu_exp=np.mean(arrival_time)\n",
    "data_exp=np.random.exponential(mu_exp,arrival_time.shape)\n",
    "\n",
    "data_weibull=np.random.weibull(0.38,arrival_time.shape)\n",
    "beta=np.var(arrival_time)/np.mean(arrival_time)\n",
    "alpha=np.mean(arrival_time)/beta\n",
    "data_gamma=np.random.gamma(alpha,beta,arrival_time.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.stats import lognorm\n",
    "ecdf = sm.distributions.ECDF(arrival_time,)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"b\",label=\"Original data\")\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_log)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"g\",label=\"Lognormal Distribution\")\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_exp)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"y\",label=\"Exponential distribution\")\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_weibull)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"r\",label=\"Weibull distribution\")\n",
    "\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_t)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"purple\",label=\"Gamma distribution\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Intra-arrival time\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Cumulative distribution function of order arrival time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) loop for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2, 2,figsize=(13, 13))\n",
    "for tickerb_ind in range(1,5):\n",
    "    data=data_mess_list[ticker_ind]\n",
    "    # we use the market order\n",
    "    data_order=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "\n",
    "    arrival_time=data_order[1:,0]-data_order[0:-1,0]\n",
    "    #delete the zero intra arrival time\n",
    "    arrival_time=arrival_time[arrival_time>0]\n",
    "    mu_log=np.mean(np.log(arrival_time))\n",
    "    std_log=np.std(np.log(arrival_time))\n",
    "    data_log=np.random.lognormal(mu_log,std_log,arrival_time.shape)\n",
    "\n",
    "    mu_exp=np.mean(arrival_time)\n",
    "    data_exp=np.random.exponential(mu_exp,arrival_time.shape)\n",
    "\n",
    "    data_weibull=np.random.weibull(0.38,arrival_time.shape)\n",
    "    beta=np.var(arrival_time)/np.mean(arrival_time)\n",
    "    alpha=np.mean(arrival_time)/beta\n",
    "    data_gamma=np.random.gamma(alpha,beta,arrival_time.shape)\n",
    "    ecdf = sm.distributions.ECDF(arrival_time,)\n",
    "   \n",
    "  \n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"b\",label=\"Original data\")\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_log)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"g\",label=\"Lognormal Distribution\")\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_exp)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"y\",label=\"Exponential distribution\")\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_weibull)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"r\",label=\"Weibull distribution\")\n",
    "\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_t)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"purple\",label=\"Gamma distribution\")\n",
    "    \n",
    "    \n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlabel(\"Intra-arrival time\")\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_ylabel(\"Probability\")\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].legend(loc=\"lower right\")\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_title(\"Cumulative distribution function of \\n order arrival time  for stock \"+ticker_list[ticker_ind])\n",
    "\n",
    "plt.savefig('arrival_time.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) volume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.stats import lognorm  \n",
    "import seaborn as sns\n",
    "ticker_ind=0\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_AAPL.png\")\n",
    "plt.show()\n",
    "\n",
    "ticker_ind=1\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_AMZN.png\")\n",
    "plt.show()\n",
    "\n",
    "ticker_ind=3\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale)+1.2, shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_INTC.png\")\n",
    "plt.show()\n",
    "\n",
    "ticker_ind=4\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale)+1.2, shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_MSFT.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3) Intraday seasonality\n",
    "\n",
    "observe the volume during the whole day under 5 minutes time bins. show the result of seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticker_ind=0\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "data_mess_limit=data_mess[data_mess[:,1]==1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calute the volume of limit order book in each time interval\n",
    "\n",
    "time_interval=np.linspace(data_mess_limit[:,0].min(),data_mess_limit[:,0].max(),78)\n",
    "vol=0\n",
    "vol_time=[]\n",
    "j=1\n",
    "\n",
    "for i in range(len(data_mess_limit)):\n",
    "    if  data_mess_limit[i,0]<=time_interval[j]:\n",
    "        vol=vol+data_mess_limit[i,3]\n",
    "    else: \n",
    "        j=j+1\n",
    "        vol_time.append(vol)\n",
    "        vol=data_mess_limit[i,3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the quadratic fit and vol_time\n",
    "x=range(76)\n",
    "plt.plot(x,vol_time,label=ticker_list[ticker_ind])\n",
    "qua_fit=np.poly1d(np.polyfit(x, vol_time, 2))(x)\n",
    "plt.plot(x,qua_fit,label=ticker_list[ticker_ind]+\" quadratic fit\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "xticks=np.arange(34200,57600,2400)\n",
    "plt.xticks(x[::8],xticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for limit order \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "for ticker_ind in range(1,5):\n",
    "\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_mess_limit=data_mess[data_mess[:,1]==1,:]\n",
    "    # calute the volume of limit order book in each time interval\n",
    "\n",
    "    time_interval=np.linspace(data_mess_limit[:,0].min(),data_mess_limit[:,0].max(),78)\n",
    "    vol=0\n",
    "    vol_time=[]\n",
    "    j=1\n",
    "\n",
    "    for i in range(len(data_mess_limit)):\n",
    "        if  data_mess_limit[i,0]<=time_interval[j]:\n",
    "            vol=vol+data_mess_limit[i,3]\n",
    "        else: \n",
    "            j=j+1\n",
    "            vol_time.append(vol)\n",
    "            vol=data_mess_limit[i,3]\n",
    "    # plot the quadratic fit and vol_time\n",
    "    x=range(76)\n",
    "    plt.plot(x,vol_time,\"+-\",label=ticker_list[ticker_ind])\n",
    "    qua_fit=np.poly1d(np.polyfit(x, vol_time, 2))(x)\n",
    "    plt.plot(x,qua_fit,\"--\",label=ticker_list[ticker_ind]+\" quadratic fit\")\n",
    "    plt.legend(loc=\"upper center\")\n",
    "    xticks=np.arange(34200,57600,2400)\n",
    "    plt.xticks(x[::8],xticks)\n",
    "    plt.title(\"Number of limit orders in a 5-minute interval for \"+ticker_list[ticker_ind])\n",
    "    plt.xlabel(\"Time of day(seconds)\")\n",
    "    plt.ylabel(\"Number of limit orders submitted in $\\Delta_t=5$ minutes\")\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_limit_vol_time.png\",bbox_inches='tight')\n",
    "\n",
    "    plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# market order\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "for ticker_ind in range(1,5):\n",
    "\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_mess_market=data_mess[(data_mess[:,1]==4) | (data_mess[:,1]==5),:]\n",
    "    # calute the volume of limit order book in each time interval\n",
    "\n",
    "    time_interval=np.linspace(data_mess_market[:,0].min(),data_mess_market[:,0].max(),78)\n",
    "    vol=0\n",
    "    vol_time=[]\n",
    "    j=1\n",
    "\n",
    "    for i in range(len(data_mess_market)):\n",
    "        if  data_mess_market[i,0]<=time_interval[j]:\n",
    "            vol=vol+data_mess_market[i,3]\n",
    "        else: \n",
    "            j=j+1\n",
    "            vol_time.append(vol)\n",
    "            vol=data_mess_market[i,3]\n",
    "    # plot the quadratic fit and vol_time\n",
    "    x=range(76)\n",
    "    plt.plot(x,vol_time,\"+-\",label=ticker_list[ticker_ind])\n",
    "    qua_fit=np.poly1d(np.polyfit(x, vol_time, 2))(x)\n",
    "    plt.plot(x,qua_fit,\"--\",label=ticker_list[ticker_ind]+\" quadratic fit\")\n",
    "    plt.legend(loc=\"upper center\")\n",
    "    xticks=np.arange(34200,57600,2400)\n",
    "    plt.xticks(x[::8],xticks)\n",
    "    plt.title(\"Number of market orders in a 5-minute interval for \"+ticker_list[ticker_ind])\n",
    "    plt.xlabel(\"Time of day(seconds)\")\n",
    "    plt.ylabel(\"Number of market orders submitted in $\\Delta_t=5$ minutes\")\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_market_vol_time.png\",bbox_inches='tight')\n",
    "    plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) average shape of the order books\n",
    "find the total volume for all each price level and see the volume trend based on the price levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 4) average shape of the order books\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "ticker_ind=1\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "data_order=data_order_list[ticker_ind]\n",
    "data_order_limit_ask_vol=data_order[data_mess[:,1]==1,1:40:4]\n",
    "data_order_limit_bid_vol=data_order[data_mess[:,1]==1,3:40:4]\n",
    "\n",
    "vol_ask=np.sum(data_order_limit_ask_vol,axis=0)/np.mean(np.sum(data_order_limit_ask_vol,axis=0))\n",
    "vol_bid=np.sum(data_order_limit_bid_vol,axis=0)/np.mean(np.sum(data_order_limit_bid_vol,axis=0))\n",
    "plt.plot(list(range(-10,0)),vol_bid)\n",
    "plt.plot(list(range(1,11)),vol_ask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop the stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "marker_list=[\"s\",\"D\",\"^\",\"8\"]\n",
    "color_list=[\"g\",\"b\",\"r\",\"y\"]\n",
    "for ticker_ind in range(1,5):\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "    data_order_limit_ask_vol=data_order[:,1:40:4]\n",
    "    data_order_limit_bid_vol=data_order[:,3:40:4]\n",
    "\n",
    "    vol_ask=np.sum(data_order_limit_ask_vol,axis=0)/np.mean(np.sum(data_order_limit_ask_vol,axis=0))\n",
    "    vol_bid=np.sum(data_order_limit_bid_vol,axis=0)/np.mean(np.sum(data_order_limit_bid_vol,axis=0))\n",
    "    plt.plot(list(range(-10,0)),vol_bid,\n",
    "             \"--\",marker=marker_list[ticker_ind-1],color=color_list[ticker_ind-1],label=\n",
    "            ticker_list[ticker_ind])\n",
    "    plt.plot(list(range(1,11)),vol_ask,\"--\",marker=marker_list[ticker_ind-1],color=color_list[ticker_ind-1])\n",
    "plt.ylim([0.6,1.6])\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Average quantity offered in the market order book\")\n",
    "plt.xlabel(\"Price level of limit orders (negative axis : bids ; positive axis : asks)\")\n",
    "plt.ylabel(\"Average numbers of shares(Normalized by mean)\")\n",
    "plt.savefig(\"level_quantity.png\",bbox_inches='tight')    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) placement of orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticker_ind=2\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "data_order=data_order_list[ticker_ind]\n",
    "\n",
    "data_mess_limit=data_mess[data_mess[:,1]==1,:]\n",
    "data_order_limit=data_order[data_mess[:,1]==1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spread_list=[]\n",
    "for i in range(1,len(data_mess_limit)):\n",
    "    if data_mess_limit[i,5]==-1:\n",
    "        spread=data_mess_limit[i,4]-data_order_limit[i-1,0]\n",
    "    else:\n",
    "        spread=data_order_limit[i-1,2]-data_mess_limit[i,4]\n",
    "    spread_list.append(spread)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import math\n",
    "sns.kdeplot(np.array(spread_list), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "mu = 0\n",
    "variance = np.var(spread_list)\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(min(spread_list), max(spread_list), 100)\n",
    "plt.plot(x,mlab.normpdf(x, mu, sigma),\"r--\",label=\"Gaussian\")\n",
    "plt.xlim([-10000,10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ticker_ind in range(1,5):\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "\n",
    "    data_mess_limit=data_mess[data_mess[:,1]==1,:]\n",
    "    data_order_limit=data_order[data_mess[:,1]==1,:]\n",
    "\n",
    "    spread_list=[]\n",
    "    for i in range(1,len(data_mess_limit)):\n",
    "        if data_mess_limit[i,5]==-1:\n",
    "            spread=data_mess_limit[i,4]-data_order_limit[i-1,0]\n",
    "        else:\n",
    "            spread=data_order_limit[i-1,2]-data_mess_limit[i,4]\n",
    "        spread_list.append(spread)\n",
    "\n",
    "\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import matplotlib.mlab as mlab\n",
    "    import math\n",
    "    sns.kdeplot(np.array(spread_list), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "    mu = 0\n",
    "    variance = np.var(spread_list)\n",
    "    sigma = math.sqrt(variance)\n",
    "    x = np.linspace(min(spread_list), max(spread_list), 100)\n",
    "    plt.plot(x,mlab.normpdf(x, mu, sigma),\"r--\",label=\"Gaussian\")\n",
    "    plt.xlim([min(spread_list)*0.8,max(spread_list)*0.8])\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Placement of limit orders using the\\n same best quote reference for \"+ticker_list[ticker_ind])\n",
    "    plt.xlabel(\"Price diference\")\n",
    "    plt.ylabel(\"Probability density function\")\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_placement.png\",bbox_inches='tight')    \n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
