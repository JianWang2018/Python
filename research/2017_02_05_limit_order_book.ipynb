{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit order book\n",
    "\n",
    "author: Jian Wang\n",
    "\n",
    "time: 2016-02-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Model prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for importing the AAPL data is: 2.9495279788970947\n",
      "The shape of the order data is:  (400391, 40)  of message data is:  (400391, 6)\n",
      "Time for importing the AMZN data is: 1.9624300003051758\n",
      "The shape of the order data is:  (269748, 40)  of message data is:  (269748, 6)\n",
      "Time for importing the GOOG data is: 0.9510385990142822\n",
      "The shape of the order data is:  (147916, 40)  of message data is:  (147916, 6)\n",
      "Time for importing the INTC data is: 4.270112037658691\n",
      "The shape of the order data is:  (624040, 40)  of message data is:  (624040, 6)\n",
      "Time for importing the MSFT data is: 5.74663233757019\n",
      "The shape of the order data is:  (668765, 40)  of message data is:  (668765, 6)\n",
      "Check the original data:\n",
      "\n",
      "The first five sampe of AAPL is:  [[  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86100000e+06   2.00000000e+02   5.85100000e+06   5.00000000e+00\n",
      "    5.86890000e+06   3.00000000e+02   5.85010000e+06   8.90000000e+01\n",
      "    5.86950000e+06   5.00000000e+01   5.84970000e+06   5.00000000e+00\n",
      "    5.87000000e+06   1.00000000e+02   5.84930000e+06   3.00000000e+02\n",
      "    5.87100000e+06   1.00000000e+01   5.84650000e+06   3.00000000e+02\n",
      "    5.87390000e+06   1.00000000e+02   5.84530000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84380000e+06   2.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84270000e+06   3.00000000e+02]\n",
      " [  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85320000e+06   1.80000000e+01\n",
      "    5.86100000e+06   2.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86890000e+06   3.00000000e+02   5.85100000e+06   5.00000000e+00\n",
      "    5.86950000e+06   5.00000000e+01   5.85010000e+06   8.90000000e+01\n",
      "    5.87000000e+06   1.00000000e+02   5.84970000e+06   5.00000000e+00\n",
      "    5.87100000e+06   1.00000000e+01   5.84930000e+06   3.00000000e+02\n",
      "    5.87390000e+06   1.00000000e+02   5.84650000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84530000e+06   3.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84380000e+06   2.00000000e+02]\n",
      " [  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85320000e+06   1.80000000e+01\n",
      "    5.86100000e+06   2.00000000e+02   5.85310000e+06   1.80000000e+01\n",
      "    5.86890000e+06   3.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86950000e+06   5.00000000e+01   5.85100000e+06   5.00000000e+00\n",
      "    5.87000000e+06   1.00000000e+02   5.85010000e+06   8.90000000e+01\n",
      "    5.87100000e+06   1.00000000e+01   5.84970000e+06   5.00000000e+00\n",
      "    5.87390000e+06   1.00000000e+02   5.84930000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84650000e+06   3.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84530000e+06   3.00000000e+02]]\n",
      "\n",
      "The first five sampe of AMZN is:  [[  2.23950000e+06   1.00000000e+02   2.23180000e+06   1.00000000e+02\n",
      "    2.23990000e+06   1.00000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24400000e+06   5.47000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24540000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.24890000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.26770000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29430000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02\n",
      "    2.29800000e+06   1.00000000e+02   2.18970000e+06   1.00000000e+02]\n",
      " [  2.23950000e+06   1.00000000e+02   2.23810000e+06   2.10000000e+01\n",
      "    2.23990000e+06   1.00000000e+02   2.23180000e+06   1.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24400000e+06   5.47000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24540000e+06   1.00000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24890000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.26770000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.29430000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29800000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02]\n",
      " [  2.23950000e+06   1.00000000e+02   2.23810000e+06   2.10000000e+01\n",
      "    2.23960000e+06   2.00000000e+01   2.23180000e+06   1.00000000e+02\n",
      "    2.23990000e+06   1.00000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24400000e+06   5.47000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24540000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.24890000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.26770000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29430000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02]]\n",
      "\n",
      "The first five sampe of GOOG is:  [[  5.80230000e+06   1.00000000e+02   5.79400000e+06   4.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]\n",
      " [  5.80230000e+06   1.00000000e+02   5.79400000e+06   1.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]\n",
      " [  5.80230000e+06   1.00000000e+02   5.79400000e+06   1.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]]\n",
      "\n",
      "The first five sampe of INTC is:  [[  2.75200000e+05   6.60000000e+01   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74600000e+05   7.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74500000e+05   9.00000000e+02\n",
      "    2.76100000e+05   2.30000000e+03   2.74400000e+05   2.80000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76300000e+05   2.00000000e+03   2.74200000e+05   4.06300000e+03]\n",
      " [  2.75200000e+05   1.66000000e+02   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74600000e+05   7.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74500000e+05   9.00000000e+02\n",
      "    2.76100000e+05   2.30000000e+03   2.74400000e+05   2.80000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76300000e+05   2.00000000e+03   2.74200000e+05   4.06300000e+03]\n",
      " [  2.75200000e+05   1.66000000e+02   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75500000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74600000e+05   7.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74500000e+05   9.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74400000e+05   2.80000000e+03\n",
      "    2.76100000e+05   2.30000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74200000e+05   4.06300000e+03]]\n",
      "\n",
      "The first five sampe of MSFT is:  [[  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10500000e+05   1.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10600000e+05   1.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08800000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08500000e+05   4.00000000e+02\n",
      "    3.11400000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]\n",
      " [  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10500000e+05   2.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10600000e+05   1.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08800000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08500000e+05   4.00000000e+02\n",
      "    3.11400000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]\n",
      " [  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10400000e+05   1.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10500000e+05   2.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10600000e+05   1.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08800000e+05   2.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08500000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]]\n",
      "92.37662696838379\n",
      "The shape of the total response is:\n",
      "\n",
      "(400236, 1)\n",
      "(269571, 1)\n",
      "(147766, 1)\n",
      "(622641, 1)\n",
      "(667701, 1)\n",
      "The shape of the reduced response is:\n",
      "\n",
      "(309538, 1)\n",
      "(218710, 1)\n",
      "(118877, 1)\n",
      "(458160, 1)\n",
      "(511299, 1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Aug 26 00:03:47 2016\n",
    "\n",
    "@author: jianwang\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set default parameters\n",
    "ticker_list=[\"AAPL\",\"AMZN\",\"GOOG\",\"INTC\",\"MSFT\"]\n",
    "start_ind=10*3600\n",
    "end_ind=15.5*3600\n",
    "data_order_list=[]\n",
    "data_mess_list=[]\n",
    "time_index_list=[]\n",
    "path_save='/media/jianwang/Study/Research/order_book/'\n",
    "path_load=\"/media/jianwang/Study/Research/order_book/\"\n",
    "\n",
    "## set random seed to produce the same results\n",
    "\n",
    "np.random.seed(987612345)\n",
    "\n",
    "#read the stock ticker\n",
    "#totally 5 dataset\n",
    "\n",
    "for i in range(len(ticker_list)):\n",
    "    #get the path for the csv files\n",
    "    # name_order is for the order book and name_mess for the message book\n",
    "    name_order='_2012-06-21_34200000_57600000_orderbook_10.csv'\n",
    "    name_mess='_2012-06-21_34200000_57600000_message_10.csv'\n",
    "    # calculate the cputime for reading the data\n",
    "    t=time.time()\n",
    "    # header =-1 means that the first line is not the header, otherwise, the first line will be header\n",
    "    # data_order is for order book and data mess is for message book\n",
    "    data_order_list.append(np.array(pd.read_csv(path_load+ticker_list[i]+name_order,header=-1),dtype=\"float64\"))\n",
    "    data_mess_list.append(np.array(pd.read_csv(path_load+ticker_list[i]+name_mess,header=-1),dtype=\"float64\"))\n",
    "    print(\"Time for importing the \"+ticker_list[i]+\" data is:\",time.time()-t)\n",
    "    print(\"The shape of the order data is: \",data_order_list[i].shape, \" of message data is: \", data_mess_list[i].shape)\n",
    "    # get the time index\n",
    "    time_index_list.append(data_mess＿list[i][:,0])\n",
    "\n",
    "\n",
    "#print the sample of data\n",
    "print(\"Check the original data:\")\n",
    "\n",
    "for i in range(len(ticker_list)):\n",
    "    print()\n",
    "    print(\"The first five sampe of \"+ticker_list[i]+\" is: \",data_order_list[i][:3])\n",
    "\n",
    "    # -*- coding: utf-8 -*-\n",
    "\n",
    "# # save the feature array\n",
    "# ##get the original order,message and time index data, header =-1 means that did not\n",
    "# ##read the first column as the name\n",
    "#%%\n",
    "# # use a loop to read data\n",
    "# for ticker_ind in range(len(ticker_list)):\n",
    "#     data_order=data_order_list[ticker_ind]\n",
    "#     data_mess=data_mess_list[ticker_ind]\n",
    "#     time_index=data_mess[:,0]\n",
    "#     # obtain the reduced order message and time_index dataset, half an hour after the\n",
    "#     # 9:30 and half an hour before 16:00\n",
    "#     # data_reduced is used to install the data from 10 to 15:30, take half hour for auction\n",
    "#     data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "#     data_mess_reduced=data_mess[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "#     time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "\n",
    "#     test_lower=0\n",
    "#     # test up is the up index of the original data to construct the test data\n",
    "#     test_upper=len(data_order_reduced)\n",
    "#     # data_test is the subset of data_reduced from the lower index to upper index\n",
    "#     data_order_test=data_order_reduced[test_lower:test_upper,:]\n",
    "#     data_mess_test=data_mess_reduced[test_lower:test_upper,:]\n",
    "#     t=time.time()\n",
    "#     feature_array=get_features (data_order, data_mess,data_order_test,data_mess_test)\n",
    "#     np.savetxt(path_save+ticker_list[ticker_ind]+'_feature_array.txt',feature_array,delimiter=' ')\n",
    "#     print (\"Time for building \"+ticker_list[ticker_ind]+\" is:\",time.time()-t)\n",
    "\n",
    "\n",
    "# load the feature\n",
    "#%%\n",
    "import time\n",
    "t=time.time()\n",
    "feature_array_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    feature_array_list.append(np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_feature_array.txt',\\\n",
    "                                                   sep=' ',header=-1)))\n",
    "print(time.time()-t)\n",
    "\n",
    "# this function used to build the y\n",
    "# ask_low as 1 bad high as -1 and no arbitrage as 0\n",
    "# option=1 return ask low, option =2 return bid high, option =3 return no arbi, option =4 return total(ask_low=1,\n",
    "# bid_high =-1 and no arbi =0)\n",
    "#%%\n",
    "def build_y(ask_low,bid_high,no_arbi,option):\n",
    "    if (option==1):\n",
    "        return ask_low\n",
    "    elif option==2:\n",
    "        return bid_high\n",
    "    elif option==3:\n",
    "        return no_arbi\n",
    "    elif option==4:\n",
    "        return ask_low-bid_high\n",
    "    else:\n",
    "        print(\"option should be 1,2,3,4\")\n",
    "\n",
    "## save y data\n",
    "#%%\n",
    "#time_ind=1\n",
    "#option_ind=1\n",
    "#for ticker_ind in range(len(ticker_list)):\n",
    "#    response=build_y(ask_low_time_list[ticker_ind][time_ind],bid_high_time_list[ticker_ind][time_ind],\\\n",
    "#                                 no_arbi_time_list[ticker_ind][time_ind],option=option_ind)\n",
    "#    np.savetxt(path_save+ticker_list[ticker_ind]+'_response.txt',response)\n",
    "\n",
    "\n",
    "\n",
    "## load y data\n",
    "#%%\n",
    "response_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    response_list.append((np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_response.txt',header=-1))))\n",
    "\n",
    "\n",
    "## print the shape of the response\n",
    "## note it is the total response\n",
    "#%%\n",
    "print(\"The shape of the total response is:\\n\")\n",
    "\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_list[ticker_ind].shape)\n",
    "\n",
    "# need to get the response from 10 to 15:30\n",
    "# the shape of the response and the feature array should be equal\n",
    "response_reduced_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    first_ind = np.where(time_index_list[ticker_ind]>=start_ind)[0][0]\n",
    "    last_ind=np.where(time_index_list[ticker_ind]<=end_ind)[0][-1]\n",
    "    response_reduced_list.append(response_list[ticker_ind][first_ind:last_ind+1])\n",
    "\n",
    "print(\"The shape of the reduced response is:\\n\")\n",
    "\n",
    "## print the shape of reduced response\n",
    "## response reduced is used for testing and training the model\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_reduced_list[ticker_ind].shape)\n",
    "\n",
    "    \n",
    "    # random generate a given \n",
    "def random_choice(num, key):\n",
    "    temp=np.random.choice(num,size=key,replace=False)\n",
    "    temp_sort=sorted(temp)\n",
    "    for i in range(len(temp)):\n",
    "        num[temp_sort[i]]=temp[i]\n",
    "    \n",
    "    return num\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.train and test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# Random split\n",
    "#%%---------------------------------------------------------------------\n",
    "import random\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "ticker_ind=1\n",
    "size=100000\n",
    "\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x, test_x, train_y, test_y =train_test_split(\\\n",
    "total_array[:,:134],total_array[:,134], test_size=0.1, random_state=42)\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total array shape: (100000, 135)\n",
      "train_x shape: (90000, 134)\n",
      "test_x shape: (10000, 134)\n",
      "test_y shape: (10000,)\n",
      "train_y shape: (90000,)\n",
      "[ -3.29725663e-14   3.21911417e-15  -2.69869614e-14   8.46911168e-15\n",
      "  -3.46144436e-14   6.21850472e-15   2.29875077e-14  -9.54002309e-17\n",
      "   2.64249965e-14   2.94105851e-15   2.65813749e-14  -2.01292979e-15\n",
      "  -9.73248835e-16  -2.65577683e-15   2.10961235e-14  -5.76037289e-15\n",
      "   3.57229115e-15  -7.99334673e-16  -3.98401068e-14   4.17394005e-15\n",
      "  -3.18413688e-14   1.67420399e-15   3.34200564e-14  -3.61940639e-15\n",
      "   2.86818983e-14   4.92262465e-15  -1.62683351e-14   6.08694391e-15\n",
      "  -1.97175132e-14   1.17294544e-14   2.09344010e-14  -1.94452633e-15\n",
      "  -7.41535521e-15   8.38064926e-15   1.62785084e-14  -5.12082383e-15\n",
      "   1.66164198e-14   2.53715074e-15   2.34135877e-14  -7.04031895e-16\n",
      "  -4.92501102e-16   1.09149997e-14  -5.15191100e-15  -2.83654581e-15\n",
      "   1.34903200e-16  -1.52196416e-14   3.09094108e-15  -8.15181996e-15\n",
      "  -1.26800835e-14  -1.07553361e-14   9.92727906e-15   3.27181746e-14\n",
      "  -1.34520382e-14  -2.59234763e-14  -3.74743739e-15  -3.86610844e-15\n",
      "  -3.42394820e-14   2.83961273e-15  -2.95773477e-14  -2.33786097e-14\n",
      "  -8.50315805e-15  -9.96682983e-15   2.02587206e-15   1.77335344e-14\n",
      "  -2.17840733e-14   4.10910910e-14   3.79166180e-14  -1.25397668e-14\n",
      "   6.83272821e-15   9.18794917e-15   4.16249853e-15  -2.12976131e-14\n",
      "   3.52668681e-14   7.86079769e-14  -3.59916393e-14  -2.77464453e-14\n",
      "  -1.45594524e-14  -5.62068120e-14  -1.84227222e-14  -2.84819808e-14\n",
      "  -7.40776643e-14  -6.52142291e-15   1.26121212e-15  -4.48972341e-16\n",
      "  -5.67866028e-15  -1.88709465e-15   2.91516672e-15   1.86072878e-15\n",
      "   4.97080425e-15   5.02495335e-15   5.74349692e-15  -2.48493541e-15\n",
      "  -3.55423626e-15   1.82822588e-15   1.48095101e-15   8.70322826e-15\n",
      "   1.15418755e-14   3.11202175e-15  -6.38771381e-15  -2.62744394e-15\n",
      "   2.48784326e-15   1.34995965e-15   3.32010654e-15   4.37352253e-15\n",
      "  -9.51469001e-16   3.03237312e-15  -2.49050982e-16  -1.21934091e-15\n",
      "  -7.85269361e-16  -1.42151476e-15  -1.50912071e-15   1.39462878e-15\n",
      "  -2.15791094e-15   2.61906939e-15  -9.49102756e-16  -3.47021177e-15\n",
      "   2.68023381e-15   1.21102287e-15  -6.60912528e-16  -1.55672944e-15\n",
      "   9.40290592e-16  -8.76593088e-16  -1.70094078e-15   9.97405842e-16\n",
      "   2.39162060e-15  -1.79421200e-15   9.56576238e-15  -4.44593621e-15\n",
      "   1.53828771e-15   1.01566903e-16  -7.30092530e-16   1.85594749e-15\n",
      "   2.63075349e-15   1.23469514e-15]\n",
      "[-0.23313481 -0.09956344 -0.23356082 -0.03448974 -0.23473533 -0.10548651\n",
      " -0.23224842 -0.01664171 -0.23692298 -0.07267729 -0.23157014 -0.01299124\n",
      " -0.23797559 -0.02530352 -0.22928163  0.00302752 -0.23962718 -0.02095564\n",
      " -0.22672453  0.00227335 -0.24154527 -0.05076091 -0.224214   -0.00792908\n",
      " -0.24385598 -0.0167215  -0.22140685 -0.00164715 -0.24561069  0.01266766\n",
      " -0.21875462 -0.00530002 -0.24781565 -0.00477343 -0.21616909 -0.01715761\n",
      " -0.24946138 -0.02515412 -0.21345293 -0.00278556  0.06560618  0.03104013\n",
      " -0.0139557  -0.05428933 -0.09146549 -0.11549582 -0.1394798  -0.15333734\n",
      " -0.16745005 -0.17590279 -0.23354201 -0.23361782 -0.23435243 -0.2337433\n",
      " -0.23331924 -0.23307468 -0.23290223 -0.23254897 -0.23245234 -0.23201557\n",
      " -0.03259227 -0.08118991 -0.04934414 -0.08006018 -0.08258718 -0.09720371\n",
      " -0.07732119 -0.09572036 -0.0714825  -0.05306601 -0.04950718 -0.09675901\n",
      " -0.09759146 -0.08607669 -0.10774104 -0.10515204 -0.10250518 -0.10898553\n",
      " -0.24121579 -0.22487654 -0.1080301  -0.03619016 -0.11533634  0.01388295\n",
      "  0.02292113  0.0248569   0.02642114  0.03306608  0.03101454  0.03827524\n",
      "  0.04343051  0.04264961  0.03768701  0.04778076  0.00679291  0.00953063\n",
      "  0.01057045  0.01657141  0.01399854  0.01142128  0.01559502  0.02125931\n",
      "  0.02319337  0.01844705 -0.01272964  0.00555628  0.00299974  0.00640416\n",
      "  0.01396026 -0.01604343  0.01966284 -0.00836691  0.02654394 -0.03519341\n",
      " -0.00990192  0.02462078 -0.01021232  0.00752787 -0.0028069   0.00223379\n",
      " -0.00210946 -0.00683443  0.01096539 -0.01057717 -0.04162391 -0.02268925\n",
      " -0.02366466 -0.0461331  -0.02523676 -0.02502536  0.00146665  0.00952006\n",
      "  0.0048331  -0.00423335]\n"
     ]
    }
   ],
   "source": [
    "#time series split\n",
    "#%%--------------------------------------------------------------------------------------------\n",
    "\n",
    "ticker_ind=1\n",
    "size=100000\n",
    "random_ratio=0.7\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "total_array=total_array[random_choice(list(range(size)),int(size*random_ratio)),:]\n",
    "\n",
    "\n",
    "train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x=total_array[:train_num_index,:134]\n",
    "test_x=total_array[train_num_index:,:134]\n",
    "train_y=total_array[:train_num_index,134]\n",
    "test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "print(\"train_x shape:\",train_x.shape)\n",
    "print(\"test_x shape:\",test_x.shape)\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "# scale data\n",
    "#%%\n",
    "\n",
    "# can use the processing.scale function to scale the data\n",
    "from sklearn import preprocessing\n",
    "# note that we need to transfer the data type to float\n",
    "# remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "# use scale for zero mean and one std\n",
    "scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "train_x_scale=scaler.transform(train_x)\n",
    "test_x_scale=scaler.transform(test_x)\n",
    "\n",
    "print(np.mean(train_x_scale,0))\n",
    "print(np.mean(test_x_scale,0))\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Model build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x_scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#----------------\n",
    "# logistic l1\n",
    "#-----------------\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "        \n",
    "        # set the random state to make sure that each time get the same results\n",
    "\n",
    "time_logistic=time.time()\n",
    "clf = linear_model.LogisticRegression(C=1, penalty='l1', tol=1e-6,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "time_logistic=time.time()-time_logistic    \n",
    "\n",
    "print(time_logistic)\n",
    "\n",
    "# test the training error\n",
    "predict_y_logistic =np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y_logistic==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y_logistic,train_y)\n",
    "recall = recall_score(predict_y_logistic,train_y)\n",
    "f1=f1_score(predict_y_logistic,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n",
    "#----------------\n",
    "# logistic l2\n",
    "#-----------------\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "        \n",
    "        # set the random state to make sure that each time get the same results\n",
    "\n",
    "time_logistic=time.time()\n",
    "clf = linear_model.LogisticRegression(C=1, penalty='l2', tol=1e-6,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "time_logistic=time.time()-time_logistic    \n",
    "\n",
    "print(time_logistic)\n",
    "\n",
    "# test the training error\n",
    "predict_y_logistic =np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y_logistic==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y_logistic,train_y)\n",
    "recall = recall_score(predict_y_logistic,train_y)\n",
    "f1=f1_score(predict_y_logistic,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------------------\n",
    "# SVM_poly_2\n",
    "#---------------------\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "\n",
    "import time \n",
    "from sklearn import svm\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf = svm.SVC(C=1.0,kernel='poly',degree=2,max_iter=5000,shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y =np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "#draw the crosstab chart\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#---------------\n",
    "# decision tree\n",
    "#-----------------\n",
    "\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  tree.DecisionTreeClassifier(max_depth=10,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y=np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "print(\"test time is:\", time.time()-t)\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "#draw the crosstab chart\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#-----------------------------------------\n",
    "# Adaboost \n",
    "#-----------------------------------------\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "time_ada=time.time()\n",
    "clf =  AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10),n_estimators=100,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-time_ada)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y=np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "#draw the crosstab chart\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.65606713294983\n",
      "train_accuracy is: 0.997555555556\n",
      "precision is: \t 0.890899949723\n",
      "recall is: \t 0.998309859155\n",
      "f1 score is: \t 0.941551540914\n",
      "test time is: 0.1144249439239502\n",
      "test accuracy is: 0.9965\n",
      "precision is: \t 0.801204819277\n",
      "recall is: \t 0.985185185185\n",
      "f1 score is: \t 0.883720930233\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "time_rf=time.time()\n",
    "clf =  RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-time_rf)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y=np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "print(\"test time is:\", time.time()-t)\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "# test the score for the test data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## confusion matrix plot\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## see the feature_importances\n",
    "\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_x_scale[:100,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multi-class predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## load the arbitrage time txt data\n",
    "\n",
    "ask_low_time_list=[]\n",
    "bid_high_time_list=[]\n",
    "no_arbi_time_list=[] \n",
    "time_list=[1,5,10,15,20]\n",
    "import time \n",
    "t=time.time()\n",
    "for ticker_ind in range(5):  \n",
    "    ask_low_time_list.append([])\n",
    "    bid_high_time_list.append([])\n",
    "    no_arbi_time_list.append([])\n",
    "    for time_ind in range(len(time_list)):\n",
    "        ask_low_time_list[ticker_ind].append(\n",
    "            np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_ask_low_time_'+str(time_list[time_ind])+'.txt',header=-1)))\n",
    "        bid_high_time_list[ticker_ind].append(\n",
    "            np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_bid_high_time_'+str(time_list[time_ind])+'.txt',header=-1)))\n",
    "        no_arbi_time_list[ticker_ind].append(\n",
    "            np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_no_arbi_time_'+str(time_list[time_ind])+'.txt',header=-1)))\n",
    "        \n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deal with the data\n",
    "def build_y(ask_low,bid_high,no_arbi,option):\n",
    "    if (option==1):\n",
    "        return ask_low\n",
    "    elif option==2:\n",
    "        return bid_high\n",
    "    elif option==3:\n",
    "        return no_arbi\n",
    "    elif option==4:\n",
    "        return ask_low-bid_high\n",
    "    else:\n",
    "        print(\"option should be 1,2,3,4\")\n",
    "        \n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    response=build_y(ask_low_time_list[ticker_ind][1],bid_high_time_list[ticker_ind][1],\\\n",
    "                                 no_arbi_time_list[ticker_ind][1],option=4)\n",
    "    np.savetxt(path_save+ticker_list[ticker_ind]+'_multiresponse.txt',response)\n",
    "\n",
    "response_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    response_list.append((np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_multiresponse.txt',header=-1))))\n",
    "\n",
    "    ## print the shape of the response\n",
    "## note it is the total response\n",
    "print(\"The shape of the total response is:\\n\")\n",
    "\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_list[ticker_ind].shape)\n",
    "    \n",
    "# need to get the response from 10 to 15:30\n",
    "# the shape of the response and the feature array should be equal \n",
    "response_reduced_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    first_ind = np.where(time_index_list[ticker_ind]>=start_ind)[0][0]\n",
    "    last_ind=np.where(time_index_list[ticker_ind]<=end_ind)[0][-1]\n",
    "    response_reduced_list.append(response_list[ticker_ind][first_ind:last_ind+1])\n",
    "    \n",
    "print(\"The shape of the reduced response is:\\n\")\n",
    "\n",
    "## print the shape of reduced response\n",
    "## response reduced is used for testing and training the model\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_reduced_list[ticker_ind].shape)\n",
    "    # random split data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random split\n",
    "#split the data to train and test data set\n",
    "import random\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "ticker_ind=1\n",
    "size=100000\n",
    "\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "\n",
    "print(\"total shape:\",total_array.shape)\n",
    "\n",
    "train_x, test_x, train_y, test_y =train_test_split(\\\n",
    "total_array[:,:134],total_array[:,134], test_size=0.1, random_state=42)\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "\n",
    "print(\"test shape:\",test_y.shape)\n",
    "print(\"train shape:\",train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#time series split\n",
    "#%%--------------------------------------------------------------------------------------------\n",
    "\n",
    "ticker_ind=1\n",
    "size =100000\n",
    "time_index=time_index_list[ticker_ind]\n",
    "# combine the feature and response array to random sample\n",
    "time_index_reduced=time_index[(time_index>=start_ind)&(time_index<=end_ind)]\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                            time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)[:size,:]\n",
    "\n",
    "total_array=total_array[np.random.randint(len(total_array),size=len(total_array)),:]\n",
    "\n",
    "train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x=total_array[:train_num_index,:134]\n",
    "test_x=total_array[train_num_index:,:134]\n",
    "train_y=total_array[:train_num_index,134]\n",
    "test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "print(\"train_x shape:\",train_x.shape)\n",
    "print(\"test_x shape:\",test_x.shape)\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# scale the data\n",
    "# can use the processing.scale function to scale the data\n",
    "from sklearn import preprocessing\n",
    "# note that we need to transfer the data type to float\n",
    "# remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "# use scale for zero mean and one std\n",
    "scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "train_x_scale=scaler.transform(train_x)\n",
    "test_x_scale=scaler.transform(test_x)\n",
    "\n",
    "print(np.mean(train_x_scale,0))\n",
    "print(np.mean(test_x_scale,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one vs one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only run for random forest method\n",
    "# one vs one case\n",
    "# random forest\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "## sample weights\n",
    "#sample_weights=[]\n",
    "#ratio=len(train_y)/sum(train_y==1)/10\n",
    "#for i in range(len(train_x)):\n",
    "#    if train_y[i]==0:\n",
    "#        sample_weights.append(1)\n",
    "#    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  OneVsOneClassifier(RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345))\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "%matplotlib inline\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_one.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only run for random forest method\n",
    "# one vs one case\n",
    "# adaboosting\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "## sample weights\n",
    "#sample_weights=[]\n",
    "#ratio=len(train_y)/sum(train_y==1)/10\n",
    "#for i in range(len(train_x)):\n",
    "#    if train_y[i]==0:\n",
    "#        sample_weights.append(1)\n",
    "#    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10),n_estimators=100,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_one.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------\n",
    "# one vs one case\n",
    "# svm\n",
    "#-------------------\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "\n",
    "\n",
    "## sample weights\n",
    "#sample_weights=[]\n",
    "#ratio=len(train_y)/sum(train_y==1)/10\n",
    "#for i in range(len(train_x)):\n",
    "#    if train_y[i]==0:\n",
    "#        sample_weights.append(1)\n",
    "#    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  OneVsOneClassifier(svm.SVC(C=1.0,kernel='poly',degree=2,max_iter=5000,shrinking=True, tol=0.001, verbose=False)\n",
    ")\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_one.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# only run for random forest method\n",
    "# one vs rest case\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  OneVsRestClassifier(RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345))\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_rest.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.P&L calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_index(index, value):\n",
    "    i=0\n",
    "    while index[i] <value:\n",
    "        i=i+1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## for AMZN\n",
    "ticker_ind =1\n",
    "train_ratio=0.9\n",
    "time_index=data_mess[:,0]\n",
    "data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "total_array_old=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)\n",
    "data_order=data_order_list[ticker_ind]\n",
    "data_mess=data_mess_list[ticker_ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_order_test=data_order_reduced[int(size*train_ratio):size,:]\n",
    "time_index_test=time_index_reduced[int(size*train_ratio):size]\n",
    "\n",
    "test_y_unrandom=total_array_old[int(size*train_ratio):size,134]\n",
    "print(data_order_test.shape)\n",
    "print(time_index_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_bid_high_choose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(time_index_test[:10000],data_order_test[:10000,0],\"r-\",label=\"Ask price\")\n",
    "plt.plot(time_index_test[:10000],data_order_test[:10000,2],\"b-\",label=\"Bid price\")\n",
    "\n",
    "x_ask_low_choose=time_index_test[test_y_unrandom==1]\n",
    "y_ask_low_choose=data_order_test[test_y_unrandom==1,0]\n",
    "x_bid_high_choose=time_index_test[test_y_unrandom==-1]\n",
    "y_bid_high_choose=data_order_test[test_y_unrandom==-1,2]\n",
    "\n",
    "plt.plot(x_ask_low_choose[:30],y_ask_low_choose[:30],\"gv\",markersize=8,label=\"Ask low\")\n",
    "plt.plot(x_bid_high_choose[:30],y_bid_high_choose[:30],\"r^\",markersize=8,label=\"Bid high\")\n",
    "plt.xlabel(\"Time(s)\")\n",
    "plt.ylabel(\"Price($10^{-4}$\\$)\")\n",
    "plt.legend(bbox_to_anchor=[1.4, 1])\n",
    "plt.title(\"Arbitrage opportunities for \"+ticker_list[ticker_ind]+\"(5s)\")\n",
    "plt.savefig(\"arbitrage_plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_index_test=total_array[:,135][int(size*train_ratio):size]\n",
    "# find the arbitrage occuring index\n",
    "arbi_index=list(np.where(predict_y_test!=0)[0])\n",
    "# find the index that 5 seconds later\n",
    "arbi_future_index=[]\n",
    "for i in arbi_index:\n",
    "    arbi_future_index.append(get_index(time_index_reduced,time_index_test[i]+5))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arbi_future_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_array_test=total_array[int(size*train_ratio):size,:]\n",
    "future_price=[]\n",
    "current_price=[]\n",
    "pnl=[]\n",
    "for i in range(len(arbi_index)):\n",
    "    #ask low\n",
    "    if predict_y_test[arbi_index[i]]==1 :\n",
    "        future_price=data_order_reduced[arbi_future_index[i],0]\n",
    "        current_price=total_array_test[arbi_index[i],2]\n",
    "        pnl.append(current_price-future_price)\n",
    "    # bid high\n",
    "    else: \n",
    "        future_price=data_order_reduced[arbi_future_index[i],2]\n",
    "        current_price=total_array_test[arbi_index[i],0]\n",
    "        pnl.append(future_price-current_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pnl=np.array(pnl)\n",
    "predict_arbi=predict_y_test[predict_y_test!=0]\n",
    "plt.plot(pnl[predict_arbi==1],\"b.\",label=\"Ask low PnL\")\n",
    "plt.plot(pnl[predict_arbi==-1],\"r.\",label=\"Bid High PnL\")\n",
    "\n",
    "plt.xlabel(\"Arbitrage Index\")\n",
    "plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "plt.title(\"PnL for \"+ticker_list[ticker_ind])\n",
    "plt.legend()\n",
    "plt.savefig(ticker_list[ticker_ind]+\"_pnl.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cum_pnl=np.cumsum(pnl)\n",
    "plt.plot(cum_pnl,\"b.\",label=\"Cumulative P&L\")\n",
    "plt.xlabel(\"Arbitrage Index\")\n",
    "plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "plt.title(\"Cumulative PnL for \"+ticker_list[ticker_ind])\n",
    "plt.legend()\n",
    "plt.savefig(ticker_list[ticker_ind]+\"_cum_pnl.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop for all stock to plot the pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#time series split\n",
    "#%%--------------------------------------------------------------------------------------------\n",
    "\n",
    "size =100000\n",
    "for ticker_ind in range(2,5):\n",
    "    # combine the feature and response array to random sample\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    time_index=data_mess[:,0]\n",
    "    data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    total_array_old=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                    time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)\n",
    "\n",
    "    total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)[:size,:]\n",
    "    total_array=total_array[np.random.randint(len(total_array),size=len(total_array)),:]\n",
    "\n",
    "    train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "    print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "    #split the data to train and test data set\n",
    "    train_x=total_array[:train_num_index,:134]\n",
    "    test_x=total_array[train_num_index:,:134]\n",
    "    train_y=total_array[:train_num_index,134]\n",
    "    test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "    # the y data need to reshape to size (n,) not (n,1)\n",
    "    test_y=test_y.reshape(len(test_y),)\n",
    "    train_y=train_y.reshape(len(train_y),)\n",
    "    print(\"train_x shape:\",train_x.shape)\n",
    "    print(\"test_x shape:\",test_x.shape)\n",
    "    print(\"test_y shape:\",test_y.shape)\n",
    "    print(\"train_y shape:\",train_y.shape)\n",
    "\n",
    "\n",
    "    # scale the data\n",
    "    # can use the processing.scale function to scale the data\n",
    "    from sklearn import preprocessing\n",
    "    # note that we need to transfer the data type to float\n",
    "    # remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "    # use scale for zero mean and one std\n",
    "    scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "    train_x_scale=scaler.transform(train_x)\n",
    "    test_x_scale=scaler.transform(test_x)\n",
    "\n",
    "    print(np.mean(train_x_scale,0))\n",
    "    print(np.mean(test_x_scale,0))\n",
    "\n",
    "    from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "    # change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "    t=time.time()\n",
    "    clf =  OneVsRestClassifier(RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345))\n",
    "    clf.fit(train_x_scale,train_y)\n",
    "\n",
    "    print(time.time()-t)\n",
    "\n",
    "    predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "    print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "    # define a function to prefict the result by threshold\n",
    "    # note: logistic model will return two probability\n",
    "    def predict_threshold(predict_proba, threshold):\n",
    "        res=[]\n",
    "        for i in range(len(predict_proba)):\n",
    "            res.append(int(predict_proba[i][1]>threshold))\n",
    "        return res\n",
    "\n",
    "    t=time.time()\n",
    "    predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "    print(\"test time is :\",time.time()-t)\n",
    "    print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "    # # test the score for the train data\n",
    "    # from sklearn.metrics import (precision_score, recall_score,\n",
    "    #                              f1_score)\n",
    "    # print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "    # precision= precision_score(predict_y_test,test_y)\n",
    "    # recall = recall_score(predict_y_test,test_y)\n",
    "    # f1=f1_score(predict_y_test,test_y)\n",
    "    # print(\"precision is: \\t %s\" % precision)\n",
    "    # print(\"recall is: \\t %s\" % recall)\n",
    "    # print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "    # #draw the crosstab chart\n",
    "    # %matplotlib inline\n",
    "    # ## draw chart for the cross table\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(3)\n",
    "        plt.xticks(tick_marks, [-1,0,1])\n",
    "        plt.yticks(tick_marks, [-1,0,1])\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(test_y, predict_y_test)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm)\n",
    "    plt.savefig(\"one_vs_rest.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    def get_index(index, value):\n",
    "        i=0\n",
    "        while index[i] <value:\n",
    "            i=i+1\n",
    "        return i\n",
    "\n",
    "\n",
    "    train_ratio=0.9\n",
    "    time_index=data_mess[:,0]\n",
    "    data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    total_array_old=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                    time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "\n",
    "    time_index_test=total_array[:,135][int(size*train_ratio):size]\n",
    "    # find the arbitrage occuring index\n",
    "    arbi_index=list(np.where(predict_y_test!=0)[0])\n",
    "    # find the index that 5 seconds later\n",
    "    arbi_future_index=[]\n",
    "    for i in arbi_index:\n",
    "        arbi_future_index.append(get_index(time_index_reduced,time_index_test[i]+5))\n",
    "\n",
    "    total_array_test=total_array[int(size*train_ratio):size,:]\n",
    "    future_price=[]\n",
    "    current_price=[]\n",
    "    pnl=[]\n",
    "    for i in range(len(arbi_index)):\n",
    "        #ask low\n",
    "        if predict_y_test[arbi_index[i]]==1 :\n",
    "            future_price=data_order_reduced[arbi_future_index[i],0]\n",
    "            current_price=total_array_test[arbi_index[i],2]\n",
    "            pnl.append(current_price-future_price)\n",
    "        # bid high\n",
    "        else: \n",
    "            future_price=data_order_reduced[arbi_future_index[i],2]\n",
    "            current_price=total_array_test[arbi_index[i],0]\n",
    "            pnl.append(future_price-current_price)\n",
    "\n",
    "    pnl=np.array(pnl)\n",
    "    predict_arbi=predict_y_test[predict_y_test!=0]\n",
    "    plt.plot(pnl[predict_arbi==1],\"b.\",label=\"Ask low PnL\")\n",
    "    plt.plot(pnl[predict_arbi==-1],\"r.\",label=\"Bid High PnL\")\n",
    "\n",
    "    plt.xlabel(\"Arbitrage Index\")\n",
    "    plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "    plt.title(\"PnL for \"+ticker_list[ticker_ind])\n",
    "    plt.legend()\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_pnl.png\")\n",
    "    plt.show()\n",
    "\n",
    "    cum_pnl=np.cumsum(pnl)\n",
    "    plt.plot(cum_pnl,\"b.\",label=\"Cumulative P&L\")\n",
    "    plt.xlabel(\"Arbitrage Index\")\n",
    "    plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "    plt.title(\"Cumulative PnL for \"+ticker_list[ticker_ind])\n",
    "    plt.legend()\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_cum_pnl.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the order book type\n",
    "\n",
    "use the data_mess data set to plot the chart of the order book type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Plot the order book types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "order_type_list=[]\n",
    "t=time.time()\n",
    "for ticker_ind in range(5):\n",
    "    order_type=[]\n",
    "    for i in [1,2,3,4,5]:\n",
    "        order_type.append(sum(data_mess_list[ticker_ind][:,1]==i))\n",
    "    order_type_list.append(order_type)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(order_type_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# n_groups = 5\n",
    "\n",
    "# means_men = (20, 35, 30, 35, 27)\n",
    "# std_men = (2, 3, 4, 1, 2)\n",
    "\n",
    "# means_women = (25, 32, 34, 20, 25)\n",
    "# std_women = (3, 5, 2, 3, 3)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# index = np.arange(n_groups)\n",
    "# bar_width = 0.35\n",
    "\n",
    "# opacity = 0.4\n",
    "# error_config = {'ecolor': '0.3'}\n",
    "\n",
    "# rects1 = plt.bar(index, means_men, bar_width,\n",
    "#                  alpha=opacity,\n",
    "#                  color='b',\n",
    "#                  yerr=std_men,\n",
    "#                  error_kw=error_config,\n",
    "#                  label='Men')\n",
    "\n",
    "# rects2 = plt.bar(index + bar_width, means_women, bar_width,\n",
    "#                  alpha=opacity,\n",
    "#                  color='r',\n",
    "#                  yerr=std_women,\n",
    "#                  error_kw=error_config,\n",
    "#                  label='Women')\n",
    "\n",
    "# plt.xlabel('Group')\n",
    "# plt.ylabel('Scores')\n",
    "# plt.title('Scores by group and gender')\n",
    "# plt.xticks(index + bar_width, ('A', 'B', 'C', 'D', 'E'))\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "order_type_array=np.array(order_type_list)\n",
    "\n",
    "\n",
    "n_groups=7.5\n",
    "index = np.arange(n_groups,step=1.5)    # the x locations for the groups\n",
    "ticker_list=['AAPL', 'AMZN', 'GOOG', 'INTC','MSFT']\n",
    "color_list=['red','yellow','green','blue','darkmagenta']\n",
    "type_list=['1:Order_book','2:Cancel_part','3:Delete_all','4:Execution_visible','5:Execution_hidden']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "bar_width = 0.25\n",
    "\n",
    "opacity = 0.6\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, order_type_array[:,0], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[0],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[0])\n",
    "\n",
    "rects2 = plt.bar(index + 1*bar_width, order_type_array[:,1], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[1],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[1])\n",
    "\n",
    "\n",
    "rects3 = plt.bar(index + 2*bar_width, order_type_array[:,2], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[2],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[2])\n",
    "\n",
    "rects4 = plt.bar(index + 3*bar_width, order_type_array[:,3], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[3],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[3])\n",
    "\n",
    "rects5 = plt.bar(index + 4*bar_width, order_type_array[:,4], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[4],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[4])\n",
    "\n",
    "\n",
    "plt.xlabel('Stock Ticker')\n",
    "plt.ylabel('Numbers')\n",
    "plt.title('Order Book Types')\n",
    "plt.xticks(index + bar_width*2.5, ticker_list)\n",
    "plt.yticks(np.arange(0, 700000,50000))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Plot the arbitrage situation (bid high, ask low and no arbitrage)\n",
    "\n",
    "Take the first stock which is AAPL as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_order_reduced=data_order_list[0][(time_index_list[0]>= start_ind) & (time_index_list[0]<= end_ind)]\n",
    "data_mess_reduced=data_mess_list[0][(time_index_list[0]>= start_ind) & (time_index_list[0]<= end_ind)]\n",
    "time_index_reduced=time_index_list[0][(time_index_list[0]>= start_ind) & (time_index_list[0]<= end_ind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_ind=np.where(ask_low_time_list[0][1]==1)[0][0]\n",
    "last_ind=np.where(time_index_reduced>time_index_reduced[first_ind]+5)[0][0]\n",
    "print(\"first_ind:\",first_ind)\n",
    "print(\"last_ind:\",last_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "time_index=time_index_reduced[first_ind:last_ind+1]\n",
    "ask_price=data_order_reduced[first_ind:last_ind+1,0]\n",
    "bid_price=data_order_reduced[first_ind:last_ind+1,2]\n",
    "print(ask_pirce[1])\n",
    "print(bid_price[1])\n",
    "plt.plot(time_index,ask_price,'r.-',label=\"Ask Price\")\n",
    "plt.plot(time_index,bid_price,'b.-',label=\"Bid Price\")\n",
    "\n",
    "plt.xticks=time_index\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Ask Low Arbitrage Example\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.where(bid_high_time_list[0][1]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the bid high case\n",
    "\n",
    "first_ind=np.where(bid_high_time_list[0][1]==1)[0][20]\n",
    "last_ind=np.where(time_index_list[0]>time_index_list[0][first_ind]+5)[0][0]\n",
    "print(\"first_ind:\",first_ind)\n",
    "print(\"last_ind:\",last_ind)\n",
    "\n",
    "%matplotlib qt\n",
    "time_index=time_index_list[0][first_ind:last_ind+1]\n",
    "ask_price=data_order_list[0][first_ind:last_ind+1,0]\n",
    "bid_price=data_order_list[0][first_ind:last_ind+1,2]\n",
    "print(ask_pirce[1])\n",
    "print(bid_price[1])\n",
    "plt.plot(time_index,ask_price,'r.-',label=\"Ask Price\")\n",
    "plt.plot(time_index,bid_price,'b.-',label=\"Bid Price\")\n",
    "\n",
    "plt.xticks=time_index\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Bid High Arbitrage Example\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the no arbitrage case\n",
    "first_ind=np.where(no_arbi_time_list[0][1]==1)[0][20]\n",
    "last_ind=np.where(time_index_list[0]>time_index_list[0][first_ind]+5)[0][0]\n",
    "print(\"first_ind:\",first_ind)\n",
    "print(\"last_ind:\",last_ind)\n",
    "\n",
    "%matplotlib qt\n",
    "time_index=time_index_list[0][first_ind:last_ind+1]\n",
    "ask_price=data_order_list[0][first_ind:last_ind+1,0]\n",
    "bid_price=data_order_list[0][first_ind:last_ind+1,2]\n",
    "print(ask_pirce[1])\n",
    "print(bid_price[1])\n",
    "plt.plot(time_index,ask_price,'r.-',label=\"Ask Price\")\n",
    "plt.plot(time_index,bid_price,'b.-',label=\"Bid Price\")\n",
    "\n",
    "plt.xticks=time_index\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"No Arbitrage Example\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.plot the statistical properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) cumulative distribution function for arrival time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticker_ind=2\n",
    "data=data_mess_list[ticker_ind]\n",
    "# we use the market order\n",
    "data_order=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "\n",
    "arrival_time=data_order[1:,0]-data_order[0:-1,0]\n",
    "#delete the zero intra arrival time\n",
    "arrival_time=arrival_time[arrival_time>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu_log=np.mean(np.log(arrival_time))\n",
    "std_log=np.std(np.log(arrival_time))\n",
    "data_log=np.random.lognormal(mu_log,std_log,arrival_time.shape)\n",
    "\n",
    "mu_exp=np.mean(arrival_time)\n",
    "data_exp=np.random.exponential(mu_exp,arrival_time.shape)\n",
    "\n",
    "data_weibull=np.random.weibull(0.38,arrival_time.shape)\n",
    "beta=np.var(arrival_time)/np.mean(arrival_time)\n",
    "alpha=np.mean(arrival_time)/beta\n",
    "data_gamma=np.random.gamma(alpha,beta,arrival_time.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.stats import lognorm\n",
    "ecdf = sm.distributions.ECDF(arrival_time,)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"b\",label=\"Original data\")\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_log)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"g\",label=\"Lognormal Distribution\")\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_exp)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"y\",label=\"Exponential distribution\")\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_weibull)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"r\",label=\"Weibull distribution\")\n",
    "\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_t)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"purple\",label=\"Gamma distribution\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Intra-arrival time\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Cumulative distribution function of order arrival time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) loop for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2, 2,figsize=(13, 13))\n",
    "for tickerb_ind in range(1,5):\n",
    "    data=data_mess_list[ticker_ind]\n",
    "    # we use the market order\n",
    "    data_order=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "\n",
    "    arrival_time=data_order[1:,0]-data_order[0:-1,0]\n",
    "    #delete the zero intra arrival time\n",
    "    arrival_time=arrival_time[arrival_time>0]\n",
    "    mu_log=np.mean(np.log(arrival_time))\n",
    "    std_log=np.std(np.log(arrival_time))\n",
    "    data_log=np.random.lognormal(mu_log,std_log,arrival_time.shape)\n",
    "\n",
    "    mu_exp=np.mean(arrival_time)\n",
    "    data_exp=np.random.exponential(mu_exp,arrival_time.shape)\n",
    "\n",
    "    data_weibull=np.random.weibull(0.38,arrival_time.shape)\n",
    "    beta=np.var(arrival_time)/np.mean(arrival_time)\n",
    "    alpha=np.mean(arrival_time)/beta\n",
    "    data_gamma=np.random.gamma(alpha,beta,arrival_time.shape)\n",
    "    ecdf = sm.distributions.ECDF(arrival_time,)\n",
    "   \n",
    "  \n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"b\",label=\"Original data\")\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_log)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"g\",label=\"Lognormal Distribution\")\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_exp)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"y\",label=\"Exponential distribution\")\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_weibull)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"r\",label=\"Weibull distribution\")\n",
    "\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_t)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"purple\",label=\"Gamma distribution\")\n",
    "    \n",
    "    \n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlabel(\"Intra-arrival time\")\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_ylabel(\"Probability\")\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].legend(loc=\"lower right\")\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_title(\"Cumulative distribution function of \\n order arrival time  for stock \"+ticker_list[ticker_ind])\n",
    "\n",
    "plt.savefig('arrival_time.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) volume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.stats import lognorm  \n",
    "import seaborn as sns\n",
    "ticker_ind=0\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_AAPL.png\")\n",
    "plt.show()\n",
    "\n",
    "ticker_ind=1\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_AMZN.png\")\n",
    "plt.show()\n",
    "\n",
    "ticker_ind=3\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale)+1.2, shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_INTC.png\")\n",
    "plt.show()\n",
    "\n",
    "ticker_ind=4\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale)+1.2, shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_MSFT.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3) Intraday seasonality\n",
    "\n",
    "observe the volume during the whole day under 5 minutes time bins. show the result of seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticker_ind=0\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "data_mess_limit=data_mess[data_mess[:,1]==1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calute the volume of limit order book in each time interval\n",
    "\n",
    "time_interval=np.linspace(data_mess_limit[:,0].min(),data_mess_limit[:,0].max(),78)\n",
    "vol=0\n",
    "vol_time=[]\n",
    "j=1\n",
    "\n",
    "for i in range(len(data_mess_limit)):\n",
    "    if  data_mess_limit[i,0]<=time_interval[j]:\n",
    "        vol=vol+data_mess_limit[i,3]\n",
    "    else: \n",
    "        j=j+1\n",
    "        vol_time.append(vol)\n",
    "        vol=data_mess_limit[i,3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the quadratic fit and vol_time\n",
    "x=range(76)\n",
    "plt.plot(x,vol_time,label=ticker_list[ticker_ind])\n",
    "qua_fit=np.poly1d(np.polyfit(x, vol_time, 2))(x)\n",
    "plt.plot(x,qua_fit,label=ticker_list[ticker_ind]+\" quadratic fit\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "xticks=np.arange(34200,57600,2400)\n",
    "plt.xticks(x[::8],xticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for limit order \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "for ticker_ind in range(1,5):\n",
    "\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_mess_limit=data_mess[data_mess[:,1]==1,:]\n",
    "    # calute the volume of limit order book in each time interval\n",
    "\n",
    "    time_interval=np.linspace(data_mess_limit[:,0].min(),data_mess_limit[:,0].max(),78)\n",
    "    vol=0\n",
    "    vol_time=[]\n",
    "    j=1\n",
    "\n",
    "    for i in range(len(data_mess_limit)):\n",
    "        if  data_mess_limit[i,0]<=time_interval[j]:\n",
    "            vol=vol+data_mess_limit[i,3]\n",
    "        else: \n",
    "            j=j+1\n",
    "            vol_time.append(vol)\n",
    "            vol=data_mess_limit[i,3]\n",
    "    # plot the quadratic fit and vol_time\n",
    "    x=range(76)\n",
    "    plt.plot(x,vol_time,\"+-\",label=ticker_list[ticker_ind])\n",
    "    qua_fit=np.poly1d(np.polyfit(x, vol_time, 2))(x)\n",
    "    plt.plot(x,qua_fit,\"--\",label=ticker_list[ticker_ind]+\" quadratic fit\")\n",
    "    plt.legend(loc=\"upper center\")\n",
    "    xticks=np.arange(34200,57600,2400)\n",
    "    plt.xticks(x[::8],xticks)\n",
    "    plt.title(\"Number of limit orders in a 5-minute interval for \"+ticker_list[ticker_ind])\n",
    "    plt.xlabel(\"Time of day(seconds)\")\n",
    "    plt.ylabel(\"Number of limit orders submitted in $\\Delta_t=5$ minutes\")\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_limit_vol_time.png\",bbox_inches='tight')\n",
    "\n",
    "    plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# market order\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "for ticker_ind in range(1,5):\n",
    "\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_mess_market=data_mess[(data_mess[:,1]==4) | (data_mess[:,1]==5),:]\n",
    "    # calute the volume of limit order book in each time interval\n",
    "\n",
    "    time_interval=np.linspace(data_mess_market[:,0].min(),data_mess_market[:,0].max(),78)\n",
    "    vol=0\n",
    "    vol_time=[]\n",
    "    j=1\n",
    "\n",
    "    for i in range(len(data_mess_market)):\n",
    "        if  data_mess_market[i,0]<=time_interval[j]:\n",
    "            vol=vol+data_mess_market[i,3]\n",
    "        else: \n",
    "            j=j+1\n",
    "            vol_time.append(vol)\n",
    "            vol=data_mess_market[i,3]\n",
    "    # plot the quadratic fit and vol_time\n",
    "    x=range(76)\n",
    "    plt.plot(x,vol_time,\"+-\",label=ticker_list[ticker_ind])\n",
    "    qua_fit=np.poly1d(np.polyfit(x, vol_time, 2))(x)\n",
    "    plt.plot(x,qua_fit,\"--\",label=ticker_list[ticker_ind]+\" quadratic fit\")\n",
    "    plt.legend(loc=\"upper center\")\n",
    "    xticks=np.arange(34200,57600,2400)\n",
    "    plt.xticks(x[::8],xticks)\n",
    "    plt.title(\"Number of market orders in a 5-minute interval for \"+ticker_list[ticker_ind])\n",
    "    plt.xlabel(\"Time of day(seconds)\")\n",
    "    plt.ylabel(\"Number of market orders submitted in $\\Delta_t=5$ minutes\")\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_market_vol_time.png\",bbox_inches='tight')\n",
    "    plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) average shape of the order books\n",
    "find the total volume for all each price level and see the volume trend based on the price levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 4) average shape of the order books\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "ticker_ind=1\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "data_order=data_order_list[ticker_ind]\n",
    "data_order_limit_ask_vol=data_order[data_mess[:,1]==1,1:40:4]\n",
    "data_order_limit_bid_vol=data_order[data_mess[:,1]==1,3:40:4]\n",
    "\n",
    "vol_ask=np.sum(data_order_limit_ask_vol,axis=0)/np.mean(np.sum(data_order_limit_ask_vol,axis=0))\n",
    "vol_bid=np.sum(data_order_limit_bid_vol,axis=0)/np.mean(np.sum(data_order_limit_bid_vol,axis=0))\n",
    "plt.plot(list(range(-10,0)),vol_bid)\n",
    "plt.plot(list(range(1,11)),vol_ask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop the stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "marker_list=[\"s\",\"D\",\"^\",\"8\"]\n",
    "color_list=[\"g\",\"b\",\"r\",\"y\"]\n",
    "for ticker_ind in range(1,5):\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "    data_order_limit_ask_vol=data_order[:,1:40:4]\n",
    "    data_order_limit_bid_vol=data_order[:,3:40:4]\n",
    "\n",
    "    vol_ask=np.sum(data_order_limit_ask_vol,axis=0)/np.mean(np.sum(data_order_limit_ask_vol,axis=0))\n",
    "    vol_bid=np.sum(data_order_limit_bid_vol,axis=0)/np.mean(np.sum(data_order_limit_bid_vol,axis=0))\n",
    "    plt.plot(list(range(-10,0)),vol_bid,\n",
    "             \"--\",marker=marker_list[ticker_ind-1],color=color_list[ticker_ind-1],label=\n",
    "            ticker_list[ticker_ind])\n",
    "    plt.plot(list(range(1,11)),vol_ask,\"--\",marker=marker_list[ticker_ind-1],color=color_list[ticker_ind-1])\n",
    "plt.ylim([0.6,1.6])\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Average quantity offered in the market order book\")\n",
    "plt.xlabel(\"Price level of limit orders (negative axis : bids ; positive axis : asks)\")\n",
    "plt.ylabel(\"Average numbers of shares(Normalized by mean)\")\n",
    "plt.savefig(\"level_quantity.png\",bbox_inches='tight')    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) placement of orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticker_ind=2\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "data_order=data_order_list[ticker_ind]\n",
    "\n",
    "data_mess_limit=data_mess[data_mess[:,1]==1,:]\n",
    "data_order_limit=data_order[data_mess[:,1]==1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spread_list=[]\n",
    "for i in range(1,len(data_mess_limit)):\n",
    "    if data_mess_limit[i,5]==-1:\n",
    "        spread=data_mess_limit[i,4]-data_order_limit[i-1,0]\n",
    "    else:\n",
    "        spread=data_order_limit[i-1,2]-data_mess_limit[i,4]\n",
    "    spread_list.append(spread)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import math\n",
    "sns.kdeplot(np.array(spread_list), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "mu = 0\n",
    "variance = np.var(spread_list)\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(min(spread_list), max(spread_list), 100)\n",
    "plt.plot(x,mlab.normpdf(x, mu, sigma),\"r--\",label=\"Gaussian\")\n",
    "plt.xlim([-10000,10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ticker_ind in range(1,5):\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "\n",
    "    data_mess_limit=data_mess[data_mess[:,1]==1,:]\n",
    "    data_order_limit=data_order[data_mess[:,1]==1,:]\n",
    "\n",
    "    spread_list=[]\n",
    "    for i in range(1,len(data_mess_limit)):\n",
    "        if data_mess_limit[i,5]==-1:\n",
    "            spread=data_mess_limit[i,4]-data_order_limit[i-1,0]\n",
    "        else:\n",
    "            spread=data_order_limit[i-1,2]-data_mess_limit[i,4]\n",
    "        spread_list.append(spread)\n",
    "\n",
    "\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import matplotlib.mlab as mlab\n",
    "    import math\n",
    "    sns.kdeplot(np.array(spread_list), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "    mu = 0\n",
    "    variance = np.var(spread_list)\n",
    "    sigma = math.sqrt(variance)\n",
    "    x = np.linspace(min(spread_list), max(spread_list), 100)\n",
    "    plt.plot(x,mlab.normpdf(x, mu, sigma),\"r--\",label=\"Gaussian\")\n",
    "    plt.xlim([min(spread_list)*0.8,max(spread_list)*0.8])\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Placement of limit orders using the\\n same best quote reference for \"+ticker_list[ticker_ind])\n",
    "    plt.xlabel(\"Price diference\")\n",
    "    plt.ylabel(\"Probability density function\")\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_placement.png\",bbox_inches='tight')    \n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
