{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit order book\n",
    "\n",
    "author: Jian Wang\n",
    "\n",
    "time: 2016-02-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Model prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for importing the AAPL data is: 1.6742236614227295\n",
      "The shape of the order data is:  (400391, 40)  of message data is:  (400391, 6)\n",
      "Time for importing the AMZN data is: 1.0174715518951416\n",
      "The shape of the order data is:  (269748, 40)  of message data is:  (269748, 6)\n",
      "Time for importing the GOOG data is: 0.5780067443847656\n",
      "The shape of the order data is:  (147916, 40)  of message data is:  (147916, 6)\n",
      "Time for importing the INTC data is: 2.5299482345581055\n",
      "The shape of the order data is:  (624040, 40)  of message data is:  (624040, 6)\n",
      "Time for importing the MSFT data is: 2.8362107276916504\n",
      "The shape of the order data is:  (668765, 40)  of message data is:  (668765, 6)\n",
      "Check the original data:\n",
      "\n",
      "The first five sampe of AAPL is:  [[  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86100000e+06   2.00000000e+02   5.85100000e+06   5.00000000e+00\n",
      "    5.86890000e+06   3.00000000e+02   5.85010000e+06   8.90000000e+01\n",
      "    5.86950000e+06   5.00000000e+01   5.84970000e+06   5.00000000e+00\n",
      "    5.87000000e+06   1.00000000e+02   5.84930000e+06   3.00000000e+02\n",
      "    5.87100000e+06   1.00000000e+01   5.84650000e+06   3.00000000e+02\n",
      "    5.87390000e+06   1.00000000e+02   5.84530000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84380000e+06   2.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84270000e+06   3.00000000e+02]\n",
      " [  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85320000e+06   1.80000000e+01\n",
      "    5.86100000e+06   2.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86890000e+06   3.00000000e+02   5.85100000e+06   5.00000000e+00\n",
      "    5.86950000e+06   5.00000000e+01   5.85010000e+06   8.90000000e+01\n",
      "    5.87000000e+06   1.00000000e+02   5.84970000e+06   5.00000000e+00\n",
      "    5.87100000e+06   1.00000000e+01   5.84930000e+06   3.00000000e+02\n",
      "    5.87390000e+06   1.00000000e+02   5.84650000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84530000e+06   3.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84380000e+06   2.00000000e+02]\n",
      " [  5.85940000e+06   2.00000000e+02   5.85330000e+06   1.80000000e+01\n",
      "    5.85980000e+06   2.00000000e+02   5.85320000e+06   1.80000000e+01\n",
      "    5.86100000e+06   2.00000000e+02   5.85310000e+06   1.80000000e+01\n",
      "    5.86890000e+06   3.00000000e+02   5.85300000e+06   1.50000000e+02\n",
      "    5.86950000e+06   5.00000000e+01   5.85100000e+06   5.00000000e+00\n",
      "    5.87000000e+06   1.00000000e+02   5.85010000e+06   8.90000000e+01\n",
      "    5.87100000e+06   1.00000000e+01   5.84970000e+06   5.00000000e+00\n",
      "    5.87390000e+06   1.00000000e+02   5.84930000e+06   3.00000000e+02\n",
      "    5.87650000e+06   1.16000000e+03   5.84650000e+06   3.00000000e+02\n",
      "    5.87900000e+06   5.00000000e+02   5.84530000e+06   3.00000000e+02]]\n",
      "\n",
      "The first five sampe of AMZN is:  [[  2.23950000e+06   1.00000000e+02   2.23180000e+06   1.00000000e+02\n",
      "    2.23990000e+06   1.00000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24400000e+06   5.47000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24540000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.24890000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.26770000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29430000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02\n",
      "    2.29800000e+06   1.00000000e+02   2.18970000e+06   1.00000000e+02]\n",
      " [  2.23950000e+06   1.00000000e+02   2.23810000e+06   2.10000000e+01\n",
      "    2.23990000e+06   1.00000000e+02   2.23180000e+06   1.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24400000e+06   5.47000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24540000e+06   1.00000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24890000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.26770000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.29430000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29800000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02]\n",
      " [  2.23950000e+06   1.00000000e+02   2.23810000e+06   2.10000000e+01\n",
      "    2.23960000e+06   2.00000000e+01   2.23180000e+06   1.00000000e+02\n",
      "    2.23990000e+06   1.00000000e+02   2.23070000e+06   2.00000000e+02\n",
      "    2.24000000e+06   2.20000000e+02   2.23040000e+06   1.00000000e+02\n",
      "    2.24250000e+06   1.00000000e+02   2.23000000e+06   1.00000000e+01\n",
      "    2.24400000e+06   5.47000000e+02   2.22620000e+06   1.00000000e+02\n",
      "    2.24540000e+06   1.00000000e+02   2.21300000e+06   4.00000000e+03\n",
      "    2.24890000e+06   1.00000000e+02   2.20400000e+06   1.00000000e+02\n",
      "    2.26770000e+06   1.00000000e+02   2.20250000e+06   5.00000000e+03\n",
      "    2.29430000e+06   1.00000000e+02   2.20200000e+06   1.00000000e+02]]\n",
      "\n",
      "The first five sampe of GOOG is:  [[  5.80230000e+06   1.00000000e+02   5.79400000e+06   4.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]\n",
      " [  5.80230000e+06   1.00000000e+02   5.79400000e+06   1.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]\n",
      " [  5.80230000e+06   1.00000000e+02   5.79400000e+06   1.96000000e+02\n",
      "    5.80430000e+06   1.00000000e+02   5.78700000e+06   4.00000000e+02\n",
      "    5.80500000e+06   1.00000000e+02   5.78500000e+06   5.00000000e+02\n",
      "    5.80630000e+06   1.00000000e+02   5.78000000e+06   5.00000000e+02\n",
      "    5.80670000e+06   1.00000000e+02   5.77180000e+06   1.00000000e+02\n",
      "    5.80960000e+06   5.00000000e+01   5.76940000e+06   1.00000000e+02\n",
      "    5.80970000e+06   1.00000000e+02   5.76600000e+06   1.00000000e+02\n",
      "    5.83500000e+06   1.00000000e+02   5.76260000e+06   1.00000000e+02\n",
      "    5.88000000e+06   1.00000000e+02   5.73200000e+06   2.00000000e+01\n",
      "    5.89260000e+06   1.00000000e+02   5.70000000e+06   1.00000000e+02]]\n",
      "\n",
      "The first five sampe of INTC is:  [[  2.75200000e+05   6.60000000e+01   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74600000e+05   7.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74500000e+05   9.00000000e+02\n",
      "    2.76100000e+05   2.30000000e+03   2.74400000e+05   2.80000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76300000e+05   2.00000000e+03   2.74200000e+05   4.06300000e+03]\n",
      " [  2.75200000e+05   1.66000000e+02   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74600000e+05   7.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74500000e+05   9.00000000e+02\n",
      "    2.76100000e+05   2.30000000e+03   2.74400000e+05   2.80000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76300000e+05   2.00000000e+03   2.74200000e+05   4.06300000e+03]\n",
      " [  2.75200000e+05   1.66000000e+02   2.75100000e+05   4.00000000e+02\n",
      "    2.75300000e+05   1.00000000e+03   2.75000000e+05   1.00000000e+02\n",
      "    2.75400000e+05   3.73000000e+02   2.74900000e+05   2.00000000e+02\n",
      "    2.75500000e+05   1.00000000e+02   2.74800000e+05   6.61000000e+02\n",
      "    2.75600000e+05   1.00000000e+02   2.74700000e+05   3.00000000e+02\n",
      "    2.75700000e+05   1.00000000e+02   2.74600000e+05   7.00000000e+02\n",
      "    2.75900000e+05   8.58900000e+03   2.74500000e+05   9.00000000e+02\n",
      "    2.76000000e+05   9.59000000e+02   2.74400000e+05   2.80000000e+03\n",
      "    2.76100000e+05   2.30000000e+03   2.74300000e+05   3.30000000e+03\n",
      "    2.76200000e+05   2.70000000e+03   2.74200000e+05   4.06300000e+03]]\n",
      "\n",
      "The first five sampe of MSFT is:  [[  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10500000e+05   1.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10600000e+05   1.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08800000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08500000e+05   4.00000000e+02\n",
      "    3.11400000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]\n",
      " [  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10500000e+05   2.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10600000e+05   1.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08800000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08500000e+05   4.00000000e+02\n",
      "    3.11400000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]\n",
      " [  3.09900000e+05   3.78800000e+03   3.09500000e+05   3.00000000e+02\n",
      "    3.10400000e+05   1.00000000e+02   3.09300000e+05   3.98600000e+03\n",
      "    3.10500000e+05   2.00000000e+02   3.09200000e+05   1.00000000e+02\n",
      "    3.10600000e+05   1.00000000e+02   3.09100000e+05   3.00000000e+02\n",
      "    3.10700000e+05   2.00000000e+02   3.08900000e+05   1.00000000e+02\n",
      "    3.10800000e+05   2.00000000e+02   3.08800000e+05   2.00000000e+02\n",
      "    3.10900000e+05   9.34800000e+03   3.08700000e+05   2.00000000e+02\n",
      "    3.11000000e+05   1.80000000e+03   3.08600000e+05   4.00000000e+02\n",
      "    3.11100000e+05   4.50000000e+03   3.08500000e+05   4.00000000e+02\n",
      "    3.11300000e+05   1.00000000e+02   3.08400000e+05   1.60000000e+03]]\n",
      "65.30752658843994\n",
      "The shape of the total response is:\n",
      "\n",
      "(400236, 1)\n",
      "(269571, 1)\n",
      "(147766, 1)\n",
      "(622641, 1)\n",
      "(667701, 1)\n",
      "The shape of the reduced response is:\n",
      "\n",
      "(309538, 1)\n",
      "(218710, 1)\n",
      "(118877, 1)\n",
      "(458160, 1)\n",
      "(511299, 1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Aug 26 00:03:47 2016\n",
    "\n",
    "@author: jianwang\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set default parameters\n",
    "ticker_list=[\"AAPL\",\"AMZN\",\"GOOG\",\"INTC\",\"MSFT\"]\n",
    "start_ind=10*3600\n",
    "end_ind=15.5*3600\n",
    "data_order_list=[]\n",
    "data_mess_list=[]\n",
    "time_index_list=[]\n",
    "path_save='/media/jianwang/Study/Research/order_book/'\n",
    "path_load=\"/media/jianwang/Study/Research/order_book/\"\n",
    "\n",
    "## set random seed to produce the same results\n",
    "\n",
    "np.random.seed(987612345)\n",
    "\n",
    "#read the stock ticker\n",
    "#totally 5 dataset\n",
    "\n",
    "for i in range(len(ticker_list)):\n",
    "    #get the path for the csv files\n",
    "    # name_order is for the order book and name_mess for the message book\n",
    "    name_order='_2012-06-21_34200000_57600000_orderbook_10.csv'\n",
    "    name_mess='_2012-06-21_34200000_57600000_message_10.csv'\n",
    "    # calculate the cputime for reading the data\n",
    "    t=time.time()\n",
    "    # header =-1 means that the first line is not the header, otherwise, the first line will be header\n",
    "    # data_order is for order book and data mess is for message book\n",
    "    data_order_list.append(np.array(pd.read_csv(path_load+ticker_list[i]+name_order,header=-1),dtype=\"float64\"))\n",
    "    data_mess_list.append(np.array(pd.read_csv(path_load+ticker_list[i]+name_mess,header=-1),dtype=\"float64\"))\n",
    "    print(\"Time for importing the \"+ticker_list[i]+\" data is:\",time.time()-t)\n",
    "    print(\"The shape of the order data is: \",data_order_list[i].shape, \" of message data is: \", data_mess_list[i].shape)\n",
    "    # get the time index\n",
    "    time_index_list.append(data_mess＿list[i][:,0])\n",
    "\n",
    "\n",
    "#print the sample of data\n",
    "print(\"Check the original data:\")\n",
    "\n",
    "for i in range(len(ticker_list)):\n",
    "    print()\n",
    "    print(\"The first five sampe of \"+ticker_list[i]+\" is: \",data_order_list[i][:3])\n",
    "\n",
    "    # -*- coding: utf-8 -*-\n",
    "\n",
    "# # save the feature array\n",
    "# ##get the original order,message and time index data, header =-1 means that did not\n",
    "# ##read the first column as the name\n",
    "#%%\n",
    "# # use a loop to read data\n",
    "# for ticker_ind in range(len(ticker_list)):\n",
    "#     data_order=data_order_list[ticker_ind]\n",
    "#     data_mess=data_mess_list[ticker_ind]\n",
    "#     time_index=data_mess[:,0]\n",
    "#     # obtain the reduced order message and time_index dataset, half an hour after the\n",
    "#     # 9:30 and half an hour before 16:00\n",
    "#     # data_reduced is used to install the data from 10 to 15:30, take half hour for auction\n",
    "#     data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "#     data_mess_reduced=data_mess[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "#     time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "\n",
    "#     test_lower=0\n",
    "#     # test up is the up index of the original data to construct the test data\n",
    "#     test_upper=len(data_order_reduced)\n",
    "#     # data_test is the subset of data_reduced from the lower index to upper index\n",
    "#     data_order_test=data_order_reduced[test_lower:test_upper,:]\n",
    "#     data_mess_test=data_mess_reduced[test_lower:test_upper,:]\n",
    "#     t=time.time()\n",
    "#     feature_array=get_features (data_order, data_mess,data_order_test,data_mess_test)\n",
    "#     np.savetxt(path_save+ticker_list[ticker_ind]+'_feature_array.txt',feature_array,delimiter=' ')\n",
    "#     print (\"Time for building \"+ticker_list[ticker_ind]+\" is:\",time.time()-t)\n",
    "\n",
    "\n",
    "# load the feature\n",
    "#%%\n",
    "import time\n",
    "t=time.time()\n",
    "feature_array_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    feature_array_list.append(np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_feature_array.txt',\\\n",
    "                                                   sep=' ',header=-1)))\n",
    "print(time.time()-t)\n",
    "\n",
    "# this function used to build the y\n",
    "# ask_low as 1 bad high as -1 and no arbitrage as 0\n",
    "# option=1 return ask low, option =2 return bid high, option =3 return no arbi, option =4 return total(ask_low=1,\n",
    "# bid_high =-1 and no arbi =0)\n",
    "#%%\n",
    "def build_y(ask_low,bid_high,no_arbi,option):\n",
    "    if (option==1):\n",
    "        return ask_low\n",
    "    elif option==2:\n",
    "        return bid_high\n",
    "    elif option==3:\n",
    "        return no_arbi\n",
    "    elif option==4:\n",
    "        return ask_low-bid_high\n",
    "    else:\n",
    "        print(\"option should be 1,2,3,4\")\n",
    "\n",
    "## save y data\n",
    "#%%\n",
    "#time_ind=1\n",
    "#option_ind=1\n",
    "#for ticker_ind in range(len(ticker_list)):\n",
    "#    response=build_y(ask_low_time_list[ticker_ind][time_ind],bid_high_time_list[ticker_ind][time_ind],\\\n",
    "#                                 no_arbi_time_list[ticker_ind][time_ind],option=option_ind)\n",
    "#    np.savetxt(path_save+ticker_list[ticker_ind]+'_response.txt',response)\n",
    "\n",
    "\n",
    "\n",
    "## load y data\n",
    "#%%\n",
    "response_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    response_list.append((np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_response.txt',header=-1))))\n",
    "\n",
    "\n",
    "## print the shape of the response\n",
    "## note it is the total response\n",
    "#%%\n",
    "print(\"The shape of the total response is:\\n\")\n",
    "\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_list[ticker_ind].shape)\n",
    "\n",
    "# need to get the response from 10 to 15:30\n",
    "# the shape of the response and the feature array should be equal\n",
    "response_reduced_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    first_ind = np.where(time_index_list[ticker_ind]>=start_ind)[0][0]\n",
    "    last_ind=np.where(time_index_list[ticker_ind]<=end_ind)[0][-1]\n",
    "    response_reduced_list.append(response_list[ticker_ind][first_ind:last_ind+1])\n",
    "\n",
    "print(\"The shape of the reduced response is:\\n\")\n",
    "\n",
    "## print the shape of reduced response\n",
    "## response reduced is used for testing and training the model\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_reduced_list[ticker_ind].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.train and test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Random split\n",
    "#%%---------------------------------------------------------------------\n",
    "import random\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "ticker_ind=1\n",
    "size=100000\n",
    "\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x, test_x, train_y, test_y =train_test_split(\\\n",
    "total_array[:,:134],total_array[:,134], test_size=0.1, random_state=42)\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random generate a given \n",
    "def random_choice(num, key):\n",
    "    temp=np.random.choice(num,size=key,replace=False)\n",
    "    temp_sort=sorted(temp)\n",
    "    for i in range(len(temp)):\n",
    "        num[temp_sort[i]]=temp[i]\n",
    "    \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total array shape: (400000, 135)\n",
      "train_x shape: (360000, 134)\n",
      "test_x shape: (40000, 134)\n",
      "test_y shape: (40000,)\n",
      "train_y shape: (360000,)\n"
     ]
    }
   ],
   "source": [
    "#time series split\n",
    "#%%--------------------------------------------------------------------------------------------\n",
    "\n",
    "ticker_ind=3\n",
    "size=400000\n",
    "random_ratio=0.1\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "total_array=total_array[random_choice(list(range(size)),int(size*random_ratio)),:]\n",
    "\n",
    "\n",
    "train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x=total_array[:train_num_index,:134]\n",
    "test_x=total_array[train_num_index:,:134]\n",
    "train_y=total_array[:train_num_index,134]\n",
    "test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "print(\"train_x shape:\",train_x.shape)\n",
    "print(\"test_x shape:\",test_x.shape)\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.40e-13   8.10e-15   9.01e-13  -3.84e-15   1.40e-13  -3.42e-14\n",
      "   9.01e-13   4.14e-14   1.40e-13  -2.44e-14   9.01e-13  -3.86e-14\n",
      "   1.40e-13   5.75e-14   9.01e-13   1.17e-14   1.40e-13   2.48e-15\n",
      "   9.01e-13  -5.82e-14   1.40e-13   6.13e-15   9.01e-13   5.27e-14\n",
      "   1.40e-13   5.47e-15   9.01e-13  -1.96e-14   1.40e-13  -2.06e-14\n",
      "   9.01e-13   4.94e-14   1.40e-13  -3.07e-14   9.01e-13   2.19e-14\n",
      "   1.40e-13  -2.52e-14   9.01e-13   1.05e-13  -1.28e-14  -1.28e-14\n",
      "  -1.28e-14  -1.28e-14  -1.28e-14  -1.28e-14  -1.28e-14  -1.28e-14\n",
      "  -1.28e-14  -1.28e-14   1.83e-13   1.83e-13   1.83e-13   1.83e-13\n",
      "   1.83e-13   1.83e-13   1.83e-13   1.83e-13   1.83e-13   1.83e-13\n",
      "   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00\n",
      "   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00\n",
      "   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00\n",
      "   1.40e-13   9.01e-13  -6.63e-13   1.92e-13  -5.20e-14  -5.47e-15\n",
      "  -1.21e-14  -1.21e-14  -1.21e-14  -1.21e-14  -1.21e-14  -1.21e-14\n",
      "  -1.21e-14  -1.21e-14  -1.21e-14  -1.21e-14  -9.39e-14  -9.39e-14\n",
      "  -9.39e-14  -9.39e-14  -9.39e-14  -9.39e-14  -9.39e-14  -9.39e-14\n",
      "  -9.39e-14  -9.39e-14  -5.58e-14   2.88e-16  -3.49e-14   5.48e-15\n",
      "  -4.56e-14   1.48e-14  -7.08e-14  -3.74e-14   3.49e-14  -4.41e-14\n",
      "   2.13e-14  -2.59e-14  -3.81e-14   5.09e-14   1.23e-14  -2.44e-14\n",
      "  -3.60e-14  -2.02e-14   3.86e-15   4.82e-14   1.92e-14  -1.51e-14\n",
      "  -1.66e-14  -3.51e-14  -5.34e-14   1.33e-14  -5.37e-15   8.26e-16\n",
      "   6.30e-14   1.67e-14]\n",
      "[ -1.39e+00   1.97e-01  -1.39e+00   6.69e-01  -1.39e+00   3.77e-01\n",
      "  -1.39e+00   6.84e-01  -1.39e+00   5.65e-01  -1.39e+00   3.76e-01\n",
      "  -1.39e+00   3.72e-01  -1.39e+00   4.44e-01  -1.39e+00   2.75e-01\n",
      "  -1.39e+00   3.07e-01  -1.39e+00   3.82e-01  -1.39e+00   4.26e-01\n",
      "  -1.39e+00   1.54e-01  -1.39e+00   5.11e-01  -1.39e+00   7.45e-02\n",
      "  -1.39e+00   8.33e-01  -1.39e+00   9.32e-02  -1.39e+00   9.59e-01\n",
      "  -1.39e+00  -1.97e-02  -1.39e+00   9.48e-01   7.43e-02   7.43e-02\n",
      "   7.43e-02   7.43e-02   7.43e-02   7.43e-02   7.43e-02   7.43e-02\n",
      "   7.43e-02   7.43e-02  -1.39e+00  -1.39e+00  -1.39e+00  -1.39e+00\n",
      "  -1.39e+00  -1.39e+00  -1.39e+00  -1.39e+00  -1.39e+00  -1.39e+00\n",
      "   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00\n",
      "   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00\n",
      "   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00\n",
      "  -1.39e+00  -1.39e+00   4.94e-01   1.23e+00   7.43e-02  -7.86e-01\n",
      "   1.07e-02   1.07e-02   1.07e-02   1.07e-02   1.07e-02   1.07e-02\n",
      "   1.07e-02   1.07e-02   1.07e-02   1.07e-02  -1.18e-02  -1.18e-02\n",
      "  -1.18e-02  -1.18e-02  -1.18e-02  -1.18e-02  -1.18e-02  -1.18e-02\n",
      "  -1.18e-02  -1.18e-02   4.32e-02   1.22e-03  -6.26e-02   9.94e-02\n",
      "   1.74e-03   4.86e-02  -2.53e-03  -1.08e-02   1.91e-02  -1.17e-02\n",
      "  -2.03e-02   9.99e-03   3.05e-02   5.06e-02  -5.65e-02  -7.16e-02\n",
      "   2.69e-02   4.55e-03  -4.94e-03   7.57e-05  -2.23e-02   2.23e-02\n",
      "  -3.02e-02   1.09e-01  -2.34e-02   3.75e-02  -1.38e-02  -4.69e-03\n",
      "  -6.82e-02   2.57e-02]\n"
     ]
    }
   ],
   "source": [
    "# scale data\n",
    "#%%\n",
    "\n",
    "# can use the processing.scale function to scale the data\n",
    "from sklearn import preprocessing\n",
    "# note that we need to transfer the data type to float\n",
    "# remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "# use scale for zero mean and one std\n",
    "scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "train_x_scale=scaler.transform(train_x)\n",
    "test_x_scale=scaler.transform(test_x)\n",
    "\n",
    "print(np.mean(train_x_scale,0))\n",
    "print(np.mean(test_x_scale,0))\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Model build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x_scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------\n",
    "# logistic l1\n",
    "#-----------------\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "        \n",
    "        # set the random state to make sure that each time get the same results\n",
    "\n",
    "time_logistic=time.time()\n",
    "clf = linear_model.LogisticRegression(C=1, penalty='l1', tol=1e-6,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "time_logistic=time.time()-time_logistic    \n",
    "\n",
    "print(time_logistic)\n",
    "\n",
    "# test the training error\n",
    "predict_y_logistic =np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y_logistic==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y_logistic,train_y)\n",
    "recall = recall_score(predict_y_logistic,train_y)\n",
    "f1=f1_score(predict_y_logistic,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()\n",
    "\n",
    "#----------------\n",
    "# logistic l2\n",
    "#-----------------\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "        \n",
    "        # set the random state to make sure that each time get the same results\n",
    "\n",
    "time_logistic=time.time()\n",
    "clf = linear_model.LogisticRegression(C=1, penalty='l2', tol=1e-6,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "time_logistic=time.time()-time_logistic    \n",
    "\n",
    "print(time_logistic)\n",
    "\n",
    "# test the training error\n",
    "predict_y_logistic =np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y_logistic==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y_logistic,train_y)\n",
    "recall = recall_score(predict_y_logistic,train_y)\n",
    "f1=f1_score(predict_y_logistic,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------------------\n",
    "# SVM_poly_2\n",
    "#---------------------\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "\n",
    "import time \n",
    "from sklearn import svm\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf = svm.SVC(C=1.0,kernel='poly',degree=2,max_iter=5000,shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y =np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "#draw the crosstab chart\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#---------------\n",
    "# decision tree\n",
    "#-----------------\n",
    "\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  tree.DecisionTreeClassifier(max_depth=10,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y=np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "print(\"test time is:\", time.time()-t)\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "#draw the crosstab chart\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-59e42b63bcfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mtime_ada\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m987612345\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_scale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime_ada\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                 sample_weight)\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    460\u001b[0m         \"\"\"\n\u001b[0;32m    461\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jianwang/anaconda3/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    366\u001b[0m                                            max_leaf_nodes)\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#-----------------------------------------\n",
    "# Adaboost \n",
    "#-----------------------------------------\n",
    "\n",
    "# set the sample weights for the training model\n",
    "sample_weights=[]\n",
    "ratio=len(train_y)/sum(train_y==1)/10\n",
    "for i in range(len(train_x)):\n",
    "    if train_y[i]==0:\n",
    "        sample_weights.append(1)\n",
    "    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "time_ada=time.time()\n",
    "clf =  AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10),n_estimators=100,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-time_ada)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y=np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "#draw the crosstab chart\n",
    "%matplotlib inline\n",
    "## draw chart for the cross table\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.95393657684326\n",
      "train_accuracy is: 0.999736111111\n",
      "precision is: \t 0.984625668449\n",
      "recall is: \t 0.999491094148\n",
      "f1 score is: \t 0.992002693829\n",
      "test time is: 0.37116289138793945\n",
      "test accuracy is: 0.99425\n",
      "precision is: \t 0.844594594595\n",
      "recall is: \t 1.0\n",
      "f1 score is: \t 0.915750915751\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "time_rf=time.time()\n",
    "clf =  RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-time_rf)\n",
    "\n",
    "#testing\n",
    "# test the training error\n",
    "predict_y=np.array(clf.predict(train_x_scale))\n",
    "print(\"train_accuracy is:\",sum(predict_y==train_y)/len(train_y))\n",
    "\n",
    "# test the score for the train data\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "precision= precision_score(predict_y,train_y)\n",
    "recall = recall_score(predict_y,train_y)\n",
    "f1=f1_score(predict_y,train_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test_proba =np.array(clf.predict_proba(test_x_scale))\n",
    "print(\"test time is:\", time.time()-t)\n",
    "predict_y_test=predict_threshold(predict_y_test_proba,0.5)\n",
    "\n",
    "\n",
    "# test the score for the test data\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score)\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "precision= precision_score(predict_y_test,test_y)\n",
    "recall = recall_score(predict_y_test,test_y)\n",
    "f1=f1_score(predict_y_test,test_y)\n",
    "print(\"precision is: \\t %s\" % precision)\n",
    "print(\"recall is: \\t %s\" % recall)\n",
    "print(\"f1 score is: \\t %s\" %f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[38715     0]\n",
      " [   24  1261]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVsAAAEpCAYAAAA9JYEnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMyuAk62ZWQGcbM3MCuBka2ZWgP8Pzr+ZgENOoB4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1dbcdb940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## confusion matrix plot\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [0,1])\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## see the feature_importances\n",
    "\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_x_scale[:100,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multi-class predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## load the arbitrage time txt data\n",
    "\n",
    "ask_low_time_list=[]\n",
    "bid_high_time_list=[]\n",
    "no_arbi_time_list=[] \n",
    "time_list=[1,5,10,15,20]\n",
    "import time \n",
    "t=time.time()\n",
    "for ticker_ind in range(5):  \n",
    "    ask_low_time_list.append([])\n",
    "    bid_high_time_list.append([])\n",
    "    no_arbi_time_list.append([])\n",
    "    for time_ind in range(len(time_list)):\n",
    "        ask_low_time_list[ticker_ind].append(\n",
    "            np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_ask_low_time_'+str(time_list[time_ind])+'.txt',header=-1)))\n",
    "        bid_high_time_list[ticker_ind].append(\n",
    "            np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_bid_high_time_'+str(time_list[time_ind])+'.txt',header=-1)))\n",
    "        no_arbi_time_list[ticker_ind].append(\n",
    "            np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_no_arbi_time_'+str(time_list[time_ind])+'.txt',header=-1)))\n",
    "        \n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deal with the data\n",
    "def build_y(ask_low,bid_high,no_arbi,option):\n",
    "    if (option==1):\n",
    "        return ask_low\n",
    "    elif option==2:\n",
    "        return bid_high\n",
    "    elif option==3:\n",
    "        return no_arbi\n",
    "    elif option==4:\n",
    "        return ask_low-bid_high\n",
    "    else:\n",
    "        print(\"option should be 1,2,3,4\")\n",
    "        \n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    response=build_y(ask_low_time_list[ticker_ind][1],bid_high_time_list[ticker_ind][1],\\\n",
    "                                 no_arbi_time_list[ticker_ind][1],option=4)\n",
    "    np.savetxt(path_save+ticker_list[ticker_ind]+'_multiresponse.txt',response)\n",
    "\n",
    "response_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    response_list.append((np.array(pd.read_csv(path_save+ticker_list[ticker_ind]+'_multiresponse.txt',header=-1))))\n",
    "\n",
    "    ## print the shape of the response\n",
    "## note it is the total response\n",
    "print(\"The shape of the total response is:\\n\")\n",
    "\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_list[ticker_ind].shape)\n",
    "    \n",
    "# need to get the response from 10 to 15:30\n",
    "# the shape of the response and the feature array should be equal \n",
    "response_reduced_list=[]\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    first_ind = np.where(time_index_list[ticker_ind]>=start_ind)[0][0]\n",
    "    last_ind=np.where(time_index_list[ticker_ind]<=end_ind)[0][-1]\n",
    "    response_reduced_list.append(response_list[ticker_ind][first_ind:last_ind+1])\n",
    "    \n",
    "print(\"The shape of the reduced response is:\\n\")\n",
    "\n",
    "## print the shape of reduced response\n",
    "## response reduced is used for testing and training the model\n",
    "for ticker_ind in range(len(ticker_list)):\n",
    "    print(response_reduced_list[ticker_ind].shape)\n",
    "    # random split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random split\n",
    "#split the data to train and test data set\n",
    "import random\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "ticker_ind=1\n",
    "size=100000\n",
    "\n",
    "# combine the feature and response array to random sample\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind]),axis=1)[:size,:]\n",
    "\n",
    "\n",
    "print(\"total shape:\",total_array.shape)\n",
    "\n",
    "train_x, test_x, train_y, test_y =train_test_split(\\\n",
    "total_array[:,:134],total_array[:,134], test_size=0.1, random_state=42)\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "\n",
    "print(\"test shape:\",test_y.shape)\n",
    "print(\"train shape:\",train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#time series split\n",
    "#%%--------------------------------------------------------------------------------------------\n",
    "\n",
    "ticker_ind=1\n",
    "size =100000\n",
    "time_index=time_index_list[ticker_ind]\n",
    "# combine the feature and response array to random sample\n",
    "time_index_reduced=time_index[(time_index>=start_ind)&(time_index<=end_ind)]\n",
    "total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                            time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)[:size,:]\n",
    "\n",
    "total_array=total_array[np.random.randint(len(total_array),size=len(total_array)),:]\n",
    "\n",
    "train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "#split the data to train and test data set\n",
    "train_x=total_array[:train_num_index,:134]\n",
    "test_x=total_array[train_num_index:,:134]\n",
    "train_y=total_array[:train_num_index,134]\n",
    "test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "# the y data need to reshape to size (n,) not (n,1)\n",
    "test_y=test_y.reshape(len(test_y),)\n",
    "train_y=train_y.reshape(len(train_y),)\n",
    "print(\"train_x shape:\",train_x.shape)\n",
    "print(\"test_x shape:\",test_x.shape)\n",
    "print(\"test_y shape:\",test_y.shape)\n",
    "print(\"train_y shape:\",train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scale the data\n",
    "# can use the processing.scale function to scale the data\n",
    "from sklearn import preprocessing\n",
    "# note that we need to transfer the data type to float\n",
    "# remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "# use scale for zero mean and one std\n",
    "scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "train_x_scale=scaler.transform(train_x)\n",
    "test_x_scale=scaler.transform(test_x)\n",
    "\n",
    "print(np.mean(train_x_scale,0))\n",
    "print(np.mean(test_x_scale,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one vs one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only run for random forest method\n",
    "# one vs one case\n",
    "# random forest\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "## sample weights\n",
    "#sample_weights=[]\n",
    "#ratio=len(train_y)/sum(train_y==1)/10\n",
    "#for i in range(len(train_x)):\n",
    "#    if train_y[i]==0:\n",
    "#        sample_weights.append(1)\n",
    "#    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  OneVsOneClassifier(RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345))\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "%matplotlib inline\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_one.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only run for random forest method\n",
    "# one vs one case\n",
    "# adaboosting\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "## sample weights\n",
    "#sample_weights=[]\n",
    "#ratio=len(train_y)/sum(train_y==1)/10\n",
    "#for i in range(len(train_x)):\n",
    "#    if train_y[i]==0:\n",
    "#        sample_weights.append(1)\n",
    "#    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10),n_estimators=100,random_state= 987612345)\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_one.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------\n",
    "# one vs one case\n",
    "# svm\n",
    "#-------------------\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "\n",
    "\n",
    "## sample weights\n",
    "#sample_weights=[]\n",
    "#ratio=len(train_y)/sum(train_y==1)/10\n",
    "#for i in range(len(train_x)):\n",
    "#    if train_y[i]==0:\n",
    "#        sample_weights.append(1)\n",
    "#    else: sample_weights.append(ratio)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  OneVsOneClassifier(svm.SVC(C=1.0,kernel='poly',degree=2,max_iter=5000,shrinking=True, tol=0.001, verbose=False)\n",
    ")\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_one.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# only run for random forest method\n",
    "# one vs rest case\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "t=time.time()\n",
    "clf =  OneVsRestClassifier(RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345))\n",
    "clf.fit(train_x_scale,train_y)\n",
    "\n",
    "print(time.time()-t)\n",
    "\n",
    "predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "# define a function to prefict the result by threshold\n",
    "# note: logistic model will return two probability\n",
    "def predict_threshold(predict_proba, threshold):\n",
    "    res=[]\n",
    "    for i in range(len(predict_proba)):\n",
    "        res.append(int(predict_proba[i][1]>threshold))\n",
    "    return res\n",
    "\n",
    "t=time.time()\n",
    "predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "print(\"test time is :\",time.time()-t)\n",
    "print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "# # test the score for the train data\n",
    "# from sklearn.metrics import (precision_score, recall_score,\n",
    "#                              f1_score)\n",
    "# print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "# precision= precision_score(predict_y_test,test_y)\n",
    "# recall = recall_score(predict_y_test,test_y)\n",
    "# f1=f1_score(predict_y_test,test_y)\n",
    "# print(\"precision is: \\t %s\" % precision)\n",
    "# print(\"recall is: \\t %s\" % recall)\n",
    "# print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "# #draw the crosstab chart\n",
    "# %matplotlib inline\n",
    "# ## draw chart for the cross table\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [-1,0,1])\n",
    "    plt.yticks(tick_marks, [-1,0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, predict_y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "plt.savefig(\"one_vs_rest.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.P&L calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_index(index, value):\n",
    "    i=0\n",
    "    while index[i] <value:\n",
    "        i=i+1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## for AMZN\n",
    "ticker_ind =1\n",
    "train_ratio=0.9\n",
    "time_index=data_mess[:,0]\n",
    "data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "total_array_old=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)\n",
    "data_order=data_order_list[ticker_ind]\n",
    "data_mess=data_mess_list[ticker_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_order_test=data_order_reduced[int(size*train_ratio):size,:]\n",
    "time_index_test=time_index_reduced[int(size*train_ratio):size]\n",
    "\n",
    "test_y_unrandom=total_array_old[int(size*train_ratio):size,134]\n",
    "print(data_order_test.shape)\n",
    "print(time_index_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_bid_high_choose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(time_index_test[:10000],data_order_test[:10000,0],\"r-\",label=\"Ask price\")\n",
    "plt.plot(time_index_test[:10000],data_order_test[:10000,2],\"b-\",label=\"Bid price\")\n",
    "\n",
    "x_ask_low_choose=time_index_test[test_y_unrandom==1]\n",
    "y_ask_low_choose=data_order_test[test_y_unrandom==1,0]\n",
    "x_bid_high_choose=time_index_test[test_y_unrandom==-1]\n",
    "y_bid_high_choose=data_order_test[test_y_unrandom==-1,2]\n",
    "\n",
    "plt.plot(x_ask_low_choose[:30],y_ask_low_choose[:30],\"gv\",markersize=8,label=\"Ask low\")\n",
    "plt.plot(x_bid_high_choose[:30],y_bid_high_choose[:30],\"r^\",markersize=8,label=\"Bid high\")\n",
    "plt.xlabel(\"Time(s)\")\n",
    "plt.ylabel(\"Price($10^{-4}$\\$)\")\n",
    "plt.legend(bbox_to_anchor=[1.4, 1])\n",
    "plt.title(\"Arbitrage opportunities for \"+ticker_list[ticker_ind]+\"(5s)\")\n",
    "plt.savefig(\"arbitrage_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_index_test=total_array[:,135][int(size*train_ratio):size]\n",
    "# find the arbitrage occuring index\n",
    "arbi_index=list(np.where(predict_y_test!=0)[0])\n",
    "# find the index that 5 seconds later\n",
    "arbi_future_index=[]\n",
    "for i in arbi_index:\n",
    "    arbi_future_index.append(get_index(time_index_reduced,time_index_test[i]+5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arbi_future_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_array_test=total_array[int(size*train_ratio):size,:]\n",
    "future_price=[]\n",
    "current_price=[]\n",
    "pnl=[]\n",
    "for i in range(len(arbi_index)):\n",
    "    #ask low\n",
    "    if predict_y_test[arbi_index[i]]==1 :\n",
    "        future_price=data_order_reduced[arbi_future_index[i],0]\n",
    "        current_price=total_array_test[arbi_index[i],2]\n",
    "        pnl.append(current_price-future_price)\n",
    "    # bid high\n",
    "    else: \n",
    "        future_price=data_order_reduced[arbi_future_index[i],2]\n",
    "        current_price=total_array_test[arbi_index[i],0]\n",
    "        pnl.append(future_price-current_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pnl=np.array(pnl)\n",
    "predict_arbi=predict_y_test[predict_y_test!=0]\n",
    "plt.plot(pnl[predict_arbi==1],\"b.\",label=\"Ask low PnL\")\n",
    "plt.plot(pnl[predict_arbi==-1],\"r.\",label=\"Bid High PnL\")\n",
    "\n",
    "plt.xlabel(\"Arbitrage Index\")\n",
    "plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "plt.title(\"PnL for \"+ticker_list[ticker_ind])\n",
    "plt.legend()\n",
    "plt.savefig(ticker_list[ticker_ind]+\"_pnl.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cum_pnl=np.cumsum(pnl)\n",
    "plt.plot(cum_pnl,\"b.\",label=\"Cumulative P&L\")\n",
    "plt.xlabel(\"Arbitrage Index\")\n",
    "plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "plt.title(\"Cumulative PnL for \"+ticker_list[ticker_ind])\n",
    "plt.legend()\n",
    "plt.savefig(ticker_list[ticker_ind]+\"_cum_pnl.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop for all stock to plot the pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#time series split\n",
    "#%%--------------------------------------------------------------------------------------------\n",
    "\n",
    "size =100000\n",
    "for ticker_ind in range(2,5):\n",
    "    # combine the feature and response array to random sample\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    time_index=data_mess[:,0]\n",
    "    data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    total_array_old=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                    time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)\n",
    "\n",
    "    total_array=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)[:size,:]\n",
    "    total_array=total_array[np.random.randint(len(total_array),size=len(total_array)),:]\n",
    "\n",
    "    train_num_index=int(len(total_array)*0.9)\n",
    "\n",
    "    print(\"total array shape:\",total_array.shape)\n",
    "\n",
    "    #split the data to train and test data set\n",
    "    train_x=total_array[:train_num_index,:134]\n",
    "    test_x=total_array[train_num_index:,:134]\n",
    "    train_y=total_array[:train_num_index,134]\n",
    "    test_y=total_array[train_num_index:,134]\n",
    "\n",
    "\n",
    "    # the y data need to reshape to size (n,) not (n,1)\n",
    "    test_y=test_y.reshape(len(test_y),)\n",
    "    train_y=train_y.reshape(len(train_y),)\n",
    "    print(\"train_x shape:\",train_x.shape)\n",
    "    print(\"test_x shape:\",test_x.shape)\n",
    "    print(\"test_y shape:\",test_y.shape)\n",
    "    print(\"train_y shape:\",train_y.shape)\n",
    "\n",
    "\n",
    "    # scale the data\n",
    "    # can use the processing.scale function to scale the data\n",
    "    from sklearn import preprocessing\n",
    "    # note that we need to transfer the data type to float\n",
    "    # remark: should use data_test=data_test.astype('float'),very important !!!!\n",
    "    # use scale for zero mean and one std\n",
    "    scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "\n",
    "\n",
    "    train_x_scale=scaler.transform(train_x)\n",
    "    test_x_scale=scaler.transform(test_x)\n",
    "\n",
    "    print(np.mean(train_x_scale,0))\n",
    "    print(np.mean(test_x_scale,0))\n",
    "\n",
    "    from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "    # change the depth of the tree to 6, number of estimators=100\n",
    "\n",
    "    t=time.time()\n",
    "    clf =  OneVsRestClassifier(RandomForestClassifier(max_depth=20,n_estimators=100,random_state= 987612345))\n",
    "    clf.fit(train_x_scale,train_y)\n",
    "\n",
    "    print(time.time()-t)\n",
    "\n",
    "    predict_y_test=np.array(clf.predict(train_x_scale))\n",
    "\n",
    "    print(\"train accuracy is:\",sum(predict_y_test==train_y)/len(train_y))\n",
    "\n",
    "    # define a function to prefict the result by threshold\n",
    "    # note: logistic model will return two probability\n",
    "    def predict_threshold(predict_proba, threshold):\n",
    "        res=[]\n",
    "        for i in range(len(predict_proba)):\n",
    "            res.append(int(predict_proba[i][1]>threshold))\n",
    "        return res\n",
    "\n",
    "    t=time.time()\n",
    "    predict_y_test=np.array(clf.predict(test_x_scale))\n",
    "    print(\"test time is :\",time.time()-t)\n",
    "    print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "\n",
    "    # # test the score for the train data\n",
    "    # from sklearn.metrics import (precision_score, recall_score,\n",
    "    #                              f1_score)\n",
    "    # print(\"test accuracy is:\",sum(predict_y_test==test_y)/len(test_y))\n",
    "    # precision= precision_score(predict_y_test,test_y)\n",
    "    # recall = recall_score(predict_y_test,test_y)\n",
    "    # f1=f1_score(predict_y_test,test_y)\n",
    "    # print(\"precision is: \\t %s\" % precision)\n",
    "    # print(\"recall is: \\t %s\" % recall)\n",
    "    # print(\"f1 score is: \\t %s\" %f1)\n",
    "\n",
    "\n",
    "    # #draw the crosstab chart\n",
    "    # %matplotlib inline\n",
    "    # ## draw chart for the cross table\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(3)\n",
    "        plt.xticks(tick_marks, [-1,0,1])\n",
    "        plt.yticks(tick_marks, [-1,0,1])\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(test_y, predict_y_test)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm)\n",
    "    plt.savefig(\"one_vs_rest.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    def get_index(index, value):\n",
    "        i=0\n",
    "        while index[i] <value:\n",
    "            i=i+1\n",
    "        return i\n",
    "\n",
    "\n",
    "    train_ratio=0.9\n",
    "    time_index=data_mess[:,0]\n",
    "    data_order_reduced=data_order[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    time_index_reduced=time_index[(time_index>= start_ind) & (time_index<= end_ind)]\n",
    "    total_array_old=np.concatenate((feature_array_list[ticker_ind],response_reduced_list[ticker_ind],\n",
    "                                    time_index_reduced.reshape(len(time_index_reduced),1)),axis=1)\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "\n",
    "    time_index_test=total_array[:,135][int(size*train_ratio):size]\n",
    "    # find the arbitrage occuring index\n",
    "    arbi_index=list(np.where(predict_y_test!=0)[0])\n",
    "    # find the index that 5 seconds later\n",
    "    arbi_future_index=[]\n",
    "    for i in arbi_index:\n",
    "        arbi_future_index.append(get_index(time_index_reduced,time_index_test[i]+5))\n",
    "\n",
    "    total_array_test=total_array[int(size*train_ratio):size,:]\n",
    "    future_price=[]\n",
    "    current_price=[]\n",
    "    pnl=[]\n",
    "    for i in range(len(arbi_index)):\n",
    "        #ask low\n",
    "        if predict_y_test[arbi_index[i]]==1 :\n",
    "            future_price=data_order_reduced[arbi_future_index[i],0]\n",
    "            current_price=total_array_test[arbi_index[i],2]\n",
    "            pnl.append(current_price-future_price)\n",
    "        # bid high\n",
    "        else: \n",
    "            future_price=data_order_reduced[arbi_future_index[i],2]\n",
    "            current_price=total_array_test[arbi_index[i],0]\n",
    "            pnl.append(future_price-current_price)\n",
    "\n",
    "    pnl=np.array(pnl)\n",
    "    predict_arbi=predict_y_test[predict_y_test!=0]\n",
    "    plt.plot(pnl[predict_arbi==1],\"b.\",label=\"Ask low PnL\")\n",
    "    plt.plot(pnl[predict_arbi==-1],\"r.\",label=\"Bid High PnL\")\n",
    "\n",
    "    plt.xlabel(\"Arbitrage Index\")\n",
    "    plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "    plt.title(\"PnL for \"+ticker_list[ticker_ind])\n",
    "    plt.legend()\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_pnl.png\")\n",
    "    plt.show()\n",
    "\n",
    "    cum_pnl=np.cumsum(pnl)\n",
    "    plt.plot(cum_pnl,\"b.\",label=\"Cumulative P&L\")\n",
    "    plt.xlabel(\"Arbitrage Index\")\n",
    "    plt.ylabel(\"Profit($10^{-4}$\\$)\")\n",
    "    plt.title(\"Cumulative PnL for \"+ticker_list[ticker_ind])\n",
    "    plt.legend()\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_cum_pnl.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the order book type\n",
    "\n",
    "use the data_mess data set to plot the chart of the order book type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Plot the order book types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "order_type_list=[]\n",
    "t=time.time()\n",
    "for ticker_ind in range(5):\n",
    "    order_type=[]\n",
    "    for i in [1,2,3,4,5]:\n",
    "        order_type.append(sum(data_mess_list[ticker_ind][:,1]==i))\n",
    "    order_type_list.append(order_type)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(order_type_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# n_groups = 5\n",
    "\n",
    "# means_men = (20, 35, 30, 35, 27)\n",
    "# std_men = (2, 3, 4, 1, 2)\n",
    "\n",
    "# means_women = (25, 32, 34, 20, 25)\n",
    "# std_women = (3, 5, 2, 3, 3)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# index = np.arange(n_groups)\n",
    "# bar_width = 0.35\n",
    "\n",
    "# opacity = 0.4\n",
    "# error_config = {'ecolor': '0.3'}\n",
    "\n",
    "# rects1 = plt.bar(index, means_men, bar_width,\n",
    "#                  alpha=opacity,\n",
    "#                  color='b',\n",
    "#                  yerr=std_men,\n",
    "#                  error_kw=error_config,\n",
    "#                  label='Men')\n",
    "\n",
    "# rects2 = plt.bar(index + bar_width, means_women, bar_width,\n",
    "#                  alpha=opacity,\n",
    "#                  color='r',\n",
    "#                  yerr=std_women,\n",
    "#                  error_kw=error_config,\n",
    "#                  label='Women')\n",
    "\n",
    "# plt.xlabel('Group')\n",
    "# plt.ylabel('Scores')\n",
    "# plt.title('Scores by group and gender')\n",
    "# plt.xticks(index + bar_width, ('A', 'B', 'C', 'D', 'E'))\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "order_type_array=np.array(order_type_list)\n",
    "\n",
    "\n",
    "n_groups=7.5\n",
    "index = np.arange(n_groups,step=1.5)    # the x locations for the groups\n",
    "ticker_list=['AAPL', 'AMZN', 'GOOG', 'INTC','MSFT']\n",
    "color_list=['red','yellow','green','blue','darkmagenta']\n",
    "type_list=['1:Order_book','2:Cancel_part','3:Delete_all','4:Execution_visible','5:Execution_hidden']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "bar_width = 0.25\n",
    "\n",
    "opacity = 0.6\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, order_type_array[:,0], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[0],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[0])\n",
    "\n",
    "rects2 = plt.bar(index + 1*bar_width, order_type_array[:,1], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[1],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[1])\n",
    "\n",
    "\n",
    "rects3 = plt.bar(index + 2*bar_width, order_type_array[:,2], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[2],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[2])\n",
    "\n",
    "rects4 = plt.bar(index + 3*bar_width, order_type_array[:,3], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[3],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[3])\n",
    "\n",
    "rects5 = plt.bar(index + 4*bar_width, order_type_array[:,4], bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color=color_list[4],\n",
    "                 error_kw=error_config,\n",
    "                 label=type_list[4])\n",
    "\n",
    "\n",
    "plt.xlabel('Stock Ticker')\n",
    "plt.ylabel('Numbers')\n",
    "plt.title('Order Book Types')\n",
    "plt.xticks(index + bar_width*2.5, ticker_list)\n",
    "plt.yticks(np.arange(0, 700000,50000))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Plot the arbitrage situation (bid high, ask low and no arbitrage)\n",
    "\n",
    "Take the first stock which is AAPL as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_order_reduced=data_order_list[0][(time_index_list[0]>= start_ind) & (time_index_list[0]<= end_ind)]\n",
    "data_mess_reduced=data_mess_list[0][(time_index_list[0]>= start_ind) & (time_index_list[0]<= end_ind)]\n",
    "time_index_reduced=time_index_list[0][(time_index_list[0]>= start_ind) & (time_index_list[0]<= end_ind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_ind=np.where(ask_low_time_list[0][1]==1)[0][0]\n",
    "last_ind=np.where(time_index_reduced>time_index_reduced[first_ind]+5)[0][0]\n",
    "print(\"first_ind:\",first_ind)\n",
    "print(\"last_ind:\",last_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "time_index=time_index_reduced[first_ind:last_ind+1]\n",
    "ask_price=data_order_reduced[first_ind:last_ind+1,0]\n",
    "bid_price=data_order_reduced[first_ind:last_ind+1,2]\n",
    "print(ask_pirce[1])\n",
    "print(bid_price[1])\n",
    "plt.plot(time_index,ask_price,'r.-',label=\"Ask Price\")\n",
    "plt.plot(time_index,bid_price,'b.-',label=\"Bid Price\")\n",
    "\n",
    "plt.xticks=time_index\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Ask Low Arbitrage Example\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.where(bid_high_time_list[0][1]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the bid high case\n",
    "\n",
    "first_ind=np.where(bid_high_time_list[0][1]==1)[0][20]\n",
    "last_ind=np.where(time_index_list[0]>time_index_list[0][first_ind]+5)[0][0]\n",
    "print(\"first_ind:\",first_ind)\n",
    "print(\"last_ind:\",last_ind)\n",
    "\n",
    "%matplotlib qt\n",
    "time_index=time_index_list[0][first_ind:last_ind+1]\n",
    "ask_price=data_order_list[0][first_ind:last_ind+1,0]\n",
    "bid_price=data_order_list[0][first_ind:last_ind+1,2]\n",
    "print(ask_pirce[1])\n",
    "print(bid_price[1])\n",
    "plt.plot(time_index,ask_price,'r.-',label=\"Ask Price\")\n",
    "plt.plot(time_index,bid_price,'b.-',label=\"Bid Price\")\n",
    "\n",
    "plt.xticks=time_index\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Bid High Arbitrage Example\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the no arbitrage case\n",
    "first_ind=np.where(no_arbi_time_list[0][1]==1)[0][20]\n",
    "last_ind=np.where(time_index_list[0]>time_index_list[0][first_ind]+5)[0][0]\n",
    "print(\"first_ind:\",first_ind)\n",
    "print(\"last_ind:\",last_ind)\n",
    "\n",
    "%matplotlib qt\n",
    "time_index=time_index_list[0][first_ind:last_ind+1]\n",
    "ask_price=data_order_list[0][first_ind:last_ind+1,0]\n",
    "bid_price=data_order_list[0][first_ind:last_ind+1,2]\n",
    "print(ask_pirce[1])\n",
    "print(bid_price[1])\n",
    "plt.plot(time_index,ask_price,'r.-',label=\"Ask Price\")\n",
    "plt.plot(time_index,bid_price,'b.-',label=\"Bid Price\")\n",
    "\n",
    "plt.xticks=time_index\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"No Arbitrage Example\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.plot the statistical properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) cumulative distribution function for arrival time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticker_ind=2\n",
    "data=data_mess_list[ticker_ind]\n",
    "# we use the market order\n",
    "data_order=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "\n",
    "arrival_time=data_order[1:,0]-data_order[0:-1,0]\n",
    "#delete the zero intra arrival time\n",
    "arrival_time=arrival_time[arrival_time>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu_log=np.mean(np.log(arrival_time))\n",
    "std_log=np.std(np.log(arrival_time))\n",
    "data_log=np.random.lognormal(mu_log,std_log,arrival_time.shape)\n",
    "\n",
    "mu_exp=np.mean(arrival_time)\n",
    "data_exp=np.random.exponential(mu_exp,arrival_time.shape)\n",
    "\n",
    "data_weibull=np.random.weibull(0.38,arrival_time.shape)\n",
    "beta=np.var(arrival_time)/np.mean(arrival_time)\n",
    "alpha=np.mean(arrival_time)/beta\n",
    "data_gamma=np.random.gamma(alpha,beta,arrival_time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.stats import lognorm\n",
    "ecdf = sm.distributions.ECDF(arrival_time,)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"b\",label=\"Original data\")\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_log)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"g\",label=\"Lognormal Distribution\")\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_exp)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"y\",label=\"Exponential distribution\")\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_weibull)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"r\",label=\"Weibull distribution\")\n",
    "\n",
    "\n",
    "ecdf = sm.distributions.ECDF(data_t)\n",
    "plt.xlim([0,10])\n",
    "plt.plot(ecdf.x, ecdf.y,\"purple\",label=\"Gamma distribution\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Intra-arrival time\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Cumulative distribution function of order arrival time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) loop for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2, 2,figsize=(13, 13))\n",
    "for tickerb_ind in range(1,5):\n",
    "    data=data_mess_list[ticker_ind]\n",
    "    # we use the market order\n",
    "    data_order=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "\n",
    "    arrival_time=data_order[1:,0]-data_order[0:-1,0]\n",
    "    #delete the zero intra arrival time\n",
    "    arrival_time=arrival_time[arrival_time>0]\n",
    "    mu_log=np.mean(np.log(arrival_time))\n",
    "    std_log=np.std(np.log(arrival_time))\n",
    "    data_log=np.random.lognormal(mu_log,std_log,arrival_time.shape)\n",
    "\n",
    "    mu_exp=np.mean(arrival_time)\n",
    "    data_exp=np.random.exponential(mu_exp,arrival_time.shape)\n",
    "\n",
    "    data_weibull=np.random.weibull(0.38,arrival_time.shape)\n",
    "    beta=np.var(arrival_time)/np.mean(arrival_time)\n",
    "    alpha=np.mean(arrival_time)/beta\n",
    "    data_gamma=np.random.gamma(alpha,beta,arrival_time.shape)\n",
    "    ecdf = sm.distributions.ECDF(arrival_time,)\n",
    "   \n",
    "  \n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"b\",label=\"Original data\")\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_log)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"g\",label=\"Lognormal Distribution\")\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_exp)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"y\",label=\"Exponential distribution\")\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_weibull)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"r\",label=\"Weibull distribution\")\n",
    "\n",
    "\n",
    "    ecdf = sm.distributions.ECDF(data_t)\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlim([0,10])\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].plot(ecdf.x, ecdf.y,\"purple\",label=\"Gamma distribution\")\n",
    "    \n",
    "    \n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_xlabel(\"Intra-arrival time\")\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_ylabel(\"Probability\")\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].legend(loc=\"lower right\")\n",
    "    axarr[int((ticker_ind-1)/2),(ticker_ind+1)%2].set_title(\"Cumulative distribution function of \\n order arrival time  for stock \"+ticker_list[ticker_ind])\n",
    "\n",
    "plt.savefig('arrival_time.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) volume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.stats import lognorm  \n",
    "import seaborn as sns\n",
    "ticker_ind=0\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_AAPL.png\")\n",
    "plt.show()\n",
    "\n",
    "ticker_ind=1\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_AMZN.png\")\n",
    "plt.show()\n",
    "\n",
    "ticker_ind=3\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale)+1.2, shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_INTC.png\")\n",
    "plt.show()\n",
    "\n",
    "ticker_ind=4\n",
    "x=np.linspace(0,50,1000)\n",
    "y=x**(-2.1)/500\n",
    "plt.plot(np.log(x)+3,y,\"g--\",label=\"Power law with $\\propto x^{-2.1}$\")\n",
    "y_exp=np.exp(-x)\n",
    "plt.plot(np.log(x)+2,y_exp,\"r--\",label=\"Exponential distribution\")\n",
    "data=data_mess_list[ticker_ind]\n",
    "\n",
    "data_market=data[(data[:,1]==4) | (data[:,1]==5)]\n",
    "data_order=data[data[:,1]==1]\n",
    "mean_market=np.mean(data_market[:,3])\n",
    "mean_order=np.mean(data_order[:,3])\n",
    "\n",
    "vol_market_scale=data_market[:,3]/mean_market\n",
    "vol_order_scale=data_order[:,3]/mean_order\n",
    "\n",
    "sns.kdeplot(np.log(vol_market_scale)+1.2, shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "\n",
    "plt.xlim([0,5])\n",
    "plt.ylim()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Log scale of normalized volume of market orders\")\n",
    "plt.ylabel(\"Probability functions\")\n",
    "plt.title(\"Emprical probability density function of \\n nomalized volume of \"+ticker_list[ticker_ind])\n",
    "plt.savefig(\"volume_MSFT.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3) Intraday seasonality\n",
    "\n",
    "observe the volume during the whole day under 5 minutes time bins. show the result of seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticker_ind=0\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "data_mess_limit=data_mess[data_mess[:,1]==1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calute the volume of limit order book in each time interval\n",
    "\n",
    "time_interval=np.linspace(data_mess_limit[:,0].min(),data_mess_limit[:,0].max(),78)\n",
    "vol=0\n",
    "vol_time=[]\n",
    "j=1\n",
    "\n",
    "for i in range(len(data_mess_limit)):\n",
    "    if  data_mess_limit[i,0]<=time_interval[j]:\n",
    "        vol=vol+data_mess_limit[i,3]\n",
    "    else: \n",
    "        j=j+1\n",
    "        vol_time.append(vol)\n",
    "        vol=data_mess_limit[i,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the quadratic fit and vol_time\n",
    "x=range(76)\n",
    "plt.plot(x,vol_time,label=ticker_list[ticker_ind])\n",
    "qua_fit=np.poly1d(np.polyfit(x, vol_time, 2))(x)\n",
    "plt.plot(x,qua_fit,label=ticker_list[ticker_ind]+\" quadratic fit\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "xticks=np.arange(34200,57600,2400)\n",
    "plt.xticks(x[::8],xticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for limit order \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "for ticker_ind in range(1,5):\n",
    "\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_mess_limit=data_mess[data_mess[:,1]==1,:]\n",
    "    # calute the volume of limit order book in each time interval\n",
    "\n",
    "    time_interval=np.linspace(data_mess_limit[:,0].min(),data_mess_limit[:,0].max(),78)\n",
    "    vol=0\n",
    "    vol_time=[]\n",
    "    j=1\n",
    "\n",
    "    for i in range(len(data_mess_limit)):\n",
    "        if  data_mess_limit[i,0]<=time_interval[j]:\n",
    "            vol=vol+data_mess_limit[i,3]\n",
    "        else: \n",
    "            j=j+1\n",
    "            vol_time.append(vol)\n",
    "            vol=data_mess_limit[i,3]\n",
    "    # plot the quadratic fit and vol_time\n",
    "    x=range(76)\n",
    "    plt.plot(x,vol_time,\"+-\",label=ticker_list[ticker_ind])\n",
    "    qua_fit=np.poly1d(np.polyfit(x, vol_time, 2))(x)\n",
    "    plt.plot(x,qua_fit,\"--\",label=ticker_list[ticker_ind]+\" quadratic fit\")\n",
    "    plt.legend(loc=\"upper center\")\n",
    "    xticks=np.arange(34200,57600,2400)\n",
    "    plt.xticks(x[::8],xticks)\n",
    "    plt.title(\"Number of limit orders in a 5-minute interval for \"+ticker_list[ticker_ind])\n",
    "    plt.xlabel(\"Time of day(seconds)\")\n",
    "    plt.ylabel(\"Number of limit orders submitted in $\\Delta_t=5$ minutes\")\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_limit_vol_time.png\",bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# market order\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "for ticker_ind in range(1,5):\n",
    "\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_mess_market=data_mess[(data_mess[:,1]==4) | (data_mess[:,1]==5),:]\n",
    "    # calute the volume of limit order book in each time interval\n",
    "\n",
    "    time_interval=np.linspace(data_mess_market[:,0].min(),data_mess_market[:,0].max(),78)\n",
    "    vol=0\n",
    "    vol_time=[]\n",
    "    j=1\n",
    "\n",
    "    for i in range(len(data_mess_market)):\n",
    "        if  data_mess_market[i,0]<=time_interval[j]:\n",
    "            vol=vol+data_mess_market[i,3]\n",
    "        else: \n",
    "            j=j+1\n",
    "            vol_time.append(vol)\n",
    "            vol=data_mess_market[i,3]\n",
    "    # plot the quadratic fit and vol_time\n",
    "    x=range(76)\n",
    "    plt.plot(x,vol_time,\"+-\",label=ticker_list[ticker_ind])\n",
    "    qua_fit=np.poly1d(np.polyfit(x, vol_time, 2))(x)\n",
    "    plt.plot(x,qua_fit,\"--\",label=ticker_list[ticker_ind]+\" quadratic fit\")\n",
    "    plt.legend(loc=\"upper center\")\n",
    "    xticks=np.arange(34200,57600,2400)\n",
    "    plt.xticks(x[::8],xticks)\n",
    "    plt.title(\"Number of market orders in a 5-minute interval for \"+ticker_list[ticker_ind])\n",
    "    plt.xlabel(\"Time of day(seconds)\")\n",
    "    plt.ylabel(\"Number of market orders submitted in $\\Delta_t=5$ minutes\")\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_market_vol_time.png\",bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) average shape of the order books\n",
    "find the total volume for all each price level and see the volume trend based on the price levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 4) average shape of the order books\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "ticker_ind=1\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "data_order=data_order_list[ticker_ind]\n",
    "data_order_limit_ask_vol=data_order[data_mess[:,1]==1,1:40:4]\n",
    "data_order_limit_bid_vol=data_order[data_mess[:,1]==1,3:40:4]\n",
    "\n",
    "vol_ask=np.sum(data_order_limit_ask_vol,axis=0)/np.mean(np.sum(data_order_limit_ask_vol,axis=0))\n",
    "vol_bid=np.sum(data_order_limit_bid_vol,axis=0)/np.mean(np.sum(data_order_limit_bid_vol,axis=0))\n",
    "plt.plot(list(range(-10,0)),vol_bid)\n",
    "plt.plot(list(range(1,11)),vol_ask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop the stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "marker_list=[\"s\",\"D\",\"^\",\"8\"]\n",
    "color_list=[\"g\",\"b\",\"r\",\"y\"]\n",
    "for ticker_ind in range(1,5):\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "    data_order_limit_ask_vol=data_order[:,1:40:4]\n",
    "    data_order_limit_bid_vol=data_order[:,3:40:4]\n",
    "\n",
    "    vol_ask=np.sum(data_order_limit_ask_vol,axis=0)/np.mean(np.sum(data_order_limit_ask_vol,axis=0))\n",
    "    vol_bid=np.sum(data_order_limit_bid_vol,axis=0)/np.mean(np.sum(data_order_limit_bid_vol,axis=0))\n",
    "    plt.plot(list(range(-10,0)),vol_bid,\n",
    "             \"--\",marker=marker_list[ticker_ind-1],color=color_list[ticker_ind-1],label=\n",
    "            ticker_list[ticker_ind])\n",
    "    plt.plot(list(range(1,11)),vol_ask,\"--\",marker=marker_list[ticker_ind-1],color=color_list[ticker_ind-1])\n",
    "plt.ylim([0.6,1.6])\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Average quantity offered in the market order book\")\n",
    "plt.xlabel(\"Price level of limit orders (negative axis : bids ; positive axis : asks)\")\n",
    "plt.ylabel(\"Average numbers of shares(Normalized by mean)\")\n",
    "plt.savefig(\"level_quantity.png\",bbox_inches='tight')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) placement of orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticker_ind=2\n",
    "data_mess=data_mess_list[ticker_ind]\n",
    "data_order=data_order_list[ticker_ind]\n",
    "\n",
    "data_mess_limit=data_mess[data_mess[:,1]==1,:]\n",
    "data_order_limit=data_order[data_mess[:,1]==1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spread_list=[]\n",
    "for i in range(1,len(data_mess_limit)):\n",
    "    if data_mess_limit[i,5]==-1:\n",
    "        spread=data_mess_limit[i,4]-data_order_limit[i-1,0]\n",
    "    else:\n",
    "        spread=data_order_limit[i-1,2]-data_mess_limit[i,4]\n",
    "    spread_list.append(spread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import math\n",
    "sns.kdeplot(np.array(spread_list), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "mu = 0\n",
    "variance = np.var(spread_list)\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(min(spread_list), max(spread_list), 100)\n",
    "plt.plot(x,mlab.normpdf(x, mu, sigma),\"r--\",label=\"Gaussian\")\n",
    "plt.xlim([-10000,10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ticker_ind in range(1,5):\n",
    "    data_mess=data_mess_list[ticker_ind]\n",
    "    data_order=data_order_list[ticker_ind]\n",
    "\n",
    "    data_mess_limit=data_mess[data_mess[:,1]==1,:]\n",
    "    data_order_limit=data_order[data_mess[:,1]==1,:]\n",
    "\n",
    "    spread_list=[]\n",
    "    for i in range(1,len(data_mess_limit)):\n",
    "        if data_mess_limit[i,5]==-1:\n",
    "            spread=data_mess_limit[i,4]-data_order_limit[i-1,0]\n",
    "        else:\n",
    "            spread=data_order_limit[i-1,2]-data_mess_limit[i,4]\n",
    "        spread_list.append(spread)\n",
    "\n",
    "\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import matplotlib.mlab as mlab\n",
    "    import math\n",
    "    sns.kdeplot(np.array(spread_list), shade=True,label=ticker_list[ticker_ind]+\" Data\")\n",
    "    mu = 0\n",
    "    variance = np.var(spread_list)\n",
    "    sigma = math.sqrt(variance)\n",
    "    x = np.linspace(min(spread_list), max(spread_list), 100)\n",
    "    plt.plot(x,mlab.normpdf(x, mu, sigma),\"r--\",label=\"Gaussian\")\n",
    "    plt.xlim([min(spread_list)*0.8,max(spread_list)*0.8])\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Placement of limit orders using the\\n same best quote reference for \"+ticker_list[ticker_ind])\n",
    "    plt.xlabel(\"Price diference\")\n",
    "    plt.ylabel(\"Probability density function\")\n",
    "    plt.savefig(ticker_list[ticker_ind]+\"_placement.png\",bbox_inches='tight')    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}